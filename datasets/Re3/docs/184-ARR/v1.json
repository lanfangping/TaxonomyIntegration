{
    "nodes": [
        {
            "ix": "184-ARR_v1_0",
            "content": "Graph Neural Networks for Multiparallel Word Alignment",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_2",
            "content": "After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation. Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together. First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph. Next, we use graph neural networks (GNNs) and community detection algorithms to exploit the graph structure. Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) provides a prediction model that can generalize beyond the sentences it is trained on. We show that community detection provides valuable information for multiparallel word alignment. Our method outperforms previous work on three word alignment datasets and on a downstream task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "184-ARR_v1_4",
            "content": "Word alignments are crucial for statistical machine translation (Koehn et al., 2003) and useful for many other multilingual tasks such as neural machine translation (Alkhouli and Ney, 2017;Alkhouli et al., 2016), typological analysis (Lewis and Xia, 2008;\u00d6stling, 2015;Asgari and Sch\u00fctze, 2017), annotation projection (Yarowsky and Ngai, 2001;Fossum and Abney, 2005;Wisniewski et al., 2014;Huck et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_5",
            "content": "The rise of deep learning initially led to a temporary plateau, but interest in word alignments is now increasing, demonstrated by several recent publications (Jalili Sabet et al., 2020;Dou and Neubig, 2021) Colors represent languages. Each English (yellow) node is annotated with its word. Red dashed lines sever links that incorrectly connect distinct concepts. We exploit community detection algorithms to detect distinct concepts. This provides valuable information for our GNN model and improves word alignments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_6",
            "content": "Multiparallel corpora contain sentence level parallel text in more than two languages, e.g., JW300 (Agi\u0107 and Vuli\u0107, 2019), PBC (Mayer and Cysouw, 2014) and Tatoeba. 1 While the amount of data provided by multiparallel corpora is less than bilingual corpora, this type of corpus is essential to study very low-resource languages. There are thousands of languages in the world a very small portion of which is covered by language technologies (Joshi et al., 2020). Recent work (Bird, 2020) suggests a number of approaches to develop technologies for indigenous languages. Multiparallel corpora are a valuable (and arguably complementary) resource for this aim. We use the PBC corpus since it covers more than 1300 languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_7",
            "content": "Most prior work on word alignment uses bitext, with one notable exception: (Imani et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_8",
            "content": "They introduce MPWA (MultiParallel Word Alignment), a framework that utilizes the synergy between multiple language pairs to improve bilingual word alignments. The rationale is that some of the missing alignment edges between a source and a target language can be recovered using their alignments with words in other languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_9",
            "content": "The first step in MPWA is to create bilingual alignments for all language pairs in a multiparallel corpus using a bilingual word aligner. Then the bilingual alignments for a multiparallel sentence are represented as a graph where words are nodes and initial word alignments are edges. Figure 1 gives an example: a multiparallel alignment graph for a 12-way multiparallel corpus. MPWA infers missing alignment links based on the graph structure in a postprocessing step, casting the word alignment task as an edge prediction problem. They use two traditional graph algorithms, Adamic-Adar and non-negative matrix factorization, for edge prediction. However, these standard graph algorithms are applied to individual multiparallel sentences independently and therefore cannot accumulate knowledge from multiple sentences. Moreover, their edge predictions are solely based on the structure of the graph and do not take advantage of other beneficial signals such as a word's language, relative position and word meaning. Another limitation is that it only adds links and does not remove any, which is important to improve precision.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_10",
            "content": "In this paper, we propose to use graph neural networks (GNNs) to exploit the graph structure of multiparallel word alignments and address the limitations of prior work. GNNs were proposed to extend the powerful current generation of neural network models to processing graph-structured data (Scarselli et al., 2009) and they have gained increasing popularity in many domains (Wu et al., 2020;Sanchez-Gonzalez et al., 2018;He et al., 2020). In contrast to other graph algorithms, GNNs can incorporate heterogeneous sources of signal in the form of node and edge features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_11",
            "content": "Since the nodes in the graph are words that are translations of each other, we expect them to create densely connected regions or communities. Our analysis of the structure of the multiparallel alignment graph confirms this intuition; see Figure 1. We use community detection algorithms to find communities. We show that pruning inter-community edges and adding intracommunity edges is helpful. We use community information as node features for our GNN.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_12",
            "content": "We enable the removal of alignment edges from initial alignments by inferring alignments from the alignment probability matrix. Our method predicts new alignment links independently of initial edges. Therefore it is not limited to adding edges wrt initial bilingual alignments, it can also remove them.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_13",
            "content": "For our experiments, we follow the setup of Imani et al. (2021). We train a GNN model with a link prediction objective. We show improved results for three language pairs on word alignment (English-French, Finnish-Hebrew and Finnish-Greek). As a demonstration of the importance of high-quality alignments, we use our word alignments to project annotations from highresource to low-resource languages. We improve a part-of-speech tagger for Yoruba by training it over a high-quality dataset, which is created using annotation projection. We show that our model is especially helpful for distant languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_14",
            "content": "Contributions: i) We propose a graph neural network model that incorporates a diverse set of features for word alignments in multiparallel corpora and an elegant way of training it efficiently and effectively. ii) We show that community detection improves multiparallel word alignment. iii) We show that the improved alignments improve performance on a downstream task for a low resource language. iv) We propose a new method to infer alignments from the alignment probability matrix. v) We will make our code publicly available.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_15",
            "content": "Graph Analysis with Community Detection (CD)",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "184-ARR_v1_16",
            "content": "The nodes in the alignment graph are words that are translations of each other. If the initial bilingual alignments are of good quality, we expect these translated words to form densely connected regions or communities; see Figure 1. We expect these communities to be genarally disconnected, each corresponding to a distinct connected component.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_17",
            "content": "In other words, ideally, words representing a concept should be densely connected, but there should be no links between different concepts. Clearly, this intuition will not be true for all concepts between all possible language pairs. Nonetheless, we hypothesize that identifying distinct concepts in a multiparallel word alignment graph can provide useful information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_18",
            "content": "To examine to what extent this expectation is met, we count the components in the original Eflomal-generated (\u00d6stling and Tiedemann, 2016) graph. Table 1 shows that the average number of components per sentence is less than three (\"Eflomal intersection\", columns #CC). But intuitively, the number of components should roughly correspond to sentence length (i.e., the number of content words). This indicates that there are many links that incorrectly connect different concepts. To detect such links, we use community detection (CD) algorithms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_19",
            "content": "CD algorithms find subnetworks of nodes that form tightly knit groups that are only loosely connected with a small number of links (Girvan and Newman, 2002). CD algorithms maximize the modularity measure (Newman and Girvan, 2004). Modularity measures how beneficial a division of a community into two communities is, in the sense that there are many links within communities and only a few between them. Given a graph G with n nodes and m edges and G's adjacency matrix A \u2208 R n\u00d7n , modularity is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_20",
            "content": "mod = 1 2m ij A ij \u2212 \u03b3 d i d j 2m I(c i , c j ) (1) d i is the degree of node i. I(c i , c j ) is 1 if nodes i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_21",
            "content": "and j are in the same community, 0 otherwise. We experiment with two CD algorithms:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_22",
            "content": "\u2022 Greedy modularity communities (GMC). This method uses Clauset-Newman-Moore greedy modularity maximization (Clauset et al., 2004). GMC begins with each node in its own community and greedily joins the pair of communities that most increases modularity until no such pair exists. \u2022 Label propagation communities (LPC). This method finds communities in a graph using label propagation (Cordasco and Gargano, 2010). It begins by giving a label to each node of the network. Then each node's label is updated by the most frequent label among its neighbors in each iteration. It performs label propagation on a portion of nodes at each step and quickly converges to a stable labeling.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_23",
            "content": "After detecting communities, we link all nodes inside a community and remove all intercommunity links. GMC (LPC) on average removes 3% (7%) of the edges. Table 1 reports the average number of graph components per sentence before and after runing GMC and LPC, as well as the corresponding F 1 for word alignment. We see that the number of communities found is lower for GMC than for LPC; therefore, LPC identifies more candidate links for deletion. 2 Comparing the number of communities detected with the average sentence length, GMC seems to have failed to detect enough communities to split different concepts properly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_24",
            "content": "The F 1 scores confirm this observation and show that LPC performs well at detecting the communities we are looking for.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_25",
            "content": "These results indicate that CD algorithms can provide valuable information. To exploit this in our GNN model, we add a node's community information as a GNN node feature; see \u00a73.1.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_26",
            "content": "Methods",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "184-ARR_v1_27",
            "content": "GNN in MPWA",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "184-ARR_v1_28",
            "content": "GNNs can be used in transductive or inductive settings. Transductively, the final model can only be used for inference over the same graph that it is trained on. In an inductive setting, which we use here, nodes are represented as feature vectors, and the final model has the advantage of being applicable to a different graph in inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_29",
            "content": "Model Architecture",
            "ntype": "title",
            "meta": {
                "section": "3.1.1"
            }
        },
        {
            "ix": "184-ARR_v1_30",
            "content": "Our model is inspired by the Graph Auto Encoder (GAE) model of Kipf and Welling (2016b) for link prediction. The architecture consists of an encoder and a decoder. We make changes to this model to improve the model's quality and reduce its computation cost. We use GATConv layers (Veli\u010dkovi\u0107 et al., 2018) for encoder instead of GCNConv (Kipf and Welling, 2016a)and a more sophisticated decoder instead of simple dot product for a stronger model. We also introduce a more efficient training procedure.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_31",
            "content": "The encoder is a graph attention network (GAT) (Veli\u010dkovi\u0107 et al., 2018) with two GATConv layers followed by a fully connected layer. Layers are connected by RELU non-linearities. A GATConv layer computes its output x i for a node i from its input x i as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_32",
            "content": "x i = \u03b1 i,i Wx i + j\u2208N (i) \u03b1 i,j Wx j ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_33",
            "content": "where W is a weight matrix, N (i) is some neighborhood of node i in the graph, and \u03b1 i,j is the attention coefficient indicating the importance of node j's features to node i. \u03b1 i,j is computed as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_34",
            "content": "\u03b1 i,j = exp g a [Wx i Wx j ] k\u2208N (i)\u222a{i} exp (g (a [Wx i Wx k ]))",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_35",
            "content": "(3) where is concatanation, g is LeakyReLU, and a is a weight vector. Given the features for the nodes and their alignment edges, the encoder creates a contextualized hidden representation for each node.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_36",
            "content": "Based on the hidden representations of two nodes, the decoder predicts whether a link connects them. The decoder architecture consists of a fully connected layer, a RELU non-linearity and a sigmoid layer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_37",
            "content": "Training. By default, GAE models are trained using full batches with random negative samples. This approach requires at least tens of epochs over training dataset to converge and a lot of GPU memory for graphs as big as ours. We train our model using mini-batches and an adversarial loss to decrease memory requirements and improve the performance. Using our training approach the model converges after one epoch. The negative samples are selected more elegantly, as described below. Figure 2 displays our GNN model and the training process. The outer loop iterates over the multiparallel sentences in the training set. The training set contains one graph for each sentence; the graph is constructed using the bilingual alignment edges between all language pairs. Each graph is divided into multiple batches. Each batch contains a random subset of the graph's edges as positive samples. The negative samples are created as follows. Given a sentence u 1 u 2 . . . u n in language U and its translation v 1 v 2 . . . v m in language V , for each alignment edge u i :v j in the current batch, two negative edges u i :v j and u i :v j (j = j, i = i) are randomly sampled.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_38",
            "content": "For each training batch, the encoder takes the batch's whole graph (i.e., node features for all graph nodes and all graph edges) as input and computes hidden representations for the nodes. On the decoder side, for each link of the batch, the hidden representations of the attached nodes are concatenated to create the decoder's input. The decoder's target is the link's class: 1 (resp. 0) for positive (resp. negative) links. We train with a binary classification objective:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_39",
            "content": "L = \u2212 1 b b i=1 log(p + i ) + 1 2b 2b i=1 log(p \u2212 i ) (4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_40",
            "content": "where b is the batch size and p + i and p \u2212 i are the model predictions for the i th positive and negative samples within the batch. Parameters of the encoder and decoder as well as the node-embedding feature layer are updated after each training step.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_41",
            "content": "Node Features",
            "ntype": "title",
            "meta": {
                "section": "3.1.2"
            }
        },
        {
            "ix": "184-ARR_v1_42",
            "content": "We use three main types of node features: (i) graph structural features, (ii) community-based features and (iii) word content features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_43",
            "content": "Graph structural features. We use degree, closeness (Freeman, 1978) , betweenness (Brandes, 2001) , load (Newman, 2001) and harmonic centrality (Boldi and Vigna, 2014) features as additional information about the graph structure. These features are continuous numbers, providing information about the position and connectivity of the nodes within the graph. We standardize (i.e., zscore) each feature across all nodes, and train an embedding of size four for each feature. 3 Community-based features. One way to incorporate community information into our model is to train the model based on the refined edges after the community detection step. This approach hobbles the GNN model by making decisions about many of the edges before the GNN gets to see them. Our initial experiments also confirmed that training the GNN over CD refined edges does not help. Therefore, we add community information as node features and let the GNN use them to improve its decisions. We use the community detection algorithms GMC and LPC (see \u00a72) to identify communities in the graph. Then we take the community membership information of the nodes as one-hot vectors and learn an embedding of size 32 for each of the two algorithms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_44",
            "content": "Word content features. We train embeddings for word position (size 32) and word language (size 20). We learn 100-dimensional multilingual word embeddings using Levy et al. (2017)'s sentence-ID method on the 84 PBC languages selected by Imani et al. (2021). Word embeddings serve as initialization and are updated during GNN training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_45",
            "content": "After concatenating these features, each node is represented by a 236 dimensional vector that is then fed to the encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_46",
            "content": "Inducing Alignment Edges",
            "ntype": "title",
            "meta": {
                "section": "3.1.3"
            }
        },
        {
            "ix": "184-ARR_v1_47",
            "content": "When our trained GNN model is used to predict alignment edges between a source sentence x = x 1 , x 2 , . . . , x m in language X and a target sentence \u0177 = y 1 , y 2 , . . . , y l in language Y , it produces a symmetric alignment probability matrix S 4 of size m \u00d7 l where S ij is the predicted alignment probability between words x i and y j . Using these values directly to infer alignment edges is usually suboptimal; therefore, more sophisticated methods have been suggested (Ayan and Dorr, 2006;Liang et al., 2006). Here we propose a new approach: it combines Koehn et al. (2005)'s Grow-Diag-Final-And (GDFA) with Dou and Neubig (2021)'s probability thresholding. We modify the latter to account for the variable size of the probability matrix (i.e., length of source/target sentences). Our method is not limited to adding new edges to some initial bilingual alignments, a limitation of prior work. As we predict each edge independently, some initial links can be discarded from the final alignment.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_48",
            "content": "We start by creating a set of forward (sourceto-target) alignment edges and a set of backward (target-to-source) alignment edges. To this end, first, inspired by probability thresholding (Dou and Neubig, 2021), we apply softmax to S, and zero out probabilities below a threshold to get a sourceto-target probability matrix S XY :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_49",
            "content": "S XY = S * (softmax(S) > \u03b1 l )(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_50",
            "content": "Analogously, we compute the target-to-source probability matrix S Y X :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_51",
            "content": "S Y X = S * (softmax(S ) > \u03b1 m )(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_52",
            "content": "where \u03b1 is a sensitivity hyperparameter, e.g., \u03b1 = 1 means that we pick edges with a probability higher than average. We experimentally set \u03b1 = 2. Next, from each row of S XY (S Y X ), we pick the cell with the highest value (if any exists) and add this edge to the forward (backward) set. We create the final set of alignment edges by applying the GDFA symmetrization method (Koehn et al., 2005) to forward and backward sets. The gist of GDFA is to use the intersection of forward and backward as initial alignment edges and add more edges from the union of forward and backward based on a number of heuristics. We call this method TGDFA (Thresholding GDFA).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_53",
            "content": "We also experiment with combining TGDFA with the original bilingual GDFA alignments. We do so by adding bilingual GDFA edges to the union of forward and backward before performing the GDFA heuristics. We refer to these alignments as TGDFA+orig.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_54",
            "content": "We evaluate the resulting alignments using F 1 score and alignment error rate (AER), the standard evaluation measures in the word alignment literature.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_55",
            "content": "Annotation Projection",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "184-ARR_v1_56",
            "content": "Annotation projection automatically creates linguistically annotated corpora for low-resource languages. A model trained on data with \"annotationprojected\" labels can perform better than full unsupervision. Here, we focus on universal part-ofspeech (UPOS) tagging (Petrov et al., 2012) for the low resource target language Yoruba; this language only has a small set of annotated sentences in Universal Dependencies (Nivre et al., 2020) and has poor POS results in unsupervised settings (Kondratyuk and Straka, 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_57",
            "content": "The quality of the target annotated corpus depends on the quality of the annotations in the source languages and the quality of the word alignments between sources and target. We use the Flair (Akbik et al., 2019) POS taggers for three high resource languages, English, German and French (Akbik et al., 2018), to annotate 30K verses whose Yoruba translations are available in PBC. We then transfer the POS tags from source to target using three different approaches: (i) We directly transfer annotations from English to the target. (ii) For each word in the target, we get its alignments in the three source languages and predict the majority POS to annotate the target word. (iii) We repeat (ii) using alignments from our GNN (TGDFA) model instead of the original bilingual alignments. In all three approaches, we discard any target sentence from the POS tagger training data if more than 50% of its words are annotated with the \"X\" (other) tag.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_58",
            "content": "We train a Flair SequenceTagger model on the target annotated data using mBERT embeddings (Devlin et al., 2019) and evaluate on Yoruba test from Universal Dependencies. 5 Evaluation data. For our main evaluation, we use the two word alignment gold datasets for PBC published by Imani et al. (2021): Blinker (Melamed, 1998) and HELFI (Yli-Jyr\u00e4 et al., 2020). The HELFI dataset contains the Hebrew Bible, Greek New Testament and their translations into Finnish. For HELFI, we use Imani et al. (2021)'s train/dev/test splits. The Blinker dataset provides word level alignments between English and French for 250 Bible verses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_59",
            "content": "Training data. The graph algorithms used by Imani et al. (2021) operate on each multiparallel sentence separately. In contrast, our approach allows for an inductive setting where a model is trained on a training set and then evaluated on a separate test set. We combine the verses in the training sets of Finnish-Hebrew and Finnish-Greek for a combined train set size of 24,159.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_60",
            "content": "Initial Word Alignments",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "184-ARR_v1_61",
            "content": "We use the Eflomal statistical word aligner to obtain bilingual alignments. We train it for every language pair in our experiments. We do not consider SimAlign (Jalili Sabet et al., 2020) since it is shown to perform poorly for languages whose representations in the multilingual pretrained language model are of low quality. We use Eflomal asymmetrical alignments post-processed with the intersection heuristic to get high precision bilingual alignments as input to the GNN. We use the same subset of 84 languages as Imani et al. (2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_62",
            "content": "Training Details",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "184-ARR_v1_63",
            "content": "We use PyTorch Geometric 6 to construct and train the GNN. The model's hidden layer size is 512 for both GATConv and Linear layers. We train for one epoch on the train set -a small portion of the train set is enough to learn good embeddings (see \u00a75.1.1). For training, we use a batch size of 400 and learning rate of .001 with AdamW (Loshchilov and Hutter, 2017). The whole training process takes less than 4 hours on a GeForce GTX 1080 Ti and the inference time is on the order of milliseconds per sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_64",
            "content": "Experiments and Results",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "184-ARR_v1_65",
            "content": "Multiparallel corpus results",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "184-ARR_v1_66",
            "content": "Table 2 shows results on Blinker and HELFI for our GNNs and the baselines: bilingual alignments and the traditional graph algorithms WAdAd and NMF from Imani et al. (2021). Our GNNs provide a better trade-off between precision and recall, most likely thanks to their ability to remove edges, and achieve the best F 1 and AER on all three datasets, outperforming WAdAd and NMF.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_67",
            "content": "GNN (TGDFA) achieves the best results on HELFI (FIN-HEB, FIN-GRC) while GNN (TGDFA+orig) is best on Blinker (ENG-FRA). As argued in Imani et al. (2021), this is mostly due to the different ways these two datasets were annotated. Most HELFI alignments are one-to-one, while many Blinker alignments are many-to-many: phrase-level alignments where every word in a source phrase is aligned with every word in a target phrase. This suggests that one can choose between GNN (TGDFA) and GNN (TGDFA+orig) based on the characteristics of the desired alignments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_68",
            "content": "Effect of Training Set Size",
            "ntype": "title",
            "meta": {
                "section": "5.1.1"
            }
        },
        {
            "ix": "184-ARR_v1_69",
            "content": "To investigate the effect of training set size, we train the GNN on subsets of our training data with increasing sizes. Figure 3 shows results. Performance improves fast until around 2,000 verses; then it stays mostly constant. Indeed, using more than 6,400 samples does not change the performance at all. Therefore, in the other experiments we use 6,400 randomly sampled verses from the training set to train GNNs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_70",
            "content": "Ablation Experiments",
            "ntype": "title",
            "meta": {
                "section": "5.1.2"
            }
        },
        {
            "ix": "184-ARR_v1_71",
            "content": "To examine the importance of node features, we ablate language, position, centrality, community 6 pytorch-geometric.readthedocs.io 3 shows that removal of graph structural features drastically reduces performance. Community features and language information are also important. Removal of word position information and word embeddings -which store semantic information about wordshas the least effect. Based on these results, it can be argued that the lexical information contained in the initial alignments and in the community features provides a strong signal regarding word relatedness. The novel information that is crucial is about the overall graph structure which goes beyond the local word associations that are captured by word position and word embeddings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_72",
            "content": "Effect of Word Frequency",
            "ntype": "title",
            "meta": {
                "section": "5.1.3"
            }
        },
        {
            "ix": "184-ARR_v1_73",
            "content": "We investigate the effect of word frequency on alignment performance where frequency is calculated based on the source word in the PBC; the first bin has the highest frequency. Figure 4 shows that the performance of Eflomal drops with frequency and it struggles to align very rare words. In contrast, GNN is not affected by word frequency as severely and its performance gains are even greater for rare words. WAdad which is the multilingual baseline from (Imani et al., 2021) has the same trend as GNN method, but GNN is more robust.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_74",
            "content": "Annotation Projection",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "184-ARR_v1_75",
            "content": "Table 4 presents accuracies for POS tagging in Yoruba. Unsupervised baseline performance is 50.86%. Supervised training using pseudo-labels mostly outperforms the unsupervised baseline. Projecting the majority POS labels to Yoruba improves over projecting English labels. Using the GNN model to project labels works best and outperforms , 2003), fast-align (Dyer et al., 2013) and Eflomal (\u00d6stling and Tiedemann, 2016). More recent work, including SimAlign (Jalili Sabet et al., 2020) and SHIFT-ATT/SHIFT-AET , uses pretrained neural language and machine translation models. Although neural models achieve superior performance compared to statistical aligners, they are only applicable for less than two hundred high-resource languages that are supported by multilingual language models like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020). This makes statistical models the only option for the majority of the world's languages. Multiparallel Corpora. Prior applications of using multiparallel corpora include reliable translations from small datasets (Cohn and Lapata, 2007), and phrase-based machine translation (PBMT) (Kumar et al., 2007). Multiparallel corpora are also used for language comparison (Mayer and Cysouw, 2012), typological studies (\u00d6stling, 2015;Asgari and Sch\u00fctze, 2017) and PBMT (Nakov and Ng, 2012;Bertoldi et al., 2008;Dyer et al., 2013).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_76",
            "content": "To the best of our knowledge \u00d6stling (2014) 7 is the only word alignment method designed for multiparallel corpora. However, this method is outperformed by Eflomal (\u00d6stling and Tiedemann, 2016), a \"biparallel\" method from the same author. Recently, Imani et al. (2021) proposed MPWA, which we use as our baseline.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_77",
            "content": "Graph Neural Networks (GNNs) have been used to address many problems that are inherently graph-like such as traffic networks, social networks, and physical and biological systems (Liu and Zhou, 2020). GNNs achieve impressive performance in many domains, including social networks (Wu et al., 2020) and natural science (Sanchez-Gonzalez et al., 2018) as well as NLP tasks like sentence classification (Huang et al., 2020), question generation (Pan et al., 2020), and summarization (Fernandes et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_78",
            "content": "Conclusion and Future Work",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "184-ARR_v1_79",
            "content": "We introduced graph neural networks and community detection algorithms for multiparallel word alignment. By incorporating signals from diverse sources as node features, including community features, our GNN model outperformed the baselines and prior work, establishing new state-of-the-art results on three PBC gold standard datasets. We also showed that our GNN model improves downstream task performance in low-resource languages through annotation projection.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_80",
            "content": "We have only used node features to provide signals to GNNs. In the future, other signals can be added in the form of edge features to further boost the performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v1_81",
            "content": "\u017deljko Agi\u0107, Ivan Vuli\u0107, Jw300: A widecoverage parallel corpus for low-resource languages, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "\u017deljko Agi\u0107",
                    "Ivan Vuli\u0107"
                ],
                "title": "Jw300: A widecoverage parallel corpus for low-resource languages",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_82",
            "content": "Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, Roland Vollgraf, FLAIR: An easy-to-use framework for state-of-theart NLP, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Alan Akbik",
                    "Tanja Bergmann",
                    "Duncan Blythe",
                    "Kashif Rasul",
                    "Stefan Schweter",
                    "Roland Vollgraf"
                ],
                "title": "FLAIR: An easy-to-use framework for state-of-theart NLP",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_83",
            "content": "Alan Akbik, Duncan Blythe, Roland Vollgraf, Contextual string embeddings for sequence labeling, 2018, Proceedings of the 27th International Conference on Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Alan Akbik",
                    "Duncan Blythe",
                    "Roland Vollgraf"
                ],
                "title": "Contextual string embeddings for sequence labeling",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 27th International Conference on Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_84",
            "content": "Tamer Alkhouli, Gabriel Bretschner, Jan-Thorsten Peter, Mohammed Hethnawi, Andreas Guta, Hermann Ney, Alignment-based neural machine translation, 2016, Proceedings of the First Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Tamer Alkhouli",
                    "Gabriel Bretschner",
                    "Jan-Thorsten Peter",
                    "Mohammed Hethnawi",
                    "Andreas Guta",
                    "Hermann Ney"
                ],
                "title": "Alignment-based neural machine translation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the First Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_85",
            "content": "Tamer Alkhouli, Hermann Ney, Biasing attention-based recurrent neural networks using external alignment information, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Tamer Alkhouli",
                    "Hermann Ney"
                ],
                "title": "Biasing attention-based recurrent neural networks using external alignment information",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_86",
            "content": "Ehsaneddin Asgari, Hinrich Sch\u00fctze, Past, present, future: A computational investigation of the typology of tense in 1000 languages, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Ehsaneddin Asgari",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Past, present, future: A computational investigation of the typology of tense in 1000 languages",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_87",
            "content": "Bonnie Necip Fazil Ayan,  Dorr, A maximum entropy approach to combining word alignments, 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Bonnie Necip Fazil Ayan",
                    " Dorr"
                ],
                "title": "A maximum entropy approach to combining word alignments",
                "pub_date": "2006",
                "pub_title": "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_88",
            "content": "Nicola Bertoldi, Madalina Barbaiani, Phrase-based statistical machine translation with pivot languages, 2008, In International Workshop on Spoken Language Translation, IWSLT.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Nicola Bertoldi",
                    "Madalina Barbaiani"
                ],
                "title": "Phrase-based statistical machine translation with pivot languages",
                "pub_date": "2008",
                "pub_title": "In International Workshop on Spoken Language Translation",
                "pub": "IWSLT"
            }
        },
        {
            "ix": "184-ARR_v1_89",
            "content": "Steven Bird, Decolonising speech and language technology, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Steven Bird"
                ],
                "title": "Decolonising speech and language technology",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_90",
            "content": "Paolo Boldi, Sebastiano Vigna, Axioms for centrality, 2014, Internet Mathematics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Paolo Boldi",
                    "Sebastiano Vigna"
                ],
                "title": "Axioms for centrality",
                "pub_date": "2014",
                "pub_title": "Internet Mathematics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_91",
            "content": ", A faster algorithm for betweenness centrality, 2001, Journal of mathematical sociology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [],
                "title": "A faster algorithm for betweenness centrality",
                "pub_date": "2001",
                "pub_title": "Journal of mathematical sociology",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_92",
            "content": "F Peter, Stephen Brown, Vincent Pietra, Robert Della Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "F Peter",
                    "Stephen Brown",
                    "Vincent Pietra",
                    "Robert Della Pietra",
                    " Mercer"
                ],
                "title": "The mathematics of statistical machine translation: Parameter estimation",
                "pub_date": "1993",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_93",
            "content": "Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, Qun Liu, Accurate word alignment induction from neural machine translation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Yun Chen",
                    "Yang Liu",
                    "Guanhua Chen",
                    "Xin Jiang",
                    "Qun Liu"
                ],
                "title": "Accurate word alignment induction from neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_94",
            "content": "Aaron Clauset, E Mark, Cristopher Newman,  Moore, Finding community structure in very large networks, 2004, Physical review E, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Aaron Clauset",
                    "E Mark",
                    "Cristopher Newman",
                    " Moore"
                ],
                "title": "Finding community structure in very large networks",
                "pub_date": "2004",
                "pub_title": "Physical review E",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_95",
            "content": "Trevor Cohn, Mirella Lapata, Machine translation by triangulation: Making effective use of multi-parallel corpora, 2007, Proceedings of the 45th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Trevor Cohn",
                    "Mirella Lapata"
                ],
                "title": "Machine translation by triangulation: Making effective use of multi-parallel corpora",
                "pub_date": "2007",
                "pub_title": "Proceedings of the 45th",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_96",
            "content": ", Czech Republic, , Annual Meeting of the Association of Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [],
                "title": "Czech Republic",
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association of Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_97",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Alexis Conneau",
                    "Kartikay Khandelwal",
                    "Naman Goyal",
                    "Vishrav Chaudhary",
                    "Guillaume Wenzek",
                    "Francisco Guzm\u00e1n",
                    "Edouard Grave",
                    "Myle Ott",
                    "Luke Zettlemoyer",
                    "Veselin Stoyanov"
                ],
                "title": "Unsupervised cross-lingual representation learning at scale",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_98",
            "content": "Gennaro Cordasco, Luisa Gargano, Community detection via semi-synchronous label propagation algorithms, 2010, 2010 IEEE international workshop on: business applications of social network analysis (BASNA), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Gennaro Cordasco",
                    "Luisa Gargano"
                ],
                "title": "Community detection via semi-synchronous label propagation algorithms",
                "pub_date": "2010",
                "pub_title": "2010 IEEE international workshop on: business applications of social network analysis (BASNA)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "184-ARR_v1_99",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "184-ARR_v1_100",
            "content": "Yi Zi, Graham Dou,  Neubig, Word alignment by fine-tuning embeddings on parallel corpora, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Yi Zi",
                    "Graham Dou",
                    " Neubig"
                ],
                "title": "Word alignment by fine-tuning embeddings on parallel corpora",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_101",
            "content": "Chris Dyer, Victor Chahuneau, Noah Smith, A simple, fast, and effective reparameterization of IBM model 2, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Chris Dyer",
                    "Victor Chahuneau",
                    "Noah Smith"
                ],
                "title": "A simple, fast, and effective reparameterization of IBM model 2",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_102",
            "content": "Patrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt, Structured neural summarization, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Patrick Fernandes",
                    "Miltiadis Allamanis",
                    "Marc Brockschmidt"
                ],
                "title": "Structured neural summarization",
                "pub_date": "2019-05-06",
                "pub_title": "7th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_103",
            "content": "Victoria Fossum, Steven Abney, Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora, 2005, Second International Joint Conference on Natural Language Processing: Full Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Victoria Fossum",
                    "Steven Abney"
                ],
                "title": "Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora",
                "pub_date": "2005",
                "pub_title": "Second International Joint Conference on Natural Language Processing: Full Papers",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_104",
            "content": "C Linton,  Freeman, Centrality in social networks conceptual clarification, 1978, Social networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "C Linton",
                    " Freeman"
                ],
                "title": "Centrality in social networks conceptual clarification",
                "pub_date": "1978",
                "pub_title": "Social networks",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_105",
            "content": "Michelle Girvan, E Mark,  Newman, Community structure in social and biological networks, 2002, Proceedings of the national academy of sciences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Michelle Girvan",
                    "E Mark",
                    " Newman"
                ],
                "title": "Community structure in social and biological networks",
                "pub_date": "2002",
                "pub_title": "Proceedings of the national academy of sciences",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_106",
            "content": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, Meng Wang, LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation, 2020, Association for Computing Machinery, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Xiangnan He",
                    "Kuan Deng",
                    "Xiang Wang",
                    "Yan Li",
                    "Yong-Dong Zhang",
                    "Meng Wang"
                ],
                "title": "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                "pub_date": "2020",
                "pub_title": "Association for Computing Machinery",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_107",
            "content": "Lianzhe Huang, Xin Sun, Sujian Li, Linhao Zhang, Houfeng Wang, Syntax-aware graph attention network for aspect-level sentiment classification, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Lianzhe Huang",
                    "Xin Sun",
                    "Sujian Li",
                    "Linhao Zhang",
                    "Houfeng Wang"
                ],
                "title": "Syntax-aware graph attention network for aspect-level sentiment classification",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_108",
            "content": "Matthias Huck, Diana Dutka, Alexander Fraser, Cross-lingual annotation projection is effective for neural part-of-speech tagging, 2019, Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Matthias Huck",
                    "Diana Dutka",
                    "Alexander Fraser"
                ],
                "title": "Cross-lingual annotation projection is effective for neural part-of-speech tagging",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_109",
            "content": "UNKNOWN, None, , Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2021. Graph algorithms for multiparallel word alignment, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2021. Graph algorithms for multiparallel word alignment",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_110",
            "content": "Masoud Jalili Sabet, Philipp Dufter, Fran\u00e7ois Yvon, Hinrich Sch\u00fctze, SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Masoud Jalili Sabet",
                    "Philipp Dufter",
                    "Fran\u00e7ois Yvon",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_111",
            "content": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, Monojit Choudhury, The state and fate of linguistic diversity and inclusion in the nlp world, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Pratik Joshi",
                    "Sebastin Santy",
                    "Amar Budhiraja",
                    "Kalika Bali",
                    "Monojit Choudhury"
                ],
                "title": "The state and fate of linguistic diversity and inclusion in the nlp world",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_112",
            "content": "UNKNOWN, None, 2016, Semisupervised classification with graph convolutional networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Semisupervised classification with graph convolutional networks",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_113",
            "content": "UNKNOWN, None, 2016, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_114",
            "content": "UNKNOWN, None, 2005, Edinburgh system description for the 2005 iwslt speech translation evaluation. International Workshop on Spoken Language Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2005",
                "pub_title": "Edinburgh system description for the 2005 iwslt speech translation evaluation. International Workshop on Spoken Language Translation",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_115",
            "content": "UNKNOWN, None, 2003, Statistical phrase-based translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2003",
                "pub_title": "Statistical phrase-based translation",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_116",
            "content": "Dan Kondratyuk, Milan Straka, 75 languages, 1 model: Parsing Universal Dependencies universally, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Dan Kondratyuk",
                    "Milan Straka"
                ],
                "title": "75 languages, 1 model: Parsing Universal Dependencies universally",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_117",
            "content": "Shankar Kumar, Franz Och, Wolfgang Macherey, Improving word alignment with bridge languages, 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Shankar Kumar",
                    "Franz Och",
                    "Wolfgang Macherey"
                ],
                "title": "Improving word alignment with bridge languages",
                "pub_date": "2007",
                "pub_title": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_118",
            "content": "Omer Levy, Anders S\u00f8gaard, Yoav Goldberg, A strong baseline for learning cross-lingual word embeddings from sentence alignments, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Omer Levy",
                    "Anders S\u00f8gaard",
                    "Yoav Goldberg"
                ],
                "title": "A strong baseline for learning cross-lingual word embeddings from sentence alignments",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_119",
            "content": "D William, Fei Lewis,  Xia, Automatically identifying computationally relevant typological features, 2008, Proceedings of the Third International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "D William",
                    "Fei Lewis",
                    " Xia"
                ],
                "title": "Automatically identifying computationally relevant typological features",
                "pub_date": "2008",
                "pub_title": "Proceedings of the Third International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_120",
            "content": "Percy Liang, Ben Taskar, Dan Klein, Alignment by agreement, 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Percy Liang",
                    "Ben Taskar",
                    "Dan Klein"
                ],
                "title": "Alignment by agreement",
                "pub_date": "2006",
                "pub_title": "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_121",
            "content": "Zhiyuan Liu, Jie Zhou, Introduction to graph neural networks, 2020, Synthesis Lectures on Artificial Intelligence and Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Zhiyuan Liu",
                    "Jie Zhou"
                ],
                "title": "Introduction to graph neural networks",
                "pub_date": "2020",
                "pub_title": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_122",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_123",
            "content": "Thomas Mayer, Michael Cysouw, Language comparison through sparse multilingual word alignment, 2012, Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Thomas Mayer",
                    "Michael Cysouw"
                ],
                "title": "Language comparison through sparse multilingual word alignment",
                "pub_date": "2012",
                "pub_title": "Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_124",
            "content": "Thomas Mayer, Michael Cysouw, Creating a massively parallel bible corpus, 2014, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Thomas Mayer",
                    "Michael Cysouw"
                ],
                "title": "Creating a massively parallel bible corpus",
                "pub_date": "2014",
                "pub_title": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_125",
            "content": "UNKNOWN, None, 1998, Manual annotation of translational equivalence: The blinker project, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": "1998",
                "pub_title": "Manual annotation of translational equivalence: The blinker project",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_126",
            "content": "Preslav Nakov, Hwee Tou Ng, Improving statistical machine translation for a resource-poor language using related resource-rich languages, 2012, Journal of Artificial Intelligence Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Preslav Nakov",
                    "Hwee Tou Ng"
                ],
                "title": "Improving statistical machine translation for a resource-poor language using related resource-rich languages",
                "pub_date": "2012",
                "pub_title": "Journal of Artificial Intelligence Research",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_127",
            "content": "E Mark,  Newman, Scientific collaboration networks. ii. shortest paths, weighted networks, and centrality, 2001, Physical review E, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "E Mark",
                    " Newman"
                ],
                "title": "Scientific collaboration networks. ii. shortest paths, weighted networks, and centrality",
                "pub_date": "2001",
                "pub_title": "Physical review E",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_128",
            "content": "E Mark, Michelle Newman,  Girvan, Finding and evaluating community structure in networks, 2004, Physical review E, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "E Mark",
                    "Michelle Newman",
                    " Girvan"
                ],
                "title": "Finding and evaluating community structure in networks",
                "pub_date": "2004",
                "pub_title": "Physical review E",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_129",
            "content": "Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Jan Haji\u010d, Christopher Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, Daniel Zeman, Universal Dependencies v2: An evergrowing multilingual treebank collection, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Joakim Nivre",
                    "Marie-Catherine De Marneffe",
                    "Filip Ginter",
                    "Jan Haji\u010d",
                    "Christopher Manning",
                    "Sampo Pyysalo",
                    "Sebastian Schuster",
                    "Francis Tyers",
                    "Daniel Zeman"
                ],
                "title": "Universal Dependencies v2: An evergrowing multilingual treebank collection",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 12th Language Resources and Evaluation Conference",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_130",
            "content": "Josef Franz, Hermann Och,  Ney, A systematic comparison of various statistical alignment models, 2003, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Josef Franz",
                    "Hermann Och",
                    " Ney"
                ],
                "title": "A systematic comparison of various statistical alignment models",
                "pub_date": "2003",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_131",
            "content": "Robert \u00d6stling, Bayesian word alignment for massively parallel texts, 2014, Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Robert \u00d6stling"
                ],
                "title": "Bayesian word alignment for massively parallel texts",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_132",
            "content": "Robert \u00d6stling, Word order typology through multilingual word alignment, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Robert \u00d6stling"
                ],
                "title": "Word order typology through multilingual word alignment",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v1_133",
            "content": "Robert \u00d6stling, J\u00f6rg Tiedemann, Efficient word alignment with Markov Chain Monte Carlo, 2016, The Prague Bulletin of Mathematical Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Robert \u00d6stling",
                    "J\u00f6rg Tiedemann"
                ],
                "title": "Efficient word alignment with Markov Chain Monte Carlo",
                "pub_date": "2016",
                "pub_title": "The Prague Bulletin of Mathematical Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_134",
            "content": "Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, Min-Yen Kan, Semantic graphs for generating deep questions, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Liangming Pan",
                    "Yuxi Xie",
                    "Yansong Feng",
                    "Tat-Seng Chua",
                    "Min-Yen Kan"
                ],
                "title": "Semantic graphs for generating deep questions",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_135",
            "content": "Slav Petrov, Dipanjan Das, Ryan Mcdonald, A universal part-of-speech tagset, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Slav Petrov",
                    "Dipanjan Das",
                    "Ryan Mcdonald"
                ],
                "title": "A universal part-of-speech tagset",
                "pub_date": "2012",
                "pub_title": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_136",
            "content": "Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia, Graph networks as learnable physics engines for inference and control, 2018, Proceedings of the 35th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Alvaro Sanchez-Gonzalez",
                    "Nicolas Heess",
                    "Jost Springenberg",
                    "Josh Merel",
                    "Martin Riedmiller",
                    "Raia Hadsell",
                    "Peter Battaglia"
                ],
                "title": "Graph networks as learnable physics engines for inference and control",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 35th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "184-ARR_v1_137",
            "content": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, Gabriele Monfardini, The graph neural network model, 2009, IEEE Transactions on Neural Networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Franco Scarselli",
                    "Marco Gori",
                    "Ah Chung Tsoi",
                    "Markus Hagenbuchner",
                    "Gabriele Monfardini"
                ],
                "title": "The graph neural network model",
                "pub_date": "2009",
                "pub_title": "IEEE Transactions on Neural Networks",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_138",
            "content": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, Graph attention networks, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Petar Veli\u010dkovi\u0107",
                    "Guillem Cucurull",
                    "Arantxa Casanova",
                    "Adriana Romero",
                    "Pietro Li\u00f2",
                    "Yoshua Bengio"
                ],
                "title": "Graph attention networks",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_139",
            "content": "Guillaume Wisniewski, Nicolas P\u00e9cheux, Souhir Gahbiche-Braham, Fran\u00e7ois Yvon, Crosslingual part-of-speech tagging through ambiguous learning, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Guillaume Wisniewski",
                    "Nicolas P\u00e9cheux",
                    "Souhir Gahbiche-Braham",
                    "Fran\u00e7ois Yvon"
                ],
                "title": "Crosslingual part-of-speech tagging through ambiguous learning",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_140",
            "content": "Yongji Wu, Defu Lian, Yiheng Xu, Le Wu, Enhong Chen, Graph convolutional networks with markov random field reasoning for social spammer detection, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": [
                    "Yongji Wu",
                    "Defu Lian",
                    "Yiheng Xu",
                    "Le Wu",
                    "Enhong Chen"
                ],
                "title": "Graph convolutional networks with markov random field reasoning for social spammer detection",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_141",
            "content": "David Yarowsky, Grace Ngai, Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora, 2001, Second Meeting of the North American Chapter of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": [
                    "David Yarowsky",
                    "Grace Ngai"
                ],
                "title": "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora",
                "pub_date": "2001",
                "pub_title": "Second Meeting of the North American Chapter of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v1_142",
            "content": "Anssi Yli-Jyr\u00e4, Josi Purhonen, Matti Liljeqvist, Arto Antturi, Pekka Nieminen, Kari R\u00e4ntil\u00e4, Valtter Luoto, HELFI: a Hebrew-Greek-Finnish parallel Bible corpus with cross-lingual morpheme alignment, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": [
                    "Anssi Yli-Jyr\u00e4",
                    "Josi Purhonen",
                    "Matti Liljeqvist",
                    "Arto Antturi",
                    "Pekka Nieminen",
                    "Kari R\u00e4ntil\u00e4",
                    "Valtter Luoto"
                ],
                "title": "HELFI: a Hebrew-Greek-Finnish parallel Bible corpus with cross-lingual morpheme alignment",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 12th Language Resources and Evaluation Conference",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "184-ARR_v1_0@0",
            "content": "Graph Neural Networks for Multiparallel Word Alignment",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_0",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_2@0",
            "content": "After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_2",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_2@1",
            "content": "Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_2",
            "start": 199,
            "end": 323,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_2@2",
            "content": "Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_2",
            "start": 325,
            "end": 445,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_2@3",
            "content": "First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_2",
            "start": 447,
            "end": 557,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_2@4",
            "content": "Next, we use graph neural networks (GNNs) and community detection algorithms to exploit the graph structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_2",
            "start": 559,
            "end": 666,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_2@5",
            "content": "Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) provides a prediction model that can generalize beyond the sentences it is trained on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_2",
            "start": 668,
            "end": 987,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_2@6",
            "content": "We show that community detection provides valuable information for multiparallel word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_2",
            "start": 989,
            "end": 1084,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_2@7",
            "content": "Our method outperforms previous work on three word alignment datasets and on a downstream task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_2",
            "start": 1086,
            "end": 1180,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_4@0",
            "content": "Word alignments are crucial for statistical machine translation (Koehn et al., 2003) and useful for many other multilingual tasks such as neural machine translation (Alkhouli and Ney, 2017;Alkhouli et al., 2016), typological analysis (Lewis and Xia, 2008;\u00d6stling, 2015;Asgari and Sch\u00fctze, 2017), annotation projection (Yarowsky and Ngai, 2001;Fossum and Abney, 2005;Wisniewski et al., 2014;Huck et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_4",
            "start": 0,
            "end": 408,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_5@0",
            "content": "The rise of deep learning initially led to a temporary plateau, but interest in word alignments is now increasing, demonstrated by several recent publications (Jalili Sabet et al., 2020;Dou and Neubig, 2021) Colors represent languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_5",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_5@1",
            "content": "Each English (yellow) node is annotated with its word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_5",
            "start": 236,
            "end": 289,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_5@2",
            "content": "Red dashed lines sever links that incorrectly connect distinct concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_5",
            "start": 291,
            "end": 362,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_5@3",
            "content": "We exploit community detection algorithms to detect distinct concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_5",
            "start": 364,
            "end": 433,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_5@4",
            "content": "This provides valuable information for our GNN model and improves word alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_5",
            "start": 435,
            "end": 516,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_6@0",
            "content": "Multiparallel corpora contain sentence level parallel text in more than two languages, e.g., JW300 (Agi\u0107 and Vuli\u0107, 2019), PBC (Mayer and Cysouw, 2014) and Tatoeba.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_6",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_6@1",
            "content": "1 While the amount of data provided by multiparallel corpora is less than bilingual corpora, this type of corpus is essential to study very low-resource languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_6",
            "start": 165,
            "end": 327,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_6@2",
            "content": "There are thousands of languages in the world a very small portion of which is covered by language technologies (Joshi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_6",
            "start": 329,
            "end": 461,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_6@3",
            "content": "Recent work (Bird, 2020) suggests a number of approaches to develop technologies for indigenous languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_6",
            "start": 463,
            "end": 568,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_6@4",
            "content": "Multiparallel corpora are a valuable (and arguably complementary) resource for this aim.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_6",
            "start": 570,
            "end": 657,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_6@5",
            "content": "We use the PBC corpus since it covers more than 1300 languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_6",
            "start": 659,
            "end": 721,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_7@0",
            "content": "Most prior work on word alignment uses bitext, with one notable exception: (Imani et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_7",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_8@0",
            "content": "They introduce MPWA (MultiParallel Word Alignment), a framework that utilizes the synergy between multiple language pairs to improve bilingual word alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_8",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_8@1",
            "content": "The rationale is that some of the missing alignment edges between a source and a target language can be recovered using their alignments with words in other languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_8",
            "start": 160,
            "end": 326,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_9@0",
            "content": "The first step in MPWA is to create bilingual alignments for all language pairs in a multiparallel corpus using a bilingual word aligner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_9",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_9@1",
            "content": "Then the bilingual alignments for a multiparallel sentence are represented as a graph where words are nodes and initial word alignments are edges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_9",
            "start": 138,
            "end": 283,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_9@2",
            "content": "Figure 1 gives an example: a multiparallel alignment graph for a 12-way multiparallel corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_9",
            "start": 285,
            "end": 377,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_9@3",
            "content": "MPWA infers missing alignment links based on the graph structure in a postprocessing step, casting the word alignment task as an edge prediction problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_9",
            "start": 379,
            "end": 531,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_9@4",
            "content": "They use two traditional graph algorithms, Adamic-Adar and non-negative matrix factorization, for edge prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_9",
            "start": 533,
            "end": 646,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_9@5",
            "content": "However, these standard graph algorithms are applied to individual multiparallel sentences independently and therefore cannot accumulate knowledge from multiple sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_9",
            "start": 648,
            "end": 818,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_9@6",
            "content": "Moreover, their edge predictions are solely based on the structure of the graph and do not take advantage of other beneficial signals such as a word's language, relative position and word meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_9",
            "start": 820,
            "end": 1015,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_9@7",
            "content": "Another limitation is that it only adds links and does not remove any, which is important to improve precision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_9",
            "start": 1017,
            "end": 1127,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_10@0",
            "content": "In this paper, we propose to use graph neural networks (GNNs) to exploit the graph structure of multiparallel word alignments and address the limitations of prior work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_10",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_10@1",
            "content": "GNNs were proposed to extend the powerful current generation of neural network models to processing graph-structured data (Scarselli et al., 2009) and they have gained increasing popularity in many domains (Wu et al., 2020;Sanchez-Gonzalez et al., 2018;He et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_10",
            "start": 169,
            "end": 438,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_10@2",
            "content": "In contrast to other graph algorithms, GNNs can incorporate heterogeneous sources of signal in the form of node and edge features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_10",
            "start": 440,
            "end": 569,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_11@0",
            "content": "Since the nodes in the graph are words that are translations of each other, we expect them to create densely connected regions or communities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_11",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_11@1",
            "content": "Our analysis of the structure of the multiparallel alignment graph confirms this intuition; see Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_11",
            "start": 143,
            "end": 247,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_11@2",
            "content": "We use community detection algorithms to find communities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_11",
            "start": 249,
            "end": 306,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_11@3",
            "content": "We show that pruning inter-community edges and adding intracommunity edges is helpful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_11",
            "start": 308,
            "end": 393,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_11@4",
            "content": "We use community information as node features for our GNN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_11",
            "start": 395,
            "end": 452,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_12@0",
            "content": "We enable the removal of alignment edges from initial alignments by inferring alignments from the alignment probability matrix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_12",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_12@1",
            "content": "Our method predicts new alignment links independently of initial edges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_12",
            "start": 128,
            "end": 198,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_12@2",
            "content": "Therefore it is not limited to adding edges wrt initial bilingual alignments, it can also remove them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_12",
            "start": 200,
            "end": 301,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_13@0",
            "content": "For our experiments, we follow the setup of Imani et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_13",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_13@1",
            "content": "We train a GNN model with a link prediction objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_13",
            "start": 65,
            "end": 118,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_13@2",
            "content": "We show improved results for three language pairs on word alignment (English-French, Finnish-Hebrew and Finnish-Greek).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_13",
            "start": 120,
            "end": 238,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_13@3",
            "content": "As a demonstration of the importance of high-quality alignments, we use our word alignments to project annotations from highresource to low-resource languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_13",
            "start": 240,
            "end": 398,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_13@4",
            "content": "We improve a part-of-speech tagger for Yoruba by training it over a high-quality dataset, which is created using annotation projection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_13",
            "start": 400,
            "end": 534,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_13@5",
            "content": "We show that our model is especially helpful for distant languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_13",
            "start": 536,
            "end": 602,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_14@0",
            "content": "Contributions: i) We propose a graph neural network model that incorporates a diverse set of features for word alignments in multiparallel corpora and an elegant way of training it efficiently and effectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_14",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_14@1",
            "content": "ii) We show that community detection improves multiparallel word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_14",
            "start": 210,
            "end": 284,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_14@2",
            "content": "iii) We show that the improved alignments improve performance on a downstream task for a low resource language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_14",
            "start": 286,
            "end": 396,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_14@3",
            "content": "iv) We propose a new method to infer alignments from the alignment probability matrix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_14",
            "start": 398,
            "end": 483,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_14@4",
            "content": "v) We will make our code publicly available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_14",
            "start": 485,
            "end": 528,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_15@0",
            "content": "Graph Analysis with Community Detection (CD)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_15",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_16@0",
            "content": "The nodes in the alignment graph are words that are translations of each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_16",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_16@1",
            "content": "If the initial bilingual alignments are of good quality, we expect these translated words to form densely connected regions or communities; see Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_16",
            "start": 80,
            "end": 232,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_16@2",
            "content": "We expect these communities to be genarally disconnected, each corresponding to a distinct connected component.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_16",
            "start": 234,
            "end": 344,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_17@0",
            "content": "In other words, ideally, words representing a concept should be densely connected, but there should be no links between different concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_17",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_17@1",
            "content": "Clearly, this intuition will not be true for all concepts between all possible language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_17",
            "start": 140,
            "end": 233,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_17@2",
            "content": "Nonetheless, we hypothesize that identifying distinct concepts in a multiparallel word alignment graph can provide useful information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_17",
            "start": 235,
            "end": 368,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_18@0",
            "content": "To examine to what extent this expectation is met, we count the components in the original Eflomal-generated (\u00d6stling and Tiedemann, 2016) graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_18",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_18@1",
            "content": "Table 1 shows that the average number of components per sentence is less than three (\"Eflomal intersection\", columns #CC).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_18",
            "start": 146,
            "end": 267,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_18@2",
            "content": "But intuitively, the number of components should roughly correspond to sentence length (i.e., the number of content words).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_18",
            "start": 269,
            "end": 391,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_18@3",
            "content": "This indicates that there are many links that incorrectly connect different concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_18",
            "start": 393,
            "end": 477,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_18@4",
            "content": "To detect such links, we use community detection (CD) algorithms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_18",
            "start": 479,
            "end": 543,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_19@0",
            "content": "CD algorithms find subnetworks of nodes that form tightly knit groups that are only loosely connected with a small number of links (Girvan and Newman, 2002).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_19",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_19@1",
            "content": "CD algorithms maximize the modularity measure (Newman and Girvan, 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_19",
            "start": 158,
            "end": 229,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_19@2",
            "content": "Modularity measures how beneficial a division of a community into two communities is, in the sense that there are many links within communities and only a few between them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_19",
            "start": 231,
            "end": 402,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_19@3",
            "content": "Given a graph G with n nodes and m edges and G's adjacency matrix A \u2208 R n\u00d7n , modularity is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_19",
            "start": 404,
            "end": 506,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_20@0",
            "content": "mod = 1 2m ij A ij \u2212 \u03b3 d i d j 2m I(c i , c j ) (1) d i is the degree of node i. I(c i , c j ) is 1 if nodes i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_20",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_21@0",
            "content": "and j are in the same community, 0 otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_21",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_21@1",
            "content": "We experiment with two CD algorithms:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_21",
            "start": 46,
            "end": 82,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_22@0",
            "content": "\u2022 Greedy modularity communities (GMC). This method uses Clauset-Newman-Moore greedy modularity maximization (Clauset et al., 2004). GMC begins with each node in its own community and greedily joins the pair of communities that most increases modularity until no such pair exists. \u2022 Label propagation communities (LPC). This method finds communities in a graph using label propagation (Cordasco and Gargano, 2010). It begins by giving a label to each node of the network. Then each node's label is updated by the most frequent label among its neighbors in each iteration. It performs label propagation on a portion of nodes at each step and quickly converges to a stable labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_22",
            "start": 0,
            "end": 678,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_23@0",
            "content": "After detecting communities, we link all nodes inside a community and remove all intercommunity links.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_23",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_23@1",
            "content": "GMC (LPC) on average removes 3% (7%) of the edges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_23",
            "start": 103,
            "end": 152,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_23@2",
            "content": "Table 1 reports the average number of graph components per sentence before and after runing GMC and LPC, as well as the corresponding F 1 for word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_23",
            "start": 154,
            "end": 310,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_23@3",
            "content": "We see that the number of communities found is lower for GMC than for LPC; therefore, LPC identifies more candidate links for deletion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_23",
            "start": 312,
            "end": 446,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_23@4",
            "content": "2 Comparing the number of communities detected with the average sentence length, GMC seems to have failed to detect enough communities to split different concepts properly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_23",
            "start": 448,
            "end": 619,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_24@0",
            "content": "The F 1 scores confirm this observation and show that LPC performs well at detecting the communities we are looking for.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_24",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_25@0",
            "content": "These results indicate that CD algorithms can provide valuable information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_25",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_25@1",
            "content": "To exploit this in our GNN model, we add a node's community information as a GNN node feature; see \u00a73.1.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_25",
            "start": 76,
            "end": 181,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_26@0",
            "content": "Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_26",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_27@0",
            "content": "GNN in MPWA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_27",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_28@0",
            "content": "GNNs can be used in transductive or inductive settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_28",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_28@1",
            "content": "Transductively, the final model can only be used for inference over the same graph that it is trained on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_28",
            "start": 56,
            "end": 160,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_28@2",
            "content": "In an inductive setting, which we use here, nodes are represented as feature vectors, and the final model has the advantage of being applicable to a different graph in inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_28",
            "start": 162,
            "end": 339,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_29@0",
            "content": "Model Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_29",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_30@0",
            "content": "Our model is inspired by the Graph Auto Encoder (GAE) model of Kipf and Welling (2016b) for link prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_30",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_30@1",
            "content": "The architecture consists of an encoder and a decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_30",
            "start": 109,
            "end": 162,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_30@2",
            "content": "We make changes to this model to improve the model's quality and reduce its computation cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_30",
            "start": 164,
            "end": 256,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_30@3",
            "content": "We use GATConv layers (Veli\u010dkovi\u0107 et al., 2018) for encoder instead of GCNConv (Kipf and Welling, 2016a)and a more sophisticated decoder instead of simple dot product for a stronger model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_30",
            "start": 258,
            "end": 445,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_30@4",
            "content": "We also introduce a more efficient training procedure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_30",
            "start": 447,
            "end": 500,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_31@0",
            "content": "The encoder is a graph attention network (GAT) (Veli\u010dkovi\u0107 et al., 2018) with two GATConv layers followed by a fully connected layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_31",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_31@1",
            "content": "Layers are connected by RELU non-linearities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_31",
            "start": 134,
            "end": 178,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_31@2",
            "content": "A GATConv layer computes its output x i for a node i from its input x i as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_31",
            "start": 180,
            "end": 253,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_32@0",
            "content": "x i = \u03b1 i,i Wx i + j\u2208N (i) \u03b1 i,j Wx j ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_32",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_33@0",
            "content": "where W is a weight matrix, N (i) is some neighborhood of node i in the graph, and \u03b1 i,j is the attention coefficient indicating the importance of node j's features to node i. \u03b1 i,j is computed as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_33",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_34@0",
            "content": "\u03b1 i,j = exp g a [Wx i Wx j ] k\u2208N (i)\u222a{i} exp (g (a [Wx i Wx k ]))",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_34",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_35@0",
            "content": "(3) where is concatanation, g is LeakyReLU, and a is a weight vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_35",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_35@1",
            "content": "Given the features for the nodes and their alignment edges, the encoder creates a contextualized hidden representation for each node.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_35",
            "start": 70,
            "end": 202,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_36@0",
            "content": "Based on the hidden representations of two nodes, the decoder predicts whether a link connects them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_36",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_36@1",
            "content": "The decoder architecture consists of a fully connected layer, a RELU non-linearity and a sigmoid layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_36",
            "start": 101,
            "end": 203,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@0",
            "content": "Training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@1",
            "content": "By default, GAE models are trained using full batches with random negative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 10,
            "end": 92,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@2",
            "content": "This approach requires at least tens of epochs over training dataset to converge and a lot of GPU memory for graphs as big as ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 94,
            "end": 224,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@3",
            "content": "We train our model using mini-batches and an adversarial loss to decrease memory requirements and improve the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 226,
            "end": 347,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@4",
            "content": "Using our training approach the model converges after one epoch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 349,
            "end": 412,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@5",
            "content": "The negative samples are selected more elegantly, as described below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 414,
            "end": 482,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@6",
            "content": "Figure 2 displays our GNN model and the training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 484,
            "end": 540,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@7",
            "content": "The outer loop iterates over the multiparallel sentences in the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 542,
            "end": 618,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@8",
            "content": "The training set contains one graph for each sentence; the graph is constructed using the bilingual alignment edges between all language pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 620,
            "end": 762,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@9",
            "content": "Each graph is divided into multiple batches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 764,
            "end": 807,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@10",
            "content": "Each batch contains a random subset of the graph's edges as positive samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 809,
            "end": 885,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@11",
            "content": "The negative samples are created as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 887,
            "end": 930,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_37@12",
            "content": "Given a sentence u 1 u 2 . . . u n in language U and its translation v 1 v 2 . . . v m in language V , for each alignment edge u i :v j in the current batch, two negative edges u i :v j and u i :v j (j = j, i = i) are randomly sampled.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_37",
            "start": 932,
            "end": 1166,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_38@0",
            "content": "For each training batch, the encoder takes the batch's whole graph (i.e., node features for all graph nodes and all graph edges) as input and computes hidden representations for the nodes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_38",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_38@1",
            "content": "On the decoder side, for each link of the batch, the hidden representations of the attached nodes are concatenated to create the decoder's input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_38",
            "start": 189,
            "end": 333,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_38@2",
            "content": "The decoder's target is the link's class: 1 (resp. 0) for positive (resp. negative) links.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_38",
            "start": 335,
            "end": 424,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_38@3",
            "content": "We train with a binary classification objective:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_38",
            "start": 426,
            "end": 473,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_39@0",
            "content": "L = \u2212 1 b b i=1 log(p + i ) + 1 2b 2b i=1 log(p \u2212 i ) (4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_39",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_40@0",
            "content": "where b is the batch size and p + i and p \u2212 i are the model predictions for the i th positive and negative samples within the batch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_40",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_40@1",
            "content": "Parameters of the encoder and decoder as well as the node-embedding feature layer are updated after each training step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_40",
            "start": 133,
            "end": 251,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_41@0",
            "content": "Node Features",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_41",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_42@0",
            "content": "We use three main types of node features: (i) graph structural features, (ii) community-based features and (iii) word content features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_42",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@0",
            "content": "Graph structural features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@1",
            "content": "We use degree, closeness (Freeman, 1978) , betweenness (Brandes, 2001) , load (Newman, 2001) and harmonic centrality (Boldi and Vigna, 2014) features as additional information about the graph structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 27,
            "end": 228,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@2",
            "content": "These features are continuous numbers, providing information about the position and connectivity of the nodes within the graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 230,
            "end": 356,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@3",
            "content": "We standardize (i.e., zscore) each feature across all nodes, and train an embedding of size four for each feature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 358,
            "end": 471,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@4",
            "content": "3 Community-based features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 473,
            "end": 499,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@5",
            "content": "One way to incorporate community information into our model is to train the model based on the refined edges after the community detection step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 501,
            "end": 644,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@6",
            "content": "This approach hobbles the GNN model by making decisions about many of the edges before the GNN gets to see them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 646,
            "end": 757,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@7",
            "content": "Our initial experiments also confirmed that training the GNN over CD refined edges does not help.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 759,
            "end": 855,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@8",
            "content": "Therefore, we add community information as node features and let the GNN use them to improve its decisions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 857,
            "end": 963,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@9",
            "content": "We use the community detection algorithms GMC and LPC (see \u00a72) to identify communities in the graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 965,
            "end": 1064,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_43@10",
            "content": "Then we take the community membership information of the nodes as one-hot vectors and learn an embedding of size 32 for each of the two algorithms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_43",
            "start": 1066,
            "end": 1212,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_44@0",
            "content": "Word content features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_44",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_44@1",
            "content": "We train embeddings for word position (size 32) and word language (size 20).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_44",
            "start": 23,
            "end": 98,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_44@2",
            "content": "We learn 100-dimensional multilingual word embeddings using Levy et al. (2017)'s sentence-ID method on the 84 PBC languages selected by Imani et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_44",
            "start": 100,
            "end": 255,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_44@3",
            "content": "Word embeddings serve as initialization and are updated during GNN training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_44",
            "start": 257,
            "end": 332,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_45@0",
            "content": "After concatenating these features, each node is represented by a 236 dimensional vector that is then fed to the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_45",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_46@0",
            "content": "Inducing Alignment Edges",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_46",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_47@0",
            "content": "When our trained GNN model is used to predict alignment edges between a source sentence x = x 1 , x 2 , . . . , x m in language X and a target sentence \u0177 = y 1 , y 2 , . . . , y l in language Y , it produces a symmetric alignment probability matrix S 4 of size m \u00d7 l where S ij is the predicted alignment probability between words x i and y j .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_47",
            "start": 0,
            "end": 343,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_47@1",
            "content": "Using these values directly to infer alignment edges is usually suboptimal; therefore, more sophisticated methods have been suggested (Ayan and Dorr, 2006;Liang et al., 2006).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_47",
            "start": 345,
            "end": 519,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_47@2",
            "content": "Here we propose a new approach: it combines Koehn et al. (2005)'s Grow-Diag-Final-And (GDFA) with Dou and Neubig (2021)'s probability thresholding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_47",
            "start": 521,
            "end": 667,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_47@3",
            "content": "We modify the latter to account for the variable size of the probability matrix (i.e., length of source/target sentences).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_47",
            "start": 669,
            "end": 790,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_47@4",
            "content": "Our method is not limited to adding new edges to some initial bilingual alignments, a limitation of prior work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_47",
            "start": 792,
            "end": 902,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_47@5",
            "content": "As we predict each edge independently, some initial links can be discarded from the final alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_47",
            "start": 904,
            "end": 1003,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_48@0",
            "content": "We start by creating a set of forward (sourceto-target) alignment edges and a set of backward (target-to-source) alignment edges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_48",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_48@1",
            "content": "To this end, first, inspired by probability thresholding (Dou and Neubig, 2021), we apply softmax to S, and zero out probabilities below a threshold to get a sourceto-target probability matrix S XY :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_48",
            "start": 130,
            "end": 328,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_49@0",
            "content": "S XY = S * (softmax(S) > \u03b1 l )(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_49",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_50@0",
            "content": "Analogously, we compute the target-to-source probability matrix S Y X :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_50",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_51@0",
            "content": "S Y X = S * (softmax(S ) > \u03b1 m )(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_51",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_52@0",
            "content": "where \u03b1 is a sensitivity hyperparameter, e.g., \u03b1 = 1 means that we pick edges with a probability higher than average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_52",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_52@1",
            "content": "We experimentally set \u03b1 = 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_52",
            "start": 118,
            "end": 145,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_52@2",
            "content": "Next, from each row of S XY (S Y X ), we pick the cell with the highest value (if any exists) and add this edge to the forward (backward) set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_52",
            "start": 147,
            "end": 288,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_52@3",
            "content": "We create the final set of alignment edges by applying the GDFA symmetrization method (Koehn et al., 2005) to forward and backward sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_52",
            "start": 290,
            "end": 425,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_52@4",
            "content": "The gist of GDFA is to use the intersection of forward and backward as initial alignment edges and add more edges from the union of forward and backward based on a number of heuristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_52",
            "start": 427,
            "end": 611,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_52@5",
            "content": "We call this method TGDFA (Thresholding GDFA).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_52",
            "start": 613,
            "end": 658,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_53@0",
            "content": "We also experiment with combining TGDFA with the original bilingual GDFA alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_53",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_53@1",
            "content": "We do so by adding bilingual GDFA edges to the union of forward and backward before performing the GDFA heuristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_53",
            "start": 85,
            "end": 199,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_53@2",
            "content": "We refer to these alignments as TGDFA+orig.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_53",
            "start": 201,
            "end": 243,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_54@0",
            "content": "We evaluate the resulting alignments using F 1 score and alignment error rate (AER), the standard evaluation measures in the word alignment literature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_54",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_55@0",
            "content": "Annotation Projection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_55",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_56@0",
            "content": "Annotation projection automatically creates linguistically annotated corpora for low-resource languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_56",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_56@1",
            "content": "A model trained on data with \"annotationprojected\" labels can perform better than full unsupervision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_56",
            "start": 105,
            "end": 205,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_56@2",
            "content": "Here, we focus on universal part-ofspeech (UPOS) tagging (Petrov et al., 2012) for the low resource target language Yoruba; this language only has a small set of annotated sentences in Universal Dependencies (Nivre et al., 2020) and has poor POS results in unsupervised settings (Kondratyuk and Straka, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_56",
            "start": 207,
            "end": 515,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_57@0",
            "content": "The quality of the target annotated corpus depends on the quality of the annotations in the source languages and the quality of the word alignments between sources and target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_57",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_57@1",
            "content": "We use the Flair (Akbik et al., 2019) POS taggers for three high resource languages, English, German and French (Akbik et al., 2018), to annotate 30K verses whose Yoruba translations are available in PBC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_57",
            "start": 176,
            "end": 379,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_57@2",
            "content": "We then transfer the POS tags from source to target using three different approaches: (i) We directly transfer annotations from English to the target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_57",
            "start": 381,
            "end": 530,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_57@3",
            "content": "(ii) For each word in the target, we get its alignments in the three source languages and predict the majority POS to annotate the target word. (iii) We repeat (ii) using alignments from our GNN (TGDFA) model instead of the original bilingual alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_57",
            "start": 532,
            "end": 785,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_57@4",
            "content": "In all three approaches, we discard any target sentence from the POS tagger training data if more than 50% of its words are annotated with the \"X\" (other) tag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_57",
            "start": 787,
            "end": 945,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_58@0",
            "content": "We train a Flair SequenceTagger model on the target annotated data using mBERT embeddings (Devlin et al., 2019) and evaluate on Yoruba test from Universal Dependencies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_58",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_58@1",
            "content": "5 Evaluation data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_58",
            "start": 169,
            "end": 186,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_58@2",
            "content": "For our main evaluation, we use the two word alignment gold datasets for PBC published by Imani et al. (2021): Blinker (Melamed, 1998) and HELFI (Yli-Jyr\u00e4 et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_58",
            "start": 188,
            "end": 356,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_58@3",
            "content": "The HELFI dataset contains the Hebrew Bible, Greek New Testament and their translations into Finnish.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_58",
            "start": 358,
            "end": 458,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_58@4",
            "content": "For HELFI, we use Imani et al. (2021)'s train/dev/test splits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_58",
            "start": 460,
            "end": 521,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_58@5",
            "content": "The Blinker dataset provides word level alignments between English and French for 250 Bible verses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_58",
            "start": 523,
            "end": 621,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_59@0",
            "content": "Training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_59",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_59@1",
            "content": "The graph algorithms used by Imani et al. (2021) operate on each multiparallel sentence separately.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_59",
            "start": 15,
            "end": 113,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_59@2",
            "content": "In contrast, our approach allows for an inductive setting where a model is trained on a training set and then evaluated on a separate test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_59",
            "start": 115,
            "end": 257,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_59@3",
            "content": "We combine the verses in the training sets of Finnish-Hebrew and Finnish-Greek for a combined train set size of 24,159.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_59",
            "start": 259,
            "end": 377,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_60@0",
            "content": "Initial Word Alignments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_60",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_61@0",
            "content": "We use the Eflomal statistical word aligner to obtain bilingual alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_61",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_61@1",
            "content": "We train it for every language pair in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_61",
            "start": 76,
            "end": 130,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_61@2",
            "content": "We do not consider SimAlign (Jalili Sabet et al., 2020) since it is shown to perform poorly for languages whose representations in the multilingual pretrained language model are of low quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_61",
            "start": 132,
            "end": 324,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_61@3",
            "content": "We use Eflomal asymmetrical alignments post-processed with the intersection heuristic to get high precision bilingual alignments as input to the GNN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_61",
            "start": 326,
            "end": 474,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_61@4",
            "content": "We use the same subset of 84 languages as Imani et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_61",
            "start": 476,
            "end": 537,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_62@0",
            "content": "Training Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_62",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_63@0",
            "content": "We use PyTorch Geometric 6 to construct and train the GNN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_63",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_63@1",
            "content": "The model's hidden layer size is 512 for both GATConv and Linear layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_63",
            "start": 59,
            "end": 130,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_63@2",
            "content": "We train for one epoch on the train set -a small portion of the train set is enough to learn good embeddings (see \u00a75.1.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_63",
            "start": 132,
            "end": 253,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_63@3",
            "content": "For training, we use a batch size of 400 and learning rate of .001 with AdamW (Loshchilov and Hutter, 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_63",
            "start": 255,
            "end": 362,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_63@4",
            "content": "The whole training process takes less than 4 hours on a GeForce GTX 1080 Ti and the inference time is on the order of milliseconds per sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_63",
            "start": 364,
            "end": 507,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_64@0",
            "content": "Experiments and Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_64",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_65@0",
            "content": "Multiparallel corpus results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_65",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_66@0",
            "content": "Table 2 shows results on Blinker and HELFI for our GNNs and the baselines: bilingual alignments and the traditional graph algorithms WAdAd and NMF from Imani et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_66",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_66@1",
            "content": "Our GNNs provide a better trade-off between precision and recall, most likely thanks to their ability to remove edges, and achieve the best F 1 and AER on all three datasets, outperforming WAdAd and NMF.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_66",
            "start": 173,
            "end": 375,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_67@0",
            "content": "GNN (TGDFA) achieves the best results on HELFI (FIN-HEB, FIN-GRC) while GNN (TGDFA+orig) is best on Blinker (ENG-FRA).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_67",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_67@1",
            "content": "As argued in Imani et al. (2021), this is mostly due to the different ways these two datasets were annotated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_67",
            "start": 119,
            "end": 227,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_67@2",
            "content": "Most HELFI alignments are one-to-one, while many Blinker alignments are many-to-many: phrase-level alignments where every word in a source phrase is aligned with every word in a target phrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_67",
            "start": 229,
            "end": 420,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_67@3",
            "content": "This suggests that one can choose between GNN (TGDFA) and GNN (TGDFA+orig) based on the characteristics of the desired alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_67",
            "start": 422,
            "end": 551,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_68@0",
            "content": "Effect of Training Set Size",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_68",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_69@0",
            "content": "To investigate the effect of training set size, we train the GNN on subsets of our training data with increasing sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_69",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_69@1",
            "content": "Figure 3 shows results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_69",
            "start": 120,
            "end": 142,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_69@2",
            "content": "Performance improves fast until around 2,000 verses; then it stays mostly constant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_69",
            "start": 144,
            "end": 226,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_69@3",
            "content": "Indeed, using more than 6,400 samples does not change the performance at all.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_69",
            "start": 228,
            "end": 304,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_69@4",
            "content": "Therefore, in the other experiments we use 6,400 randomly sampled verses from the training set to train GNNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_69",
            "start": 306,
            "end": 414,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_70@0",
            "content": "Ablation Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_70",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_71@0",
            "content": "To examine the importance of node features, we ablate language, position, centrality, community 6 pytorch-geometric.readthedocs.io 3 shows that removal of graph structural features drastically reduces performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_71",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_71@1",
            "content": "Community features and language information are also important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_71",
            "start": 214,
            "end": 276,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_71@2",
            "content": "Removal of word position information and word embeddings -which store semantic information about wordshas the least effect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_71",
            "start": 278,
            "end": 400,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_71@3",
            "content": "Based on these results, it can be argued that the lexical information contained in the initial alignments and in the community features provides a strong signal regarding word relatedness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_71",
            "start": 402,
            "end": 589,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_71@4",
            "content": "The novel information that is crucial is about the overall graph structure which goes beyond the local word associations that are captured by word position and word embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_71",
            "start": 591,
            "end": 766,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_72@0",
            "content": "Effect of Word Frequency",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_72",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_73@0",
            "content": "We investigate the effect of word frequency on alignment performance where frequency is calculated based on the source word in the PBC; the first bin has the highest frequency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_73",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_73@1",
            "content": "Figure 4 shows that the performance of Eflomal drops with frequency and it struggles to align very rare words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_73",
            "start": 177,
            "end": 286,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_73@2",
            "content": "In contrast, GNN is not affected by word frequency as severely and its performance gains are even greater for rare words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_73",
            "start": 288,
            "end": 408,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_73@3",
            "content": "WAdad which is the multilingual baseline from (Imani et al., 2021) has the same trend as GNN method, but GNN is more robust.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_73",
            "start": 410,
            "end": 533,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_74@0",
            "content": "Annotation Projection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_74",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@0",
            "content": "Table 4 presents accuracies for POS tagging in Yoruba.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@1",
            "content": "Unsupervised baseline performance is 50.86%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 55,
            "end": 98,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@2",
            "content": "Supervised training using pseudo-labels mostly outperforms the unsupervised baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 100,
            "end": 184,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@3",
            "content": "Projecting the majority POS labels to Yoruba improves over projecting English labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 186,
            "end": 270,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@4",
            "content": "Using the GNN model to project labels works best and outperforms , 2003), fast-align (Dyer et al., 2013) and Eflomal (\u00d6stling and Tiedemann, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 272,
            "end": 418,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@5",
            "content": "More recent work, including SimAlign (Jalili Sabet et al., 2020) and SHIFT-ATT/SHIFT-AET , uses pretrained neural language and machine translation models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 420,
            "end": 573,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@6",
            "content": "Although neural models achieve superior performance compared to statistical aligners, they are only applicable for less than two hundred high-resource languages that are supported by multilingual language models like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 575,
            "end": 851,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@7",
            "content": "This makes statistical models the only option for the majority of the world's languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 853,
            "end": 940,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@8",
            "content": "Multiparallel Corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 942,
            "end": 963,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@9",
            "content": "Prior applications of using multiparallel corpora include reliable translations from small datasets (Cohn and Lapata, 2007), and phrase-based machine translation (PBMT) (Kumar et al., 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 965,
            "end": 1154,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_75@10",
            "content": "Multiparallel corpora are also used for language comparison (Mayer and Cysouw, 2012), typological studies (\u00d6stling, 2015;Asgari and Sch\u00fctze, 2017) and PBMT (Nakov and Ng, 2012;Bertoldi et al., 2008;Dyer et al., 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_75",
            "start": 1156,
            "end": 1372,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_76@0",
            "content": "To the best of our knowledge \u00d6stling (2014) 7 is the only word alignment method designed for multiparallel corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_76",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_76@1",
            "content": "However, this method is outperformed by Eflomal (\u00d6stling and Tiedemann, 2016), a \"biparallel\" method from the same author.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_76",
            "start": 116,
            "end": 237,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_76@2",
            "content": "Recently, Imani et al. (2021) proposed MPWA, which we use as our baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_76",
            "start": 239,
            "end": 312,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_77@0",
            "content": "Graph Neural Networks (GNNs) have been used to address many problems that are inherently graph-like such as traffic networks, social networks, and physical and biological systems (Liu and Zhou, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_77",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_77@1",
            "content": "GNNs achieve impressive performance in many domains, including social networks (Wu et al., 2020) and natural science (Sanchez-Gonzalez et al., 2018) as well as NLP tasks like sentence classification (Huang et al., 2020), question generation (Pan et al., 2020), and summarization (Fernandes et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_77",
            "start": 201,
            "end": 504,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_78@0",
            "content": "Conclusion and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_78",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_79@0",
            "content": "We introduced graph neural networks and community detection algorithms for multiparallel word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_79",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_79@1",
            "content": "By incorporating signals from diverse sources as node features, including community features, our GNN model outperformed the baselines and prior work, establishing new state-of-the-art results on three PBC gold standard datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_79",
            "start": 105,
            "end": 333,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_79@2",
            "content": "We also showed that our GNN model improves downstream task performance in low-resource languages through annotation projection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_79",
            "start": 335,
            "end": 461,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_80@0",
            "content": "We have only used node features to provide signals to GNNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_80",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_80@1",
            "content": "In the future, other signals can be added in the form of edge features to further boost the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_80",
            "start": 60,
            "end": 163,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_81@0",
            "content": "\u017deljko Agi\u0107, Ivan Vuli\u0107, Jw300: A widecoverage parallel corpus for low-resource languages, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_81",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_82@0",
            "content": "Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, Roland Vollgraf, FLAIR: An easy-to-use framework for state-of-theart NLP, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_82",
            "start": 0,
            "end": 327,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_83@0",
            "content": "Alan Akbik, Duncan Blythe, Roland Vollgraf, Contextual string embeddings for sequence labeling, 2018, Proceedings of the 27th International Conference on Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_83",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_84@0",
            "content": "Tamer Alkhouli, Gabriel Bretschner, Jan-Thorsten Peter, Mohammed Hethnawi, Andreas Guta, Hermann Ney, Alignment-based neural machine translation, 2016, Proceedings of the First Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_84",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_85@0",
            "content": "Tamer Alkhouli, Hermann Ney, Biasing attention-based recurrent neural networks using external alignment information, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_85",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_86@0",
            "content": "Ehsaneddin Asgari, Hinrich Sch\u00fctze, Past, present, future: A computational investigation of the typology of tense in 1000 languages, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_86",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_87@0",
            "content": "Bonnie Necip Fazil Ayan,  Dorr, A maximum entropy approach to combining word alignments, 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_87",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_88@0",
            "content": "Nicola Bertoldi, Madalina Barbaiani, Phrase-based statistical machine translation with pivot languages, 2008, In International Workshop on Spoken Language Translation, IWSLT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_88",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_89@0",
            "content": "Steven Bird, Decolonising speech and language technology, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_89",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_90@0",
            "content": "Paolo Boldi, Sebastiano Vigna, Axioms for centrality, 2014, Internet Mathematics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_90",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_91@0",
            "content": ", A faster algorithm for betweenness centrality, 2001, Journal of mathematical sociology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_91",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_92@0",
            "content": "F Peter, Stephen Brown, Vincent Pietra, Robert Della Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_92",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_93@0",
            "content": "Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, Qun Liu, Accurate word alignment induction from neural machine translation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_93",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_94@0",
            "content": "Aaron Clauset, E Mark, Cristopher Newman,  Moore, Finding community structure in very large networks, 2004, Physical review E, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_94",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_95@0",
            "content": "Trevor Cohn, Mirella Lapata, Machine translation by triangulation: Making effective use of multi-parallel corpora, 2007, Proceedings of the 45th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_95",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_96@0",
            "content": ", Czech Republic, , Annual Meeting of the Association of Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_96",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_97@0",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_97",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_98@0",
            "content": "Gennaro Cordasco, Luisa Gargano, Community detection via semi-synchronous label propagation algorithms, 2010, 2010 IEEE international workshop on: business applications of social network analysis (BASNA), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_98",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_99@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_99",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_100@0",
            "content": "Yi Zi, Graham Dou,  Neubig, Word alignment by fine-tuning embeddings on parallel corpora, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_100",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_101@0",
            "content": "Chris Dyer, Victor Chahuneau, Noah Smith, A simple, fast, and effective reparameterization of IBM model 2, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_101",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_102@0",
            "content": "Patrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt, Structured neural summarization, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_102",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_103@0",
            "content": "Victoria Fossum, Steven Abney, Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora, 2005, Second International Joint Conference on Natural Language Processing: Full Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_103",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_104@0",
            "content": "C Linton,  Freeman, Centrality in social networks conceptual clarification, 1978, Social networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_104",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_105@0",
            "content": "Michelle Girvan, E Mark,  Newman, Community structure in social and biological networks, 2002, Proceedings of the national academy of sciences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_105",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_106@0",
            "content": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, Meng Wang, LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation, 2020, Association for Computing Machinery, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_106",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_107@0",
            "content": "Lianzhe Huang, Xin Sun, Sujian Li, Linhao Zhang, Houfeng Wang, Syntax-aware graph attention network for aspect-level sentiment classification, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_107",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_108@0",
            "content": "Matthias Huck, Diana Dutka, Alexander Fraser, Cross-lingual annotation projection is effective for neural part-of-speech tagging, 2019, Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_108",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_109@0",
            "content": "UNKNOWN, None, , Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2021. Graph algorithms for multiparallel word alignment, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_109",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_110@0",
            "content": "Masoud Jalili Sabet, Philipp Dufter, Fran\u00e7ois Yvon, Hinrich Sch\u00fctze, SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_110",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_111@0",
            "content": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, Monojit Choudhury, The state and fate of linguistic diversity and inclusion in the nlp world, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_111",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_112@0",
            "content": "UNKNOWN, None, 2016, Semisupervised classification with graph convolutional networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_112",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_113@0",
            "content": "UNKNOWN, None, 2016, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_113",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_114@0",
            "content": "UNKNOWN, None, 2005, Edinburgh system description for the 2005 iwslt speech translation evaluation. International Workshop on Spoken Language Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_114",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_115@0",
            "content": "UNKNOWN, None, 2003, Statistical phrase-based translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_115",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_116@0",
            "content": "Dan Kondratyuk, Milan Straka, 75 languages, 1 model: Parsing Universal Dependencies universally, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_116",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_117@0",
            "content": "Shankar Kumar, Franz Och, Wolfgang Macherey, Improving word alignment with bridge languages, 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_117",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_118@0",
            "content": "Omer Levy, Anders S\u00f8gaard, Yoav Goldberg, A strong baseline for learning cross-lingual word embeddings from sentence alignments, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_118",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_119@0",
            "content": "D William, Fei Lewis,  Xia, Automatically identifying computationally relevant typological features, 2008, Proceedings of the Third International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_119",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_120@0",
            "content": "Percy Liang, Ben Taskar, Dan Klein, Alignment by agreement, 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_120",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_121@0",
            "content": "Zhiyuan Liu, Jie Zhou, Introduction to graph neural networks, 2020, Synthesis Lectures on Artificial Intelligence and Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_121",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_122@0",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_122",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_123@0",
            "content": "Thomas Mayer, Michael Cysouw, Language comparison through sparse multilingual word alignment, 2012, Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_123",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_124@0",
            "content": "Thomas Mayer, Michael Cysouw, Creating a massively parallel bible corpus, 2014, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_124",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_125@0",
            "content": "UNKNOWN, None, 1998, Manual annotation of translational equivalence: The blinker project, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_125",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_126@0",
            "content": "Preslav Nakov, Hwee Tou Ng, Improving statistical machine translation for a resource-poor language using related resource-rich languages, 2012, Journal of Artificial Intelligence Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_126",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_127@0",
            "content": "E Mark,  Newman, Scientific collaboration networks. ii. shortest paths, weighted networks, and centrality, 2001, Physical review E, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_127",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_128@0",
            "content": "E Mark, Michelle Newman,  Girvan, Finding and evaluating community structure in networks, 2004, Physical review E, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_128",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_129@0",
            "content": "Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Jan Haji\u010d, Christopher Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, Daniel Zeman, Universal Dependencies v2: An evergrowing multilingual treebank collection, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_129",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_130@0",
            "content": "Josef Franz, Hermann Och,  Ney, A systematic comparison of various statistical alignment models, 2003, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_130",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_131@0",
            "content": "Robert \u00d6stling, Bayesian word alignment for massively parallel texts, 2014, Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_131",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_132@0",
            "content": "Robert \u00d6stling, Word order typology through multilingual word alignment, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_132",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_133@0",
            "content": "Robert \u00d6stling, J\u00f6rg Tiedemann, Efficient word alignment with Markov Chain Monte Carlo, 2016, The Prague Bulletin of Mathematical Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_133",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_134@0",
            "content": "Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, Min-Yen Kan, Semantic graphs for generating deep questions, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_134",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_135@0",
            "content": "Slav Petrov, Dipanjan Das, Ryan Mcdonald, A universal part-of-speech tagset, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_135",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_136@0",
            "content": "Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia, Graph networks as learnable physics engines for inference and control, 2018, Proceedings of the 35th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_136",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_137@0",
            "content": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, Gabriele Monfardini, The graph neural network model, 2009, IEEE Transactions on Neural Networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_137",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_138@0",
            "content": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, Graph attention networks, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_138",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_139@0",
            "content": "Guillaume Wisniewski, Nicolas P\u00e9cheux, Souhir Gahbiche-Braham, Fran\u00e7ois Yvon, Crosslingual part-of-speech tagging through ambiguous learning, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_139",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_140@0",
            "content": "Yongji Wu, Defu Lian, Yiheng Xu, Le Wu, Enhong Chen, Graph convolutional networks with markov random field reasoning for social spammer detection, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_140",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_141@0",
            "content": "David Yarowsky, Grace Ngai, Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora, 2001, Second Meeting of the North American Chapter of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_141",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "184-ARR_v1_142@0",
            "content": "Anssi Yli-Jyr\u00e4, Josi Purhonen, Matti Liljeqvist, Arto Antturi, Pekka Nieminen, Kari R\u00e4ntil\u00e4, Valtter Luoto, HELFI: a Hebrew-Greek-Finnish parallel Bible corpus with cross-lingual morpheme alignment, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v1_142",
            "start": 0,
            "end": 275,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "184-ARR_v1_0",
            "tgt_ix": "184-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_0",
            "tgt_ix": "184-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_1",
            "tgt_ix": "184-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_1",
            "tgt_ix": "184-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_0",
            "tgt_ix": "184-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_2",
            "tgt_ix": "184-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_4",
            "tgt_ix": "184-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_5",
            "tgt_ix": "184-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_6",
            "tgt_ix": "184-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_7",
            "tgt_ix": "184-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_8",
            "tgt_ix": "184-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_9",
            "tgt_ix": "184-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_10",
            "tgt_ix": "184-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_11",
            "tgt_ix": "184-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_12",
            "tgt_ix": "184-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_13",
            "tgt_ix": "184-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_0",
            "tgt_ix": "184-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_14",
            "tgt_ix": "184-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_16",
            "tgt_ix": "184-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_17",
            "tgt_ix": "184-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_18",
            "tgt_ix": "184-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_19",
            "tgt_ix": "184-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_20",
            "tgt_ix": "184-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_21",
            "tgt_ix": "184-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_23",
            "tgt_ix": "184-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_24",
            "tgt_ix": "184-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_0",
            "tgt_ix": "184-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_25",
            "tgt_ix": "184-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_26",
            "tgt_ix": "184-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_26",
            "tgt_ix": "184-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_27",
            "tgt_ix": "184-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_27",
            "tgt_ix": "184-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_26",
            "tgt_ix": "184-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_28",
            "tgt_ix": "184-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_30",
            "tgt_ix": "184-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_31",
            "tgt_ix": "184-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_32",
            "tgt_ix": "184-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_33",
            "tgt_ix": "184-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_34",
            "tgt_ix": "184-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_35",
            "tgt_ix": "184-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_36",
            "tgt_ix": "184-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_38",
            "tgt_ix": "184-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_39",
            "tgt_ix": "184-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_26",
            "tgt_ix": "184-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_40",
            "tgt_ix": "184-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_42",
            "tgt_ix": "184-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_44",
            "tgt_ix": "184-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_41",
            "tgt_ix": "184-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_41",
            "tgt_ix": "184-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_41",
            "tgt_ix": "184-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_41",
            "tgt_ix": "184-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_41",
            "tgt_ix": "184-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_26",
            "tgt_ix": "184-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_45",
            "tgt_ix": "184-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_47",
            "tgt_ix": "184-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_48",
            "tgt_ix": "184-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_49",
            "tgt_ix": "184-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_50",
            "tgt_ix": "184-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_51",
            "tgt_ix": "184-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_52",
            "tgt_ix": "184-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_53",
            "tgt_ix": "184-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_46",
            "tgt_ix": "184-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_46",
            "tgt_ix": "184-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_46",
            "tgt_ix": "184-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_46",
            "tgt_ix": "184-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_46",
            "tgt_ix": "184-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_46",
            "tgt_ix": "184-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_46",
            "tgt_ix": "184-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_46",
            "tgt_ix": "184-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_46",
            "tgt_ix": "184-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_26",
            "tgt_ix": "184-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_54",
            "tgt_ix": "184-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_56",
            "tgt_ix": "184-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_57",
            "tgt_ix": "184-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_58",
            "tgt_ix": "184-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_55",
            "tgt_ix": "184-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_55",
            "tgt_ix": "184-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_55",
            "tgt_ix": "184-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_55",
            "tgt_ix": "184-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_55",
            "tgt_ix": "184-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_0",
            "tgt_ix": "184-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_59",
            "tgt_ix": "184-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_60",
            "tgt_ix": "184-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_60",
            "tgt_ix": "184-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_0",
            "tgt_ix": "184-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_61",
            "tgt_ix": "184-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_62",
            "tgt_ix": "184-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_62",
            "tgt_ix": "184-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_0",
            "tgt_ix": "184-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_63",
            "tgt_ix": "184-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_64",
            "tgt_ix": "184-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_64",
            "tgt_ix": "184-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_66",
            "tgt_ix": "184-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_65",
            "tgt_ix": "184-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_65",
            "tgt_ix": "184-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_65",
            "tgt_ix": "184-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_64",
            "tgt_ix": "184-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_67",
            "tgt_ix": "184-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_68",
            "tgt_ix": "184-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_68",
            "tgt_ix": "184-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_64",
            "tgt_ix": "184-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_69",
            "tgt_ix": "184-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_70",
            "tgt_ix": "184-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_70",
            "tgt_ix": "184-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_64",
            "tgt_ix": "184-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_71",
            "tgt_ix": "184-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_72",
            "tgt_ix": "184-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_72",
            "tgt_ix": "184-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_64",
            "tgt_ix": "184-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_73",
            "tgt_ix": "184-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_76",
            "tgt_ix": "184-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_74",
            "tgt_ix": "184-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_74",
            "tgt_ix": "184-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_74",
            "tgt_ix": "184-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_74",
            "tgt_ix": "184-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_0",
            "tgt_ix": "184-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_77",
            "tgt_ix": "184-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_79",
            "tgt_ix": "184-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_78",
            "tgt_ix": "184-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_78",
            "tgt_ix": "184-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_78",
            "tgt_ix": "184-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v1_0",
            "tgt_ix": "184-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_1",
            "tgt_ix": "184-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_2",
            "tgt_ix": "184-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_2",
            "tgt_ix": "184-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_2",
            "tgt_ix": "184-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_2",
            "tgt_ix": "184-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_2",
            "tgt_ix": "184-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_2",
            "tgt_ix": "184-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_2",
            "tgt_ix": "184-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_2",
            "tgt_ix": "184-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_3",
            "tgt_ix": "184-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_4",
            "tgt_ix": "184-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_5",
            "tgt_ix": "184-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_5",
            "tgt_ix": "184-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_5",
            "tgt_ix": "184-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_5",
            "tgt_ix": "184-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_5",
            "tgt_ix": "184-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_6",
            "tgt_ix": "184-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_6",
            "tgt_ix": "184-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_6",
            "tgt_ix": "184-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_6",
            "tgt_ix": "184-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_6",
            "tgt_ix": "184-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_6",
            "tgt_ix": "184-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_7",
            "tgt_ix": "184-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_8",
            "tgt_ix": "184-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_8",
            "tgt_ix": "184-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_9",
            "tgt_ix": "184-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_9",
            "tgt_ix": "184-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_9",
            "tgt_ix": "184-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_9",
            "tgt_ix": "184-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_9",
            "tgt_ix": "184-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_9",
            "tgt_ix": "184-ARR_v1_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_9",
            "tgt_ix": "184-ARR_v1_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_9",
            "tgt_ix": "184-ARR_v1_9@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_10",
            "tgt_ix": "184-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_10",
            "tgt_ix": "184-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_10",
            "tgt_ix": "184-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_11",
            "tgt_ix": "184-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_11",
            "tgt_ix": "184-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_11",
            "tgt_ix": "184-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_11",
            "tgt_ix": "184-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_11",
            "tgt_ix": "184-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_12",
            "tgt_ix": "184-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_12",
            "tgt_ix": "184-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_12",
            "tgt_ix": "184-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_13",
            "tgt_ix": "184-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_13",
            "tgt_ix": "184-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_13",
            "tgt_ix": "184-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_13",
            "tgt_ix": "184-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_13",
            "tgt_ix": "184-ARR_v1_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_13",
            "tgt_ix": "184-ARR_v1_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_14",
            "tgt_ix": "184-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_14",
            "tgt_ix": "184-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_14",
            "tgt_ix": "184-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_14",
            "tgt_ix": "184-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_14",
            "tgt_ix": "184-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_15",
            "tgt_ix": "184-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_16",
            "tgt_ix": "184-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_16",
            "tgt_ix": "184-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_16",
            "tgt_ix": "184-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_17",
            "tgt_ix": "184-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_17",
            "tgt_ix": "184-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_17",
            "tgt_ix": "184-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_18",
            "tgt_ix": "184-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_18",
            "tgt_ix": "184-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_18",
            "tgt_ix": "184-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_18",
            "tgt_ix": "184-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_18",
            "tgt_ix": "184-ARR_v1_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_19",
            "tgt_ix": "184-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_19",
            "tgt_ix": "184-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_19",
            "tgt_ix": "184-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_19",
            "tgt_ix": "184-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_20",
            "tgt_ix": "184-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_21",
            "tgt_ix": "184-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_21",
            "tgt_ix": "184-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_22",
            "tgt_ix": "184-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_23",
            "tgt_ix": "184-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_23",
            "tgt_ix": "184-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_23",
            "tgt_ix": "184-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_23",
            "tgt_ix": "184-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_23",
            "tgt_ix": "184-ARR_v1_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_24",
            "tgt_ix": "184-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_25",
            "tgt_ix": "184-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_25",
            "tgt_ix": "184-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_26",
            "tgt_ix": "184-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_27",
            "tgt_ix": "184-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_28",
            "tgt_ix": "184-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_28",
            "tgt_ix": "184-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_28",
            "tgt_ix": "184-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_29",
            "tgt_ix": "184-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_30",
            "tgt_ix": "184-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_30",
            "tgt_ix": "184-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_30",
            "tgt_ix": "184-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_30",
            "tgt_ix": "184-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_30",
            "tgt_ix": "184-ARR_v1_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_31",
            "tgt_ix": "184-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_31",
            "tgt_ix": "184-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_31",
            "tgt_ix": "184-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_32",
            "tgt_ix": "184-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_33",
            "tgt_ix": "184-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_34",
            "tgt_ix": "184-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_35",
            "tgt_ix": "184-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_35",
            "tgt_ix": "184-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_36",
            "tgt_ix": "184-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_36",
            "tgt_ix": "184-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_37",
            "tgt_ix": "184-ARR_v1_37@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_38",
            "tgt_ix": "184-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_38",
            "tgt_ix": "184-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_38",
            "tgt_ix": "184-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_38",
            "tgt_ix": "184-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_39",
            "tgt_ix": "184-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_40",
            "tgt_ix": "184-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_40",
            "tgt_ix": "184-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_41",
            "tgt_ix": "184-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_42",
            "tgt_ix": "184-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_43",
            "tgt_ix": "184-ARR_v1_43@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_44",
            "tgt_ix": "184-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_44",
            "tgt_ix": "184-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_44",
            "tgt_ix": "184-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_44",
            "tgt_ix": "184-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_45",
            "tgt_ix": "184-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_46",
            "tgt_ix": "184-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_47",
            "tgt_ix": "184-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_47",
            "tgt_ix": "184-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_47",
            "tgt_ix": "184-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_47",
            "tgt_ix": "184-ARR_v1_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_47",
            "tgt_ix": "184-ARR_v1_47@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_47",
            "tgt_ix": "184-ARR_v1_47@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_48",
            "tgt_ix": "184-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_48",
            "tgt_ix": "184-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_49",
            "tgt_ix": "184-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_50",
            "tgt_ix": "184-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_51",
            "tgt_ix": "184-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_52",
            "tgt_ix": "184-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_52",
            "tgt_ix": "184-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_52",
            "tgt_ix": "184-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_52",
            "tgt_ix": "184-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_52",
            "tgt_ix": "184-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_52",
            "tgt_ix": "184-ARR_v1_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_53",
            "tgt_ix": "184-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_53",
            "tgt_ix": "184-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_53",
            "tgt_ix": "184-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_54",
            "tgt_ix": "184-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_55",
            "tgt_ix": "184-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_56",
            "tgt_ix": "184-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_56",
            "tgt_ix": "184-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_56",
            "tgt_ix": "184-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_57",
            "tgt_ix": "184-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_57",
            "tgt_ix": "184-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_57",
            "tgt_ix": "184-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_57",
            "tgt_ix": "184-ARR_v1_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_57",
            "tgt_ix": "184-ARR_v1_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_58",
            "tgt_ix": "184-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_58",
            "tgt_ix": "184-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_58",
            "tgt_ix": "184-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_58",
            "tgt_ix": "184-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_58",
            "tgt_ix": "184-ARR_v1_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_58",
            "tgt_ix": "184-ARR_v1_58@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_59",
            "tgt_ix": "184-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_59",
            "tgt_ix": "184-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_59",
            "tgt_ix": "184-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_59",
            "tgt_ix": "184-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_60",
            "tgt_ix": "184-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_61",
            "tgt_ix": "184-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_61",
            "tgt_ix": "184-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_61",
            "tgt_ix": "184-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_61",
            "tgt_ix": "184-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_61",
            "tgt_ix": "184-ARR_v1_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_62",
            "tgt_ix": "184-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_63",
            "tgt_ix": "184-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_63",
            "tgt_ix": "184-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_63",
            "tgt_ix": "184-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_63",
            "tgt_ix": "184-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_63",
            "tgt_ix": "184-ARR_v1_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_64",
            "tgt_ix": "184-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_65",
            "tgt_ix": "184-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_66",
            "tgt_ix": "184-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_66",
            "tgt_ix": "184-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_67",
            "tgt_ix": "184-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_67",
            "tgt_ix": "184-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_67",
            "tgt_ix": "184-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_67",
            "tgt_ix": "184-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_68",
            "tgt_ix": "184-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_69",
            "tgt_ix": "184-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_69",
            "tgt_ix": "184-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_69",
            "tgt_ix": "184-ARR_v1_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_69",
            "tgt_ix": "184-ARR_v1_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_69",
            "tgt_ix": "184-ARR_v1_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_70",
            "tgt_ix": "184-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_71",
            "tgt_ix": "184-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_71",
            "tgt_ix": "184-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_71",
            "tgt_ix": "184-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_71",
            "tgt_ix": "184-ARR_v1_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_71",
            "tgt_ix": "184-ARR_v1_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_72",
            "tgt_ix": "184-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_73",
            "tgt_ix": "184-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_73",
            "tgt_ix": "184-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_73",
            "tgt_ix": "184-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_73",
            "tgt_ix": "184-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_74",
            "tgt_ix": "184-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_75",
            "tgt_ix": "184-ARR_v1_75@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_76",
            "tgt_ix": "184-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_76",
            "tgt_ix": "184-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_76",
            "tgt_ix": "184-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_77",
            "tgt_ix": "184-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_77",
            "tgt_ix": "184-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_78",
            "tgt_ix": "184-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_79",
            "tgt_ix": "184-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_79",
            "tgt_ix": "184-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_79",
            "tgt_ix": "184-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_80",
            "tgt_ix": "184-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_80",
            "tgt_ix": "184-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_81",
            "tgt_ix": "184-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_82",
            "tgt_ix": "184-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_83",
            "tgt_ix": "184-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_84",
            "tgt_ix": "184-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_85",
            "tgt_ix": "184-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_86",
            "tgt_ix": "184-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_87",
            "tgt_ix": "184-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_88",
            "tgt_ix": "184-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_89",
            "tgt_ix": "184-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_90",
            "tgt_ix": "184-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_91",
            "tgt_ix": "184-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_92",
            "tgt_ix": "184-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_93",
            "tgt_ix": "184-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_94",
            "tgt_ix": "184-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_95",
            "tgt_ix": "184-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_96",
            "tgt_ix": "184-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_97",
            "tgt_ix": "184-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_98",
            "tgt_ix": "184-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_99",
            "tgt_ix": "184-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_100",
            "tgt_ix": "184-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_101",
            "tgt_ix": "184-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_102",
            "tgt_ix": "184-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_103",
            "tgt_ix": "184-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_104",
            "tgt_ix": "184-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_105",
            "tgt_ix": "184-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_106",
            "tgt_ix": "184-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_107",
            "tgt_ix": "184-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_108",
            "tgt_ix": "184-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_109",
            "tgt_ix": "184-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_110",
            "tgt_ix": "184-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_111",
            "tgt_ix": "184-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_112",
            "tgt_ix": "184-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_113",
            "tgt_ix": "184-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_114",
            "tgt_ix": "184-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_115",
            "tgt_ix": "184-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_116",
            "tgt_ix": "184-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_117",
            "tgt_ix": "184-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_118",
            "tgt_ix": "184-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_119",
            "tgt_ix": "184-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_120",
            "tgt_ix": "184-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_121",
            "tgt_ix": "184-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_122",
            "tgt_ix": "184-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_123",
            "tgt_ix": "184-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_124",
            "tgt_ix": "184-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_125",
            "tgt_ix": "184-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_126",
            "tgt_ix": "184-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_127",
            "tgt_ix": "184-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_128",
            "tgt_ix": "184-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_129",
            "tgt_ix": "184-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_130",
            "tgt_ix": "184-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_131",
            "tgt_ix": "184-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_132",
            "tgt_ix": "184-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_133",
            "tgt_ix": "184-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_134",
            "tgt_ix": "184-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_135",
            "tgt_ix": "184-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_136",
            "tgt_ix": "184-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_137",
            "tgt_ix": "184-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_138",
            "tgt_ix": "184-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_139",
            "tgt_ix": "184-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_140",
            "tgt_ix": "184-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_141",
            "tgt_ix": "184-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v1_142",
            "tgt_ix": "184-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1471,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "184-ARR",
        "version": 1
    }
}