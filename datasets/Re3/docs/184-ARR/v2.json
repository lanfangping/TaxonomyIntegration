{
    "nodes": [
        {
            "ix": "184-ARR_v2_0",
            "content": "Graph Neural Networks for Multiparallel Word Alignment",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_2",
            "content": "After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation. Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together. First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph. Next, we use graph neural networks (GNNs) to exploit the graph structure. Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) yields a prediction model that can generalize beyond the training sentences. We show that community detection provides valuable information for multiparallel word alignment. Our method outperforms previous work on three word alignment datasets and on a downstream task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "184-ARR_v2_4",
            "content": "Word alignments are crucial for statistical machine translation (Koehn et al., 2003) and useful for many other multilingual tasks such as neural machine translation (Alkhouli and Ney, 2017;Alkhouli et al., 2016), typological analysis (Lewis and Xia, 2008;\u00d6stling, 2015;Asgari and Sch\u00fctze, 2017) and annotation projection (Yarowsky and Ngai, 2001;Fossum and Abney, 2005;Wisniewski et al., 2014;Huck et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_5",
            "content": "The rise of deep learning initially led to a temporary plateau, but interest in word alignments is now increasing, demonstrated by several recent publications (Jalili Sabet et al., 2020;Dou and Neubig, 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_6",
            "content": "While word alignment is usually considered for bilingual corpora, our work addresses the problem Colors represent languages. Each English (yellow) node is annotated with its word. Red dashed lines cut links that incorrectly connect distinct concepts. We exploit community detection algorithms to detect distinct concepts. This provides valuable information for our GNN model and improves word alignments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_7",
            "content": "of word alignment in multiparallel corpora, containing sentence level parallel text in more than two languages. Examples of multiparallel corpora are JW300 (Agi\u0107 and Vuli\u0107, 2019), PBC (Mayer and Cysouw, 2014) which covers the highest number of languages (1334), and Tatoeba. 1 While the perlanguage amount of data provided in such corpora is less than bilingual corpora, they support highly low-resource languages, many of which are not covered by existing language technologies (Joshi et al., 2020). Therefore, these corpora are essential for studying many of the world's low-resource languages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_8",
            "content": "We consider the task of word alignment for multiparallel sentences. The basic motivation is that the alignment between words in languages U and V can benefit from word-level alignments of U and V with a translation in a third language W . Following up on the work of Imani Googhari et al. (2021), we model multilingual word alignments with tools borrowed from graph theory (community detection algorithms) combined with neural network based models, specifically, the graph neural network (GNN) model of Scarselli et al. (2009).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_9",
            "content": "GNNs were proposed to extend the powerful current generation of neural network models to the processing of graph-structured data and they have gained increasing popularity in many domains (Wu et al., 2020;Sanchez-Gonzalez et al., 2018;He et al., 2020). GNNs can incorporate heterogeneous sources of signal in the form of node and edge features. We use this property to take into account properties of the whole alignment graph, notably its tendency to cluster into communities, see Figure 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_10",
            "content": "With our new proposed methods, we obtain improved results on word alignment for three language pairs: English-French, Finnish-Hebrew and Finnish-Greek. As a demonstration of the importance of high-quality alignments, we use our word alignments to project annotations from highresource to low-resource languages. We improve a part-of-speech tagger for Yoruba by training it over a high-quality dataset, which is created using annotation projection.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_11",
            "content": "Contributions: i) We propose a graph neural network model to enhance word alignments in a multiparallel corpus. The model incorporates a diverse set of features for word alignments in multiparallel corpora and an elegant way of training it efficiently and effectively. ii) We show that community detection improves multiparallel word alignment. iii) We show that the improved alignments improve performance on a downstream task for a low resource language. iv) We propose a new method to infer alignments from the alignment probability matrix. v) We will make our code publicly available.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_12",
            "content": "MultiParallel Word Alignment Graphs",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "184-ARR_v2_13",
            "content": "Building MultiParallel Word Alignment Graphs",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "184-ARR_v2_14",
            "content": "Our starting point is the work of Imani Googhari et al. ( 2021), who introduce MPWA (MultiParallel Word Alignment), a framework that utilizes the synergy between multiple language pairs to improve bilingual word alignments for a target language pair. The rationale is that some of the missing alignment edges between a source and a target language can be recovered using their alignments with words in other languages. An MPWA graph is constructed using the following two steps:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_15",
            "content": "1. create initial bilingual alignments for all language pairs in a multiparallel corpus using a bilingual word aligner;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_16",
            "content": "2. represent the bilingual alignments for each multiparallel sentence in a graph containing one vertex for each token occurring in any language and one edge for each initial bilingual word alignment link.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_17",
            "content": "Potentially missing alignment links are then added based on the graph structure in an inference step, casting the word alignment task as an edge prediction problem. Figure 1 gives an example of a multiparallel word alignment graph for a 12-way multiparallel sentence. Imani Googhari et al. ( 2021) use two traditional graph algorithms, Adamic-Adar and non-negative matrix factorization, for predicting new alignment edges from the MPWA graph. However, these graph algorithms are applied to individual multiparallel sentences independently and therefore cannot accumulate knowledge from multiple sentences. Moreover, their edge predictions are solely based on the structure of the graph and do not take advantage of other beneficial signals such as a word's language, relative position and meaning. Another limitation of this work is that it only adds links and does not remove any, which is important to improve precision.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_18",
            "content": "This work addresses these shortcomings by using GNNs to predict alignment edges from MPWA graphs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_19",
            "content": "Community Detection in Alignment Graphs",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "184-ARR_v2_20",
            "content": "One important advantage of GNNs over traditional graph algorithms is that they can directly incorporate signals from different sources in the form of node and edge features. We utilize this by taking into account the following observation: The nodes in the alignment graph are words in parallel sentences that are translations of each other. If the initial bilingual alignments are of good quality, we expect words that are mutual translations to form densely connected regions or communities; see Figure 1. These communities should not be linked to each other, each corresponding to a distinct connected component. In other words, ideally, words representing a concept should be densely connected, but there should be no links between different concepts. While, this intuition will not be true for all concepts between all possible language pairs, we nonetheless hypothesize that identifying distinct concepts in a multiparallel word alignment graph can provide useful information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_21",
            "content": "To examine to what extent these expectations are met, we count the components in the original Eflomal-generated (\u00d6stling and Tiedemann, 2016) graph (see \u00a74.2 for details on the initial alignments). Table 1 shows that the average number of components per sentence is less than three (\"Eflomal intersection\", columns #CC). But intuitively, the number of components should roughly correspond to sentence length (or, more precisely, the number of content words). This indicates that there are many links that incorrectly connect different concepts. To detect such links, we use community detection (CD) algorithms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_22",
            "content": "CD algorithms find subnetworks of nodes that form tightly knit groups that are only loosely connected with a small number of links (Girvan and Newman, 2002). One well-known approach to CD attempts to maximize the modularity measure (Newman and Girvan, 2004). Modularity assesses how beneficial a division of a community into two communities is, in the sense that there are many links within communities and only a few between them. Given a graph G with n nodes and m edges and G's adjacency matrix A \u2208 R n\u00d7n , modularity is defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_23",
            "content": "mod = 1 2m ij A ij \u2212 \u03b3 d i d j 2m I(c i , c j ) (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_24",
            "content": "where d i is the degree of node i. I(c i , c j ) is 1 if nodes i and j are in the same community, 0 otherwise. As exact modularity maximization is intractable, we experiment with two CD algorithms implementing different heuristic approaches:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_25",
            "content": "\u2022 Greedy modularity communities (GMC). This method uses Clauset-Newman-Moore greedy modularity maximization (Clauset et al., 2004). GMC begins with each node in its own community and greedily joins the pair of communities that most increases modularity until no such pair exists. \u2022 Label propagation communities (LPC). This method finds communities in a graph using label propagation (Cordasco and Gargano, 2010). It begins by giving a label to each node of the network. Then each node's label is updated by the most frequent label among its neighbors in each iteration. It performs label propagation on a portion of nodes at each step and quickly converges to a stable labeling.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_26",
            "content": "After detecting communities, we link all nodes inside a community and remove all intercommunity links. GMC (LPC) on average removes 3% (7%) of the edges. Table 1 reports the average number of graph components per sentence before and after running GMC and LPC, as well as the corresponding F 1 for word alignment (see \u00a74.1 for details on the evaluation datasets). We see that the number of communities found is lower for GMC than for LPC; therefore, LPC identifies more candidate links for deletion. 2 Comparing the number of communities detected with the average sentence length, GMC seems to have failed to detect enough communities to split different concepts properly. The F 1 scores confirm this observation and show that LPC performs well at detecting the communities we are looking for.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_27",
            "content": "This analysis shows that CD algorithms compute valuable information for word alignments. To exploit this in our GNN model, we add node community information as a node feature; see \u00a73.1.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_28",
            "content": "Predicting and using MultiParallel",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "184-ARR_v2_29",
            "content": "Word Alignments (MPWAs)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_30",
            "content": "GNNs for MPWA",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "184-ARR_v2_31",
            "content": "GNNs can be used in transductive or inductive settings. Transductively, the final model can only be used for inference over the same graph that it is trained on. In an inductive setting, which we use here, nodes are represented as feature vectors, and the final model has the advantage of being applicable to a different graph in inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_32",
            "content": "Below is the step-by-step overview of our GNNbased approach for an MPWA graph:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_33",
            "content": "1. run community detection algorithms on the initial graph ( \u00a72.2);",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_34",
            "content": "2. obtain features for the nodes of the graph ( \u00a73.1.3);",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_35",
            "content": "3. compute node embeddings from node features and initial alignment links in the GNN encoding step ( \u00a73.1.2);",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_36",
            "content": "4. learn to distinguish between nodes that are aligned together and that are not aligned together in the GNN decoding step ( \u00a73.1.2);",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_37",
            "content": "After the GNN model is trained on multiple MPWA graphs, it is used to infer an alignment probability matrix between tokens in a source language and tokens in a target language for a multiparallel sentence, see \u00a73.1.4. Our method predicts new alignment links from this matrix, independently of initial edges. Therefore, given an initial bilingual alignment, it is not limited to adding edges, but it can also remove them.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_38",
            "content": "Model Architecture",
            "ntype": "title",
            "meta": {
                "section": "3.1.1"
            }
        },
        {
            "ix": "184-ARR_v2_39",
            "content": "Our model is inspired by the Graph Auto Encoder (GAE) model of Kipf and Welling (2016) for link prediction. A GAE model consists of an encoder and a decoder. The encoder creates a hidden representation for each node of the graph and the decoder predicts the links of the graph given the nodes' representations. Using the graph of word alignments, the model will learn the word alignment task. Therefore it will be able to predict word alignments that are missed by the original bilingual word aligner and also detect incorrect alignment edges.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_40",
            "content": "We make changes to this model to improve the model's quality and reduce its computational cost. We use GATConv layers (Veli\u010dkovi\u0107 et al., 2018) for the encoder instead of GCNConv (Kipf and Welling, 2017) and a more sophisticated decoder instead of simple dot product for a stronger model. We also introduce a more efficient training procedure.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_41",
            "content": "The encoder is a graph attention network (GAT) (Veli\u010dkovi\u0107 et al., 2018) with two GATConv layers followed by a fully connected layer. Layers are connected by RELU non-linearities. A GATConv layer computes its output x i for a node i from its input x i as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_42",
            "content": "x i = \u03b1 i,i Wx i + j\u2208N (i) \u03b1 i,j Wx j ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_43",
            "content": "where W is a weight matrix, N (i) is some neighborhood of node i in the graph, and \u03b1 i,j is the attention coefficient indicating the importance of node j's features to node i. \u03b1 i,j is computed as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_44",
            "content": "\u03b1 i,j = exp g a [Wx i Wx j ] k\u2208N (i)\u222a{i} exp (g (a [Wx i Wx k ]))",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_45",
            "content": "(3) where is concatenation, g is LeakyReLU, and a is a weight vector. Given the features for the nodes and their alignment edges, the encoder creates a contextualized hidden representation for each node.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_46",
            "content": "Based on the hidden representations of two nodes, the decoder predicts whether a link connects them. The decoder architecture consists of a fully connected layer, a RELU non-linearity and a sigmoid layer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_47",
            "content": "Training",
            "ntype": "title",
            "meta": {
                "section": "3.1.2"
            }
        },
        {
            "ix": "184-ARR_v2_48",
            "content": "By default, GAE models are trained using full batches with random negative samples. This approach requires at least tens of epochs over the training dataset to converge and a lot of GPU memory for graphs as large as ours. We train our model using mini-batches to decrease memory requirements and improve the performance. Using our training approach the model converges after one epoch. We take care to select informative negative samples (as opposed to random selection) as described below.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_49",
            "content": "Figure 2 displays our GNN model and the training process. The training set contains one graph for each sentence. Each graph is divided into multiple batches. Each batch contains a random subset of the graph's edges as positive samples. The negative samples are created as follows. Given a sentence u 1 u 2 . . . u n in language U and its translation v 1 v 2 . . . v m in language V , for each alignment edge u i :v j in the current batch, two negative edges u i :v j and u i :v j (j = j, i = i) are randomly sampled.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_50",
            "content": "For each training batch, the encoder takes the batch's whole graph (i.e., node features for all graph nodes and all graph edges) as input and computes hidden representations for the nodes. On the decoder side, for each link between two nodes in the batch, the hidden representations of the two nodes are concatenated to create the decoder's input. The decoder's target is the link class: 1 (resp. 0) for positive (resp. negative) links. We train with a binary classification objective:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_51",
            "content": "L = \u2212 1 b b i=1 log(p + i ) + 1 2b 2b i=1 log(p \u2212 i ) (4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_52",
            "content": "where b is the batch size and p + i and p \u2212 i are the model predictions for the i th positive and negative samples within the batch. Parameters of the encoder and decoder as well as the node-embedding feature layer are updated after each training step.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_53",
            "content": "Node Features",
            "ntype": "title",
            "meta": {
                "section": "3.1.3"
            }
        },
        {
            "ix": "184-ARR_v2_54",
            "content": "We use three main types of node features: (i) graph structural features, (ii) community-based features and (iii) word content features.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_55",
            "content": "Graph structural features. We use degree, closeness (Freeman, 1978) , betweenness (Brandes, 2001) , load (Newman, 2001) and harmonic centrality (Boldi and Vigna, 2014) features as additional information about the graph structure. These features are continuous numbers, providing information about the position and connectivity of the nodes within the graph. We standardize (i.e., zscore) each feature across all nodes, and train an embedding of size four for each feature. 3 Community-based features. One way to incorporate community information into our model is to 3 Learning a size-four embedding instead of a single number gives the feature a weight similar to other features -which have a feature vector of about the same size.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_56",
            "content": "train the model based on a refined set of edges after the community detection step. This approach hobbles the GNN model by making decisions about many of the edges before the GNN gets to see them. Our initial experiments also confirmed that training the GNN over CD refined edges does not help. Therefore, we add community information as node features and let the GNN use them to improve its decisions. We use the community detection algorithms GMC and LPC (see \u00a7 \u00a72.2) to identify communities in the graph. Then we represent the community membership information of the nodes as one-hot vectors and learn an embedding of size 32 for each of the two algorithms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_57",
            "content": "Word content features. We train embeddings for word position (size 32) and word language (size 20). We learn 100-dimensional multilingual word embeddings using Levy et al. (2017)'s sentence-ID method on the 84 PBC languages selected by Imani Googhari et al. (2021). Word embeddings serve as initialization and are updated during GNN training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_58",
            "content": "After concatenating these features, each node is represented by a 236 dimensional vector that is then fed to the encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_59",
            "content": "Inducing Bilingual Alignment Edges",
            "ntype": "title",
            "meta": {
                "section": "3.1.4"
            }
        },
        {
            "ix": "184-ARR_v2_60",
            "content": "Given a source sentence x = x 1 , x 2 , . . . , x m in language X and a target sentence \u0177 = y 1 , y 2 , . . . , y l in language Y , we feed all possible alignment links between source and target to the decoder. This produces a symmetric alignment probability matrix S of size m \u00d7 l where S ij is the predicted alignment probability between words x i and y j . Using these values directly to infer alignment edges is usually suboptimal; therefore, more sophisticated methods have been suggested (Ayan and Dorr, 2006;Liang et al., 2006). Here we propose a new approach: it combines Koehn et al. (2005)'s Grow-Diag-Final-And (GDFA) with Dou and Neubig (2021)'s probability thresholding. We modify the latter to account for the variable size of the probability matrix (i.e., length of source/target sentences). Our method is not limited to adding new edges to some initial bilingual alignments, a limitation of prior work. As we predict each edge independently, some initial links can be discarded from the final alignment.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_61",
            "content": "We start by creating a set of forward (sourceto-target) alignment edges and a set of backward (target-to-source) alignment edges. To this end, first, inspired by probability thresholding (Dou and Neubig, 2021), we apply softmax to S, and zero out probabilities below a threshold to get a sourceto-target probability matrix S XY :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_62",
            "content": "S XY = S * (softmax(S) > \u03b1 l ) (5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_63",
            "content": "Analogously, we compute the target-to-source probability matrix S Y X :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_64",
            "content": "S Y X = S * (softmax(S ) > \u03b1 m ) (6",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_65",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_66",
            "content": "where \u03b1 is a sensitivity hyperparameter, e.g., \u03b1 = 1 means that we pick edges with a probability higher than average. We experimentally set \u03b1 = 2. Next, from each row of S XY (S Y X ), we pick the cell with the highest value (if any exists) and add this edge to the forward (backward) set. We create the final set of alignment edges by applying the GDFA symmetrization method (Koehn et al., 2005) to forward and backward sets. The gist of GDFA is to use the intersection of forward and backward as initial alignment edges and add more edges from the union of forward and backward based on a number of heuristics. We call this method TGDFA (Thresholding GDFA).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_67",
            "content": "We also experiment with combining TGDFA with the original bilingual GDFA alignments. We do so by adding bilingual GDFA edges to the union of forward and backward before performing the GDFA heuristics. We refer to these alignments as TGDFA+orig.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_68",
            "content": "We evaluate the resulting alignments using F 1 score and alignment error rate (AER), the standard metrics in the word alignment literature.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_69",
            "content": "Annotation Projection",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "184-ARR_v2_70",
            "content": "Annotation projection automatically creates linguistically annotated corpora for low-resource lan-guages. A model trained on data with \"annotationprojected\" labels can perform better than a completely unsupervised method. Here, we focus on universal part-of-speech (UPOS) tagging (Petrov et al., 2012) for the low resource target language Yoruba; this language only has a small set of annotated sentences in Universal Dependencies (Nivre et al., 2020) and has poor POS results in unsupervised settings (Kondratyuk and Straka, 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_71",
            "content": "The quality of the target annotated corpus depends on the quality of the annotations in the source languages and the quality of the word alignments between sources and target. We use the Flair (Akbik et al., 2019) POS taggers for three high resource languages, English, German and French (Akbik et al., 2018), to annotate 30K verses whose Yoruba translations are available in PBC. We then transfer the POS tags from source to target using three different approaches: (i) We directly transfer annotations from English to the target. (ii) For each word in the target, we get its Eflomal bilingual alignments in the three source languages and predict the majority POS to annotate the target word. (iii) The same as in (ii), but we use our GNN (TGDFA) alignments (instead of Eflomal alignments) to project from source to target. In all three approaches, we discard any target sentence from the POS tagger training data if more than 50% of its words are annotated with the \"X\" (other) tag.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_72",
            "content": "We train a Flair SequenceTagger model on the target annotated data using mBERT embeddings (Devlin et al., 2019) Evaluation data. For our main evaluation, we use the two word alignment gold datasets for PBC published by Imani Googhari et al. (2021): Blinker (Melamed, 1998) and HELFI (Yli-Jyr\u00e4 et al., 2020). The HELFI dataset contains the Hebrew Bible, Greek New Testament and their translations into Finnish. For HELFI, we use Imani Training data. The graph algorithms used by Imani Googhari et al. ( 2021) operate on each multiparallel sentence separately. In contrast, our approach allows for an inductive setting where a model is trained on a training set, accumulating knowledge from multiple multiparallel sentences. We combine the verses in the training sets of Finnish-Hebrew and Finnish-Greek for a combined training set size of 24,159. 5",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_73",
            "content": "Initial Word Alignments",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "184-ARR_v2_74",
            "content": "We use the Eflomal statistical word aligner to obtain bilingual alignments. We train it for every language pair in our experiments. We do not consider SimAlign (Jalili Sabet et al., 2020) since it is shown to perform poorly for languages whose representations in the multilingual pretrained language model are of low quality. We use Eflomal asymmetrical alignments post-processed with the intersection heuristic to get high precision bilingual alignments as input to the GNN. We use the same subset of 84 languages as Imani Googhari et al. (2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_75",
            "content": "Training Details",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "184-ARR_v2_76",
            "content": "We use PyTorch Geometric 6 to construct and train the GNN. The model's hidden layer size is 512 for both GATConv and Linear layers. We train for one epoch on the training set -a small portion of the training set is enough to learn good embeddings (see \u00a75.1.1). For training, we use a batch size of 400 and learning rate of .001 with AdamW (Loshchilov and Hutter, 2017). The whole training 5 Note that we do not use any gold alignments for training the GNN. Using the verses from HELFI train split as our training set is for convenience. Our ablation experiment (Figure 3) show that a smaller subset of the training set is sufficient to achieve good performance 6 pytorch-geometric.readthedocs.io process takes less than 4 hours on a GeForce GTX 1080 Ti and the inference time is on the order of milliseconds per sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_77",
            "content": "5 Experiments and Results",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_78",
            "content": "Multiparallel corpus results",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "184-ARR_v2_79",
            "content": "Table 2 shows results on Blinker and HELFI for our GNNs and the baselines: bilingual alignments and two graph-based algorithms WAdAd and NMF from Imani Googhari et al. (2021). Our GNNs yield a better trade-off between precision and recall, most likely thanks to their ability to remove edges, and achieve the best F 1 and AER on all three datasets, outperforming WAdAd and NMF. GNN (TGDFA) achieves the best results on HELFI (FIN-HEB, FIN-GRC) while GNN (TGDFA+orig) is best on Blinker (ENG-FRA). As argued in Imani Googhari et al. (2021), this is mostly due to the different ways these two datasets were annotated. Most HELFI alignments are one-to-one, while many Blinker alignments are many-to-many: phrase-level alignments where every word in a source phrase is aligned with every word in a target phrase. This suggests that one can choose between GNN (TGDFA) and GNN (TGDFA+orig) based on the desired characteristics of the alignment.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_80",
            "content": "Effect of Training Set Size",
            "ntype": "title",
            "meta": {
                "section": "5.1.1"
            }
        },
        {
            "ix": "184-ARR_v2_81",
            "content": "To investigate the effect of training set size, we train the GNN on subsets of our training data with increasing sizes. Figure 3 shows results. Performance improves fast until around 2,000 verses; then it stays mostly constant. Using more than 6,400 samples does not change the performance at all. Therefore, in the other experiments we use 6,400 randomly sampled verses from the training set to train GNNs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_82",
            "content": "Ablation Experiments",
            "ntype": "title",
            "meta": {
                "section": "5.1.2"
            }
        },
        {
            "ix": "184-ARR_v2_83",
            "content": "To examine the importance of node features, we ablate language, position, centrality, community and word embedding features. Table 3 shows that removal of graph structural features drastically reduces performance. Community features and language information are also important. Removal of word position information and word embeddings -which store semantic information about wordshas the least effect. Based on these results, it can be argued that the lexical information contained in the initial alignments and in the community features provides a strong signal regarding word relatedness. The novel information that is crucial is about the overall graph structure which goes beyond the local word associations that are captured by word position and word embeddings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_84",
            "content": "Effect of Word Frequency",
            "ntype": "title",
            "meta": {
                "section": "5.1.3"
            }
        },
        {
            "ix": "184-ARR_v2_85",
            "content": "We investigate the effect of word frequency on alignment performance where frequency is calculated based on the source word in the PBC; the first bin has the highest frequency. Figure 4 shows that the performance of Eflomal drops with frequency and it struggles to align very rare words. In contrast, GNN is not affected by word frequency as severely and its performance gains are even greater for rare words. WAdad which is the multilingual baseline from (Imani Googhari et al., 2021) has the same trend as the GNN method, but the GNN method is more robust. , 2003), fast-align (Dyer et al., 2013) and Eflomal (\u00d6stling and Tiedemann, 2016). More recent work, including SimAlign (Jalili Sabet et al., 2020) and SHIFT-ATT/SHIFT-AET , uses pretrained neural language and machine translation models. Although neural models achieve superior performance compared to statistical aligners, they can only be used for fewer than two hundred high-resource languages that are supported by multilingual language models like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020). This makes statistical models the only option for the majority of the world's languages. Multiparallel Corpora. Prior applications of using multiparallel corpora include reliable translations from small datasets (Cohn and Lapata, 2007), and phrase-based machine translation (PBMT) (Kumar et al., 2007). Multiparallel corpora are also used for language comparison (Mayer and Cysouw, 2012), typological studies (\u00d6stling, 2015;Asgari and Sch\u00fctze, 2017) and PBMT (Nakov and Ng, 2012;Bertoldi et al., 2008;Dyer et al., 2013). ImaniGooghari et al. (2021) provide a tool to browse a word-aligned multiparallel corpus, which can be used for the comparative study of languages and for error analysis in machine translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_86",
            "content": "Annotation Projection",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "184-ARR_v2_87",
            "content": "To the best of our knowledge Lardilleux and Lepage (2008) and \u00d6stling (2014) 7 are the only word alignment methods designed for multiparallel corpora. However, the latter method is outperformed by Eflomal (\u00d6stling and Tiedemann, 2016), a bilingual method from the same author. Recently, Imani Googhari et al. (2021) proposed MPWA, which we use as our baseline.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_88",
            "content": "Graph Neural Networks (GNNs) have been used to address many problems that are inherently graph-like such as traffic networks, social networks, and physical and biological systems (Liu and Zhou, 2020). GNNs achieve impressive performance in many domains, including social networks (Wu et al., 2020) and natural science (Sanchez-Gonzalez et al., 2018) as well as NLP tasks like sentence classification (Huang et al., 2020), question generation (Pan et al., 2020), summarization (Fernandes et al., 2019) and derivational morphology (Hofmann et al., 2020). 7 github.com/robertostling/eflomal",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_89",
            "content": "Conclusion and Future Work",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "184-ARR_v2_90",
            "content": "We introduced graph neural networks and community detection algorithms for multiparallel word alignment. By incorporating signals from diverse sources as node features, including community features, our GNN model outperformed the baselines and prior work, establishing new state-of-the-art results on three PBC gold standard datasets. We also showed that our GNN model improves downstream task performance in low-resource languages through annotation projection.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_91",
            "content": "We have only used node features to provide signals to GNNs. In the future, other signals can be added in the form of edge features to further boost the performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "184-ARR_v2_92",
            "content": "\u017deljko Agi\u0107, Ivan Vuli\u0107, Jw300: A widecoverage parallel corpus for low-resource languages, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "\u017deljko Agi\u0107",
                    "Ivan Vuli\u0107"
                ],
                "title": "Jw300: A widecoverage parallel corpus for low-resource languages",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_93",
            "content": "Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, Roland Vollgraf, FLAIR: An easy-to-use framework for state-of-theart NLP, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Alan Akbik",
                    "Tanja Bergmann",
                    "Duncan Blythe",
                    "Kashif Rasul",
                    "Stefan Schweter",
                    "Roland Vollgraf"
                ],
                "title": "FLAIR: An easy-to-use framework for state-of-theart NLP",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_94",
            "content": "Alan Akbik, Duncan Blythe, Roland Vollgraf, Contextual string embeddings for sequence labeling, 2018, Proceedings of the 27th International Conference on Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Alan Akbik",
                    "Duncan Blythe",
                    "Roland Vollgraf"
                ],
                "title": "Contextual string embeddings for sequence labeling",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 27th International Conference on Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_95",
            "content": "Tamer Alkhouli, Gabriel Bretschner, Jan-Thorsten Peter, Mohammed Hethnawi, Andreas Guta, Hermann Ney, Alignment-based neural machine translation, 2016, Proceedings of the First Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Tamer Alkhouli",
                    "Gabriel Bretschner",
                    "Jan-Thorsten Peter",
                    "Mohammed Hethnawi",
                    "Andreas Guta",
                    "Hermann Ney"
                ],
                "title": "Alignment-based neural machine translation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the First Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_96",
            "content": "Tamer Alkhouli, Hermann Ney, Biasing attention-based recurrent neural networks using external alignment information, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Tamer Alkhouli",
                    "Hermann Ney"
                ],
                "title": "Biasing attention-based recurrent neural networks using external alignment information",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_97",
            "content": "Ehsaneddin Asgari, Hinrich Sch\u00fctze, Past, present, future: A computational investigation of the typology of tense in 1000 languages, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Ehsaneddin Asgari",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Past, present, future: A computational investigation of the typology of tense in 1000 languages",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_98",
            "content": "UNKNOWN, None, , Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_99",
            "content": "Bonnie Necip Fazil Ayan,  Dorr, A maximum entropy approach to combining word alignments, 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Bonnie Necip Fazil Ayan",
                    " Dorr"
                ],
                "title": "A maximum entropy approach to combining word alignments",
                "pub_date": "2006",
                "pub_title": "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_100",
            "content": "Nicola Bertoldi, Madalina Barbaiani, Phrase-based statistical machine translation with pivot languages, 2008, In International Workshop on Spoken Language Translation, IWSLT.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Nicola Bertoldi",
                    "Madalina Barbaiani"
                ],
                "title": "Phrase-based statistical machine translation with pivot languages",
                "pub_date": "2008",
                "pub_title": "In International Workshop on Spoken Language Translation",
                "pub": "IWSLT"
            }
        },
        {
            "ix": "184-ARR_v2_101",
            "content": "Paolo Boldi, Sebastiano Vigna, Axioms for centrality, 2014, Internet Mathematics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Paolo Boldi",
                    "Sebastiano Vigna"
                ],
                "title": "Axioms for centrality",
                "pub_date": "2014",
                "pub_title": "Internet Mathematics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_102",
            "content": ", A faster algorithm for betweenness centrality, 2001, Journal of mathematical sociology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [],
                "title": "A faster algorithm for betweenness centrality",
                "pub_date": "2001",
                "pub_title": "Journal of mathematical sociology",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_103",
            "content": "F Peter, Stephen Brown, Vincent Pietra, Robert Della Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "F Peter",
                    "Stephen Brown",
                    "Vincent Pietra",
                    "Robert Della Pietra",
                    " Mercer"
                ],
                "title": "The mathematics of statistical machine translation: Parameter estimation",
                "pub_date": "1993",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_104",
            "content": "Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, Qun Liu, Accurate word alignment induction from neural machine translation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Yun Chen",
                    "Yang Liu",
                    "Guanhua Chen",
                    "Xin Jiang",
                    "Qun Liu"
                ],
                "title": "Accurate word alignment induction from neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_105",
            "content": "Aaron Clauset, E Mark, Cristopher Newman,  Moore, Finding community structure in very large networks, 2004, Physical review E, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Aaron Clauset",
                    "E Mark",
                    "Cristopher Newman",
                    " Moore"
                ],
                "title": "Finding community structure in very large networks",
                "pub_date": "2004",
                "pub_title": "Physical review E",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_106",
            "content": "Trevor Cohn, Mirella Lapata, Machine translation by triangulation: Making effective use of multi-parallel corpora, 2007, Proceedings of the 45th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Trevor Cohn",
                    "Mirella Lapata"
                ],
                "title": "Machine translation by triangulation: Making effective use of multi-parallel corpora",
                "pub_date": "2007",
                "pub_title": "Proceedings of the 45th",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_107",
            "content": ", Czech Republic, , Annual Meeting of the Association of Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [],
                "title": "Czech Republic",
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association of Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_108",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Alexis Conneau",
                    "Kartikay Khandelwal",
                    "Naman Goyal",
                    "Vishrav Chaudhary",
                    "Guillaume Wenzek",
                    "Francisco Guzm\u00e1n",
                    "Edouard Grave",
                    "Myle Ott",
                    "Luke Zettlemoyer",
                    "Veselin Stoyanov"
                ],
                "title": "Unsupervised cross-lingual representation learning at scale",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_109",
            "content": "Gennaro Cordasco, Luisa Gargano, Community detection via semi-synchronous label propagation algorithms, 2010, 2010 IEEE international workshop on: business applications of social network analysis (BASNA), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Gennaro Cordasco",
                    "Luisa Gargano"
                ],
                "title": "Community detection via semi-synchronous label propagation algorithms",
                "pub_date": "2010",
                "pub_title": "2010 IEEE international workshop on: business applications of social network analysis (BASNA)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "184-ARR_v2_110",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "184-ARR_v2_111",
            "content": "Yi Zi, Graham Dou,  Neubig, Word alignment by fine-tuning embeddings on parallel corpora, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Yi Zi",
                    "Graham Dou",
                    " Neubig"
                ],
                "title": "Word alignment by fine-tuning embeddings on parallel corpora",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_112",
            "content": "Chris Dyer, Victor Chahuneau, Noah Smith, A simple, fast, and effective reparameterization of IBM model 2, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Chris Dyer",
                    "Victor Chahuneau",
                    "Noah Smith"
                ],
                "title": "A simple, fast, and effective reparameterization of IBM model 2",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_113",
            "content": "Patrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt, Structured neural summarization, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Patrick Fernandes",
                    "Miltiadis Allamanis",
                    "Marc Brockschmidt"
                ],
                "title": "Structured neural summarization",
                "pub_date": "2019-05-06",
                "pub_title": "7th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_114",
            "content": "Victoria Fossum, Steven Abney, Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora, 2005, Second International Joint Conference on Natural Language Processing: Full Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Victoria Fossum",
                    "Steven Abney"
                ],
                "title": "Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora",
                "pub_date": "2005",
                "pub_title": "Second International Joint Conference on Natural Language Processing: Full Papers",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_115",
            "content": "C Linton,  Freeman, Centrality in social networks conceptual clarification, 1978, Social networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "C Linton",
                    " Freeman"
                ],
                "title": "Centrality in social networks conceptual clarification",
                "pub_date": "1978",
                "pub_title": "Social networks",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_116",
            "content": "Michelle Girvan, E Mark,  Newman, Community structure in social and biological networks, 2002, Proceedings of the national academy of sciences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Michelle Girvan",
                    "E Mark",
                    " Newman"
                ],
                "title": "Community structure in social and biological networks",
                "pub_date": "2002",
                "pub_title": "Proceedings of the national academy of sciences",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_117",
            "content": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, Meng Wang, LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation, 2020, Association for Computing Machinery, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Xiangnan He",
                    "Kuan Deng",
                    "Xiang Wang",
                    "Yan Li",
                    "Yong-Dong Zhang",
                    "Meng Wang"
                ],
                "title": "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                "pub_date": "2020",
                "pub_title": "Association for Computing Machinery",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_118",
            "content": "Valentin Hofmann, Hinrich Sch\u00fctze, Janet , Pierrehumbert. 2020. A graph auto-encoder model of derivational morphology, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Valentin Hofmann",
                    "Hinrich Sch\u00fctze",
                    "Janet "
                ],
                "title": "Pierrehumbert. 2020. A graph auto-encoder model of derivational morphology",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_119",
            "content": "Lianzhe Huang, Xin Sun, Sujian Li, Linhao Zhang, Houfeng Wang, Syntax-aware graph attention network for aspect-level sentiment classification, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Lianzhe Huang",
                    "Xin Sun",
                    "Sujian Li",
                    "Linhao Zhang",
                    "Houfeng Wang"
                ],
                "title": "Syntax-aware graph attention network for aspect-level sentiment classification",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_120",
            "content": "Matthias Huck, Diana Dutka, Alexander Fraser, Cross-lingual annotation projection is effective for neural part-of-speech tagging, 2019, Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Matthias Huck",
                    "Diana Dutka",
                    "Alexander Fraser"
                ],
                "title": "Cross-lingual annotation projection is effective for neural part-of-speech tagging",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_121",
            "content": "Masoud Ayyoob Imani Googhari, Lutfi Sabet, Philipp Senel,  Dufter, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2021. Graph algorithms for multiparallel word alignment, , Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Masoud Ayyoob Imani Googhari",
                    "Lutfi Sabet",
                    "Philipp Senel",
                    " Dufter"
                ],
                "title": "Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2021. Graph algorithms for multiparallel word alignment",
                "pub_date": null,
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_122",
            "content": "Ayyoob Imanigooghari, Jalili Masoud, Philipp Sabet, Michael Dufter, Hinrich Cysou,  Sch\u00fctze, ParCourE: A parallel corpus explorer for a massively multilingual corpus, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Ayyoob Imanigooghari",
                    "Jalili Masoud",
                    "Philipp Sabet",
                    "Michael Dufter",
                    "Hinrich Cysou",
                    " Sch\u00fctze"
                ],
                "title": "ParCourE: A parallel corpus explorer for a massively multilingual corpus",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_123",
            "content": "Masoud Jalili Sabet, Philipp Dufter, Fran\u00e7ois Yvon, Hinrich Sch\u00fctze, SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Masoud Jalili Sabet",
                    "Philipp Dufter",
                    "Fran\u00e7ois Yvon",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings",
                "pub_date": "2020",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_124",
            "content": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, Monojit Choudhury, The state and fate of linguistic diversity and inclusion in the nlp world, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Pratik Joshi",
                    "Sebastin Santy",
                    "Amar Budhiraja",
                    "Kalika Bali",
                    "Monojit Choudhury"
                ],
                "title": "The state and fate of linguistic diversity and inclusion in the nlp world",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_125",
            "content": "UNKNOWN, None, 2016, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_126",
            "content": "N Thomas, Max Kipf,  Welling, Semisupervised classification with graph convolutional networks, 2017, International Conference on Learning Representations (ICLR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "N Thomas",
                    "Max Kipf",
                    " Welling"
                ],
                "title": "Semisupervised classification with graph convolutional networks",
                "pub_date": "2017",
                "pub_title": "International Conference on Learning Representations (ICLR)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_127",
            "content": "UNKNOWN, None, 2005, Edinburgh system description for the 2005 iwslt speech translation evaluation. International Workshop on Spoken Language Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": "2005",
                "pub_title": "Edinburgh system description for the 2005 iwslt speech translation evaluation. International Workshop on Spoken Language Translation",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_128",
            "content": "UNKNOWN, None, 2003, Statistical phrase-based translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": "2003",
                "pub_title": "Statistical phrase-based translation",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_129",
            "content": "Dan Kondratyuk, Milan Straka, 75 languages, 1 model: Parsing Universal Dependencies universally, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Dan Kondratyuk",
                    "Milan Straka"
                ],
                "title": "75 languages, 1 model: Parsing Universal Dependencies universally",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_130",
            "content": "Shankar Kumar, Franz Och, Wolfgang Macherey, Improving word alignment with bridge languages, 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Shankar Kumar",
                    "Franz Och",
                    "Wolfgang Macherey"
                ],
                "title": "Improving word alignment with bridge languages",
                "pub_date": "2007",
                "pub_title": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_131",
            "content": "Adrien Lardilleux, Yves Lepage, A truly multilingual, high coverage, accurate, yet simple, subsentential alignment method, 2008, The 8th conference of the Association for Machine Translation in the Americas (AMTA 2008), .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Adrien Lardilleux",
                    "Yves Lepage"
                ],
                "title": "A truly multilingual, high coverage, accurate, yet simple, subsentential alignment method",
                "pub_date": "2008",
                "pub_title": "The 8th conference of the Association for Machine Translation in the Americas (AMTA 2008)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_132",
            "content": "Omer Levy, Anders S\u00f8gaard, Yoav Goldberg, A strong baseline for learning cross-lingual word embeddings from sentence alignments, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Omer Levy",
                    "Anders S\u00f8gaard",
                    "Yoav Goldberg"
                ],
                "title": "A strong baseline for learning cross-lingual word embeddings from sentence alignments",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_133",
            "content": "D William, Fei Lewis,  Xia, Automatically identifying computationally relevant typological features, 2008, Proceedings of the Third International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "D William",
                    "Fei Lewis",
                    " Xia"
                ],
                "title": "Automatically identifying computationally relevant typological features",
                "pub_date": "2008",
                "pub_title": "Proceedings of the Third International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_134",
            "content": "Percy Liang, Ben Taskar, Dan Klein, Alignment by agreement, 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Percy Liang",
                    "Ben Taskar",
                    "Dan Klein"
                ],
                "title": "Alignment by agreement",
                "pub_date": "2006",
                "pub_title": "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_135",
            "content": "Zhiyuan Liu, Jie Zhou, Introduction to graph neural networks, 2020, Synthesis Lectures on Artificial Intelligence and Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Zhiyuan Liu",
                    "Jie Zhou"
                ],
                "title": "Introduction to graph neural networks",
                "pub_date": "2020",
                "pub_title": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_136",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_137",
            "content": "Thomas Mayer, Michael Cysouw, Language comparison through sparse multilingual word alignment, 2012, Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Thomas Mayer",
                    "Michael Cysouw"
                ],
                "title": "Language comparison through sparse multilingual word alignment",
                "pub_date": "2012",
                "pub_title": "Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_138",
            "content": "Thomas Mayer, Michael Cysouw, Creating a massively parallel bible corpus, 2014, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Thomas Mayer",
                    "Michael Cysouw"
                ],
                "title": "Creating a massively parallel bible corpus",
                "pub_date": "2014",
                "pub_title": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_139",
            "content": "UNKNOWN, None, 1998, Manual annotation of translational equivalence: The Blinker project, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": null,
                "title": null,
                "pub_date": "1998",
                "pub_title": "Manual annotation of translational equivalence: The Blinker project",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_140",
            "content": "Preslav Nakov, Hwee Tou Ng, Improving statistical machine translation for a resource-poor language using related resource-rich languages, 2012, Journal of Artificial Intelligence Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Preslav Nakov",
                    "Hwee Tou Ng"
                ],
                "title": "Improving statistical machine translation for a resource-poor language using related resource-rich languages",
                "pub_date": "2012",
                "pub_title": "Journal of Artificial Intelligence Research",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_141",
            "content": "E Mark,  Newman, Scientific collaboration networks. ii. shortest paths, weighted networks, and centrality, 2001, Physical review E, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "E Mark",
                    " Newman"
                ],
                "title": "Scientific collaboration networks. ii. shortest paths, weighted networks, and centrality",
                "pub_date": "2001",
                "pub_title": "Physical review E",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_142",
            "content": "E Mark, Michelle Newman,  Girvan, Finding and evaluating community structure in networks, 2004, Physical review E, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "E Mark",
                    "Michelle Newman",
                    " Girvan"
                ],
                "title": "Finding and evaluating community structure in networks",
                "pub_date": "2004",
                "pub_title": "Physical review E",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_143",
            "content": "Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Jan Haji\u010d, Christopher Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, Daniel Zeman, Universal Dependencies v2: An evergrowing multilingual treebank collection, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Joakim Nivre",
                    "Marie-Catherine De Marneffe",
                    "Filip Ginter",
                    "Jan Haji\u010d",
                    "Christopher Manning",
                    "Sampo Pyysalo",
                    "Sebastian Schuster",
                    "Francis Tyers",
                    "Daniel Zeman"
                ],
                "title": "Universal Dependencies v2: An evergrowing multilingual treebank collection",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 12th Language Resources and Evaluation Conference",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_144",
            "content": "Josef Franz, Hermann Och,  Ney, A systematic comparison of various statistical alignment models, 2003, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Josef Franz",
                    "Hermann Och",
                    " Ney"
                ],
                "title": "A systematic comparison of various statistical alignment models",
                "pub_date": "2003",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_145",
            "content": "Robert \u00d6stling, Bayesian word alignment for massively parallel texts, 2014, Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Robert \u00d6stling"
                ],
                "title": "Bayesian word alignment for massively parallel texts",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_146",
            "content": "Robert \u00d6stling, Word order typology through multilingual word alignment, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Robert \u00d6stling"
                ],
                "title": "Word order typology through multilingual word alignment",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "184-ARR_v2_147",
            "content": "Robert \u00d6stling, J\u00f6rg Tiedemann, Efficient word alignment with Markov Chain Monte Carlo, 2016, The Prague Bulletin of Mathematical Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Robert \u00d6stling",
                    "J\u00f6rg Tiedemann"
                ],
                "title": "Efficient word alignment with Markov Chain Monte Carlo",
                "pub_date": "2016",
                "pub_title": "The Prague Bulletin of Mathematical Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_148",
            "content": "Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, Min-Yen Kan, Semantic graphs for generating deep questions, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Liangming Pan",
                    "Yuxi Xie",
                    "Yansong Feng",
                    "Tat-Seng Chua",
                    "Min-Yen Kan"
                ],
                "title": "Semantic graphs for generating deep questions",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_149",
            "content": "Slav Petrov, Dipanjan Das, Ryan Mcdonald, A universal part-of-speech tagset, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Slav Petrov",
                    "Dipanjan Das",
                    "Ryan Mcdonald"
                ],
                "title": "A universal part-of-speech tagset",
                "pub_date": "2012",
                "pub_title": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_150",
            "content": "Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia, Graph networks as learnable physics engines for inference and control, 2018, Proceedings of the 35th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Alvaro Sanchez-Gonzalez",
                    "Nicolas Heess",
                    "Jost Springenberg",
                    "Josh Merel",
                    "Martin Riedmiller",
                    "Raia Hadsell",
                    "Peter Battaglia"
                ],
                "title": "Graph networks as learnable physics engines for inference and control",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 35th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "184-ARR_v2_151",
            "content": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, Gabriele Monfardini, The graph neural network model, 2009, IEEE Transactions on Neural Networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": [
                    "Franco Scarselli",
                    "Marco Gori",
                    "Ah Chung Tsoi",
                    "Markus Hagenbuchner",
                    "Gabriele Monfardini"
                ],
                "title": "The graph neural network model",
                "pub_date": "2009",
                "pub_title": "IEEE Transactions on Neural Networks",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_152",
            "content": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, Graph attention networks, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": [
                    "Petar Veli\u010dkovi\u0107",
                    "Guillem Cucurull",
                    "Arantxa Casanova",
                    "Adriana Romero",
                    "Pietro Li\u00f2",
                    "Yoshua Bengio"
                ],
                "title": "Graph attention networks",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_153",
            "content": "Guillaume Wisniewski, Nicolas P\u00e9cheux, Souhir Gahbiche-Braham, Fran\u00e7ois Yvon, Crosslingual part-of-speech tagging through ambiguous learning, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": [
                    "Guillaume Wisniewski",
                    "Nicolas P\u00e9cheux",
                    "Souhir Gahbiche-Braham",
                    "Fran\u00e7ois Yvon"
                ],
                "title": "Crosslingual part-of-speech tagging through ambiguous learning",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_154",
            "content": "Yongji Wu, Defu Lian, Yiheng Xu, Le Wu, Enhong Chen, Graph convolutional networks with markov random field reasoning for social spammer detection, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b62",
                "authors": [
                    "Yongji Wu",
                    "Defu Lian",
                    "Yiheng Xu",
                    "Le Wu",
                    "Enhong Chen"
                ],
                "title": "Graph convolutional networks with markov random field reasoning for social spammer detection",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_155",
            "content": "David Yarowsky, Grace Ngai, Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora, 2001, Second Meeting of the North American Chapter of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b63",
                "authors": [
                    "David Yarowsky",
                    "Grace Ngai"
                ],
                "title": "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora",
                "pub_date": "2001",
                "pub_title": "Second Meeting of the North American Chapter of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "184-ARR_v2_156",
            "content": "Anssi Yli-Jyr\u00e4, Josi Purhonen, Matti Liljeqvist, Arto Antturi, Pekka Nieminen, Kari R\u00e4ntil\u00e4, Valtter Luoto, HELFI: a Hebrew-Greek-Finnish parallel Bible corpus with cross-lingual morpheme alignment, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b64",
                "authors": [
                    "Anssi Yli-Jyr\u00e4",
                    "Josi Purhonen",
                    "Matti Liljeqvist",
                    "Arto Antturi",
                    "Pekka Nieminen",
                    "Kari R\u00e4ntil\u00e4",
                    "Valtter Luoto"
                ],
                "title": "HELFI: a Hebrew-Greek-Finnish parallel Bible corpus with cross-lingual morpheme alignment",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 12th Language Resources and Evaluation Conference",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "184-ARR_v2_0@0",
            "content": "Graph Neural Networks for Multiparallel Word Alignment",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_0",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_2@0",
            "content": "After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_2",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_2@1",
            "content": "Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_2",
            "start": 199,
            "end": 323,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_2@2",
            "content": "Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_2",
            "start": 325,
            "end": 445,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_2@3",
            "content": "First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_2",
            "start": 447,
            "end": 557,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_2@4",
            "content": "Next, we use graph neural networks (GNNs) to exploit the graph structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_2",
            "start": 559,
            "end": 631,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_2@5",
            "content": "Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) yields a prediction model that can generalize beyond the training sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_2",
            "start": 633,
            "end": 942,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_2@6",
            "content": "We show that community detection provides valuable information for multiparallel word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_2",
            "start": 944,
            "end": 1039,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_2@7",
            "content": "Our method outperforms previous work on three word alignment datasets and on a downstream task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_2",
            "start": 1041,
            "end": 1135,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_4@0",
            "content": "Word alignments are crucial for statistical machine translation (Koehn et al., 2003) and useful for many other multilingual tasks such as neural machine translation (Alkhouli and Ney, 2017;Alkhouli et al., 2016), typological analysis (Lewis and Xia, 2008;\u00d6stling, 2015;Asgari and Sch\u00fctze, 2017) and annotation projection (Yarowsky and Ngai, 2001;Fossum and Abney, 2005;Wisniewski et al., 2014;Huck et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_4",
            "start": 0,
            "end": 411,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_5@0",
            "content": "The rise of deep learning initially led to a temporary plateau, but interest in word alignments is now increasing, demonstrated by several recent publications (Jalili Sabet et al., 2020;Dou and Neubig, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_5",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_6@0",
            "content": "While word alignment is usually considered for bilingual corpora, our work addresses the problem Colors represent languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_6",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_6@1",
            "content": "Each English (yellow) node is annotated with its word.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_6",
            "start": 125,
            "end": 178,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_6@2",
            "content": "Red dashed lines cut links that incorrectly connect distinct concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_6",
            "start": 180,
            "end": 249,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_6@3",
            "content": "We exploit community detection algorithms to detect distinct concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_6",
            "start": 251,
            "end": 320,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_6@4",
            "content": "This provides valuable information for our GNN model and improves word alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_6",
            "start": 322,
            "end": 403,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_7@0",
            "content": "of word alignment in multiparallel corpora, containing sentence level parallel text in more than two languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_7",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_7@1",
            "content": "Examples of multiparallel corpora are JW300 (Agi\u0107 and Vuli\u0107, 2019), PBC (Mayer and Cysouw, 2014) which covers the highest number of languages (1334), and Tatoeba.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_7",
            "start": 112,
            "end": 273,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_7@2",
            "content": "1 While the perlanguage amount of data provided in such corpora is less than bilingual corpora, they support highly low-resource languages, many of which are not covered by existing language technologies (Joshi et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_7",
            "start": 275,
            "end": 499,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_7@3",
            "content": "Therefore, these corpora are essential for studying many of the world's low-resource languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_7",
            "start": 501,
            "end": 595,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_8@0",
            "content": "We consider the task of word alignment for multiparallel sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_8",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_8@1",
            "content": "The basic motivation is that the alignment between words in languages U and V can benefit from word-level alignments of U and V with a translation in a third language W .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_8",
            "start": 68,
            "end": 237,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_8@2",
            "content": "Following up on the work of Imani Googhari et al. (2021), we model multilingual word alignments with tools borrowed from graph theory (community detection algorithms) combined with neural network based models, specifically, the graph neural network (GNN) model of Scarselli et al. (2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_8",
            "start": 239,
            "end": 526,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_9@0",
            "content": "GNNs were proposed to extend the powerful current generation of neural network models to the processing of graph-structured data and they have gained increasing popularity in many domains (Wu et al., 2020;Sanchez-Gonzalez et al., 2018;He et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_9",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_9@1",
            "content": "GNNs can incorporate heterogeneous sources of signal in the form of node and edge features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_9",
            "start": 253,
            "end": 343,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_9@2",
            "content": "We use this property to take into account properties of the whole alignment graph, notably its tendency to cluster into communities, see Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_9",
            "start": 345,
            "end": 490,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_10@0",
            "content": "With our new proposed methods, we obtain improved results on word alignment for three language pairs: English-French, Finnish-Hebrew and Finnish-Greek.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_10",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_10@1",
            "content": "As a demonstration of the importance of high-quality alignments, we use our word alignments to project annotations from highresource to low-resource languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_10",
            "start": 152,
            "end": 310,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_10@2",
            "content": "We improve a part-of-speech tagger for Yoruba by training it over a high-quality dataset, which is created using annotation projection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_10",
            "start": 312,
            "end": 446,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_11@0",
            "content": "Contributions: i) We propose a graph neural network model to enhance word alignments in a multiparallel corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_11",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_11@1",
            "content": "The model incorporates a diverse set of features for word alignments in multiparallel corpora and an elegant way of training it efficiently and effectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_11",
            "start": 112,
            "end": 267,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_11@2",
            "content": "ii) We show that community detection improves multiparallel word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_11",
            "start": 269,
            "end": 343,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_11@3",
            "content": "iii) We show that the improved alignments improve performance on a downstream task for a low resource language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_11",
            "start": 345,
            "end": 455,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_11@4",
            "content": "iv) We propose a new method to infer alignments from the alignment probability matrix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_11",
            "start": 457,
            "end": 542,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_11@5",
            "content": "v) We will make our code publicly available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_11",
            "start": 544,
            "end": 587,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_12@0",
            "content": "MultiParallel Word Alignment Graphs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_12",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_13@0",
            "content": "Building MultiParallel Word Alignment Graphs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_13",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_14@0",
            "content": "Our starting point is the work of Imani Googhari et al. ( 2021), who introduce MPWA (MultiParallel Word Alignment), a framework that utilizes the synergy between multiple language pairs to improve bilingual word alignments for a target language pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_14",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_14@1",
            "content": "The rationale is that some of the missing alignment edges between a source and a target language can be recovered using their alignments with words in other languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_14",
            "start": 251,
            "end": 417,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_14@2",
            "content": "An MPWA graph is constructed using the following two steps:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_14",
            "start": 419,
            "end": 477,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_15@0",
            "content": "1. create initial bilingual alignments for all language pairs in a multiparallel corpus using a bilingual word aligner;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_15",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_16@0",
            "content": "2. represent the bilingual alignments for each multiparallel sentence in a graph containing one vertex for each token occurring in any language and one edge for each initial bilingual word alignment link.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_16",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_17@0",
            "content": "Potentially missing alignment links are then added based on the graph structure in an inference step, casting the word alignment task as an edge prediction problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_17",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_17@1",
            "content": "Figure 1 gives an example of a multiparallel word alignment graph for a 12-way multiparallel sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_17",
            "start": 165,
            "end": 266,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_17@2",
            "content": "Imani Googhari et al. ( 2021) use two traditional graph algorithms, Adamic-Adar and non-negative matrix factorization, for predicting new alignment edges from the MPWA graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_17",
            "start": 268,
            "end": 441,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_17@3",
            "content": "However, these graph algorithms are applied to individual multiparallel sentences independently and therefore cannot accumulate knowledge from multiple sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_17",
            "start": 443,
            "end": 604,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_17@4",
            "content": "Moreover, their edge predictions are solely based on the structure of the graph and do not take advantage of other beneficial signals such as a word's language, relative position and meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_17",
            "start": 606,
            "end": 796,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_17@5",
            "content": "Another limitation of this work is that it only adds links and does not remove any, which is important to improve precision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_17",
            "start": 798,
            "end": 921,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_18@0",
            "content": "This work addresses these shortcomings by using GNNs to predict alignment edges from MPWA graphs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_18",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_19@0",
            "content": "Community Detection in Alignment Graphs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_19",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_20@0",
            "content": "One important advantage of GNNs over traditional graph algorithms is that they can directly incorporate signals from different sources in the form of node and edge features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_20",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_20@1",
            "content": "We utilize this by taking into account the following observation: The nodes in the alignment graph are words in parallel sentences that are translations of each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_20",
            "start": 174,
            "end": 340,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_20@2",
            "content": "If the initial bilingual alignments are of good quality, we expect words that are mutual translations to form densely connected regions or communities; see Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_20",
            "start": 342,
            "end": 506,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_20@3",
            "content": "These communities should not be linked to each other, each corresponding to a distinct connected component.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_20",
            "start": 508,
            "end": 614,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_20@4",
            "content": "In other words, ideally, words representing a concept should be densely connected, but there should be no links between different concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_20",
            "start": 616,
            "end": 754,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_20@5",
            "content": "While, this intuition will not be true for all concepts between all possible language pairs, we nonetheless hypothesize that identifying distinct concepts in a multiparallel word alignment graph can provide useful information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_20",
            "start": 756,
            "end": 981,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_21@0",
            "content": "To examine to what extent these expectations are met, we count the components in the original Eflomal-generated (\u00d6stling and Tiedemann, 2016) graph (see \u00a74.2 for details on the initial alignments).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_21",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_21@1",
            "content": "Table 1 shows that the average number of components per sentence is less than three (\"Eflomal intersection\", columns #CC).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_21",
            "start": 198,
            "end": 319,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_21@2",
            "content": "But intuitively, the number of components should roughly correspond to sentence length (or, more precisely, the number of content words).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_21",
            "start": 321,
            "end": 457,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_21@3",
            "content": "This indicates that there are many links that incorrectly connect different concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_21",
            "start": 459,
            "end": 543,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_21@4",
            "content": "To detect such links, we use community detection (CD) algorithms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_21",
            "start": 545,
            "end": 609,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_22@0",
            "content": "CD algorithms find subnetworks of nodes that form tightly knit groups that are only loosely connected with a small number of links (Girvan and Newman, 2002).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_22",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_22@1",
            "content": "One well-known approach to CD attempts to maximize the modularity measure (Newman and Girvan, 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_22",
            "start": 158,
            "end": 257,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_22@2",
            "content": "Modularity assesses how beneficial a division of a community into two communities is, in the sense that there are many links within communities and only a few between them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_22",
            "start": 259,
            "end": 430,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_22@3",
            "content": "Given a graph G with n nodes and m edges and G's adjacency matrix A \u2208 R n\u00d7n , modularity is defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_22",
            "start": 432,
            "end": 534,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_23@0",
            "content": "mod = 1 2m ij A ij \u2212 \u03b3 d i d j 2m I(c i , c j ) (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_23",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_24@0",
            "content": "where d i is the degree of node i. I(c i , c j ) is 1 if nodes i and j are in the same community, 0 otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_24",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_24@1",
            "content": "As exact modularity maximization is intractable, we experiment with two CD algorithms implementing different heuristic approaches:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_24",
            "start": 111,
            "end": 240,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_25@0",
            "content": "\u2022 Greedy modularity communities (GMC). This method uses Clauset-Newman-Moore greedy modularity maximization (Clauset et al., 2004). GMC begins with each node in its own community and greedily joins the pair of communities that most increases modularity until no such pair exists. \u2022 Label propagation communities (LPC). This method finds communities in a graph using label propagation (Cordasco and Gargano, 2010). It begins by giving a label to each node of the network. Then each node's label is updated by the most frequent label among its neighbors in each iteration. It performs label propagation on a portion of nodes at each step and quickly converges to a stable labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_25",
            "start": 0,
            "end": 678,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_26@0",
            "content": "After detecting communities, we link all nodes inside a community and remove all intercommunity links.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_26",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_26@1",
            "content": "GMC (LPC) on average removes 3% (7%) of the edges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_26",
            "start": 103,
            "end": 152,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_26@2",
            "content": "Table 1 reports the average number of graph components per sentence before and after running GMC and LPC, as well as the corresponding F 1 for word alignment (see \u00a74.1 for details on the evaluation datasets).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_26",
            "start": 154,
            "end": 361,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_26@3",
            "content": "We see that the number of communities found is lower for GMC than for LPC; therefore, LPC identifies more candidate links for deletion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_26",
            "start": 363,
            "end": 497,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_26@4",
            "content": "2 Comparing the number of communities detected with the average sentence length, GMC seems to have failed to detect enough communities to split different concepts properly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_26",
            "start": 499,
            "end": 670,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_26@5",
            "content": "The F 1 scores confirm this observation and show that LPC performs well at detecting the communities we are looking for.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_26",
            "start": 672,
            "end": 791,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_27@0",
            "content": "This analysis shows that CD algorithms compute valuable information for word alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_27",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_27@1",
            "content": "To exploit this in our GNN model, we add node community information as a node feature; see \u00a73.1.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_27",
            "start": 89,
            "end": 186,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_28@0",
            "content": "Predicting and using MultiParallel",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_28",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_29@0",
            "content": "Word Alignments (MPWAs)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_29",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_30@0",
            "content": "GNNs for MPWA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_30",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_31@0",
            "content": "GNNs can be used in transductive or inductive settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_31",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_31@1",
            "content": "Transductively, the final model can only be used for inference over the same graph that it is trained on.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_31",
            "start": 56,
            "end": 160,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_31@2",
            "content": "In an inductive setting, which we use here, nodes are represented as feature vectors, and the final model has the advantage of being applicable to a different graph in inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_31",
            "start": 162,
            "end": 339,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_32@0",
            "content": "Below is the step-by-step overview of our GNNbased approach for an MPWA graph:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_32",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_33@0",
            "content": "1. run community detection algorithms on the initial graph ( \u00a72.2);",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_33",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_34@0",
            "content": "2. obtain features for the nodes of the graph ( \u00a73.1.3);",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_34",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_35@0",
            "content": "3. compute node embeddings from node features and initial alignment links in the GNN encoding step ( \u00a73.1.2);",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_35",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_36@0",
            "content": "4. learn to distinguish between nodes that are aligned together and that are not aligned together in the GNN decoding step ( \u00a73.1.2);",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_36",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_37@0",
            "content": "After the GNN model is trained on multiple MPWA graphs, it is used to infer an alignment probability matrix between tokens in a source language and tokens in a target language for a multiparallel sentence, see \u00a73.1.4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_37",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_37@1",
            "content": "Our method predicts new alignment links from this matrix, independently of initial edges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_37",
            "start": 218,
            "end": 306,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_37@2",
            "content": "Therefore, given an initial bilingual alignment, it is not limited to adding edges, but it can also remove them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_37",
            "start": 308,
            "end": 419,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_38@0",
            "content": "Model Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_38",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_39@0",
            "content": "Our model is inspired by the Graph Auto Encoder (GAE) model of Kipf and Welling (2016) for link prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_39",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_39@1",
            "content": "A GAE model consists of an encoder and a decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_39",
            "start": 108,
            "end": 156,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_39@2",
            "content": "The encoder creates a hidden representation for each node of the graph and the decoder predicts the links of the graph given the nodes' representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_39",
            "start": 158,
            "end": 309,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_39@3",
            "content": "Using the graph of word alignments, the model will learn the word alignment task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_39",
            "start": 311,
            "end": 391,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_39@4",
            "content": "Therefore it will be able to predict word alignments that are missed by the original bilingual word aligner and also detect incorrect alignment edges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_39",
            "start": 393,
            "end": 542,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_40@0",
            "content": "We make changes to this model to improve the model's quality and reduce its computational cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_40",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_40@1",
            "content": "We use GATConv layers (Veli\u010dkovi\u0107 et al., 2018) for the encoder instead of GCNConv (Kipf and Welling, 2017) and a more sophisticated decoder instead of simple dot product for a stronger model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_40",
            "start": 96,
            "end": 287,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_40@2",
            "content": "We also introduce a more efficient training procedure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_40",
            "start": 289,
            "end": 342,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_41@0",
            "content": "The encoder is a graph attention network (GAT) (Veli\u010dkovi\u0107 et al., 2018) with two GATConv layers followed by a fully connected layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_41",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_41@1",
            "content": "Layers are connected by RELU non-linearities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_41",
            "start": 134,
            "end": 178,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_41@2",
            "content": "A GATConv layer computes its output x i for a node i from its input x i as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_41",
            "start": 180,
            "end": 253,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_42@0",
            "content": "x i = \u03b1 i,i Wx i + j\u2208N (i) \u03b1 i,j Wx j ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_42",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_43@0",
            "content": "where W is a weight matrix, N (i) is some neighborhood of node i in the graph, and \u03b1 i,j is the attention coefficient indicating the importance of node j's features to node i. \u03b1 i,j is computed as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_43",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_44@0",
            "content": "\u03b1 i,j = exp g a [Wx i Wx j ] k\u2208N (i)\u222a{i} exp (g (a [Wx i Wx k ]))",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_44",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_45@0",
            "content": "(3) where is concatenation, g is LeakyReLU, and a is a weight vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_45",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_45@1",
            "content": "Given the features for the nodes and their alignment edges, the encoder creates a contextualized hidden representation for each node.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_45",
            "start": 70,
            "end": 202,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_46@0",
            "content": "Based on the hidden representations of two nodes, the decoder predicts whether a link connects them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_46",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_46@1",
            "content": "The decoder architecture consists of a fully connected layer, a RELU non-linearity and a sigmoid layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_46",
            "start": 101,
            "end": 203,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_47@0",
            "content": "Training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_47",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_48@0",
            "content": "By default, GAE models are trained using full batches with random negative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_48",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_48@1",
            "content": "This approach requires at least tens of epochs over the training dataset to converge and a lot of GPU memory for graphs as large as ours.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_48",
            "start": 84,
            "end": 220,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_48@2",
            "content": "We train our model using mini-batches to decrease memory requirements and improve the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_48",
            "start": 222,
            "end": 319,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_48@3",
            "content": "Using our training approach the model converges after one epoch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_48",
            "start": 321,
            "end": 384,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_48@4",
            "content": "We take care to select informative negative samples (as opposed to random selection) as described below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_48",
            "start": 386,
            "end": 489,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_49@0",
            "content": "Figure 2 displays our GNN model and the training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_49",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_49@1",
            "content": "The training set contains one graph for each sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_49",
            "start": 58,
            "end": 111,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_49@2",
            "content": "Each graph is divided into multiple batches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_49",
            "start": 113,
            "end": 156,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_49@3",
            "content": "Each batch contains a random subset of the graph's edges as positive samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_49",
            "start": 158,
            "end": 234,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_49@4",
            "content": "The negative samples are created as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_49",
            "start": 236,
            "end": 279,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_49@5",
            "content": "Given a sentence u 1 u 2 . . . u n in language U and its translation v 1 v 2 . . . v m in language V , for each alignment edge u i :v j in the current batch, two negative edges u i :v j and u i :v j (j = j, i = i) are randomly sampled.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_49",
            "start": 281,
            "end": 515,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_50@0",
            "content": "For each training batch, the encoder takes the batch's whole graph (i.e., node features for all graph nodes and all graph edges) as input and computes hidden representations for the nodes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_50",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_50@1",
            "content": "On the decoder side, for each link between two nodes in the batch, the hidden representations of the two nodes are concatenated to create the decoder's input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_50",
            "start": 189,
            "end": 346,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_50@2",
            "content": "The decoder's target is the link class: 1 (resp. 0) for positive (resp. negative) links.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_50",
            "start": 348,
            "end": 435,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_50@3",
            "content": "We train with a binary classification objective:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_50",
            "start": 437,
            "end": 484,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_51@0",
            "content": "L = \u2212 1 b b i=1 log(p + i ) + 1 2b 2b i=1 log(p \u2212 i ) (4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_51",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_52@0",
            "content": "where b is the batch size and p + i and p \u2212 i are the model predictions for the i th positive and negative samples within the batch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_52",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_52@1",
            "content": "Parameters of the encoder and decoder as well as the node-embedding feature layer are updated after each training step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_52",
            "start": 133,
            "end": 251,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_53@0",
            "content": "Node Features",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_53",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_54@0",
            "content": "We use three main types of node features: (i) graph structural features, (ii) community-based features and (iii) word content features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_54",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_55@0",
            "content": "Graph structural features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_55",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_55@1",
            "content": "We use degree, closeness (Freeman, 1978) , betweenness (Brandes, 2001) , load (Newman, 2001) and harmonic centrality (Boldi and Vigna, 2014) features as additional information about the graph structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_55",
            "start": 27,
            "end": 228,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_55@2",
            "content": "These features are continuous numbers, providing information about the position and connectivity of the nodes within the graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_55",
            "start": 230,
            "end": 356,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_55@3",
            "content": "We standardize (i.e., zscore) each feature across all nodes, and train an embedding of size four for each feature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_55",
            "start": 358,
            "end": 471,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_55@4",
            "content": "3 Community-based features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_55",
            "start": 473,
            "end": 499,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_55@5",
            "content": "One way to incorporate community information into our model is to 3 Learning a size-four embedding instead of a single number gives the feature a weight similar to other features -which have a feature vector of about the same size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_55",
            "start": 501,
            "end": 731,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_56@0",
            "content": "train the model based on a refined set of edges after the community detection step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_56",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_56@1",
            "content": "This approach hobbles the GNN model by making decisions about many of the edges before the GNN gets to see them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_56",
            "start": 84,
            "end": 195,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_56@2",
            "content": "Our initial experiments also confirmed that training the GNN over CD refined edges does not help.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_56",
            "start": 197,
            "end": 293,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_56@3",
            "content": "Therefore, we add community information as node features and let the GNN use them to improve its decisions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_56",
            "start": 295,
            "end": 401,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_56@4",
            "content": "We use the community detection algorithms GMC and LPC (see \u00a7 \u00a72.2) to identify communities in the graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_56",
            "start": 403,
            "end": 506,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_56@5",
            "content": "Then we represent the community membership information of the nodes as one-hot vectors and learn an embedding of size 32 for each of the two algorithms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_56",
            "start": 508,
            "end": 659,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_57@0",
            "content": "Word content features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_57",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_57@1",
            "content": "We train embeddings for word position (size 32) and word language (size 20).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_57",
            "start": 23,
            "end": 98,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_57@2",
            "content": "We learn 100-dimensional multilingual word embeddings using Levy et al. (2017)'s sentence-ID method on the 84 PBC languages selected by Imani Googhari et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_57",
            "start": 100,
            "end": 264,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_57@3",
            "content": "Word embeddings serve as initialization and are updated during GNN training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_57",
            "start": 266,
            "end": 341,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_58@0",
            "content": "After concatenating these features, each node is represented by a 236 dimensional vector that is then fed to the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_58",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_59@0",
            "content": "Inducing Bilingual Alignment Edges",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_59",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_60@0",
            "content": "Given a source sentence x = x 1 , x 2 , . . . , x m in language X and a target sentence \u0177 = y 1 , y 2 , . . . , y l in language Y , we feed all possible alignment links between source and target to the decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_60",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_60@1",
            "content": "This produces a symmetric alignment probability matrix S of size m \u00d7 l where S ij is the predicted alignment probability between words x i and y j .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_60",
            "start": 211,
            "end": 358,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_60@2",
            "content": "Using these values directly to infer alignment edges is usually suboptimal; therefore, more sophisticated methods have been suggested (Ayan and Dorr, 2006;Liang et al., 2006).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_60",
            "start": 360,
            "end": 534,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_60@3",
            "content": "Here we propose a new approach: it combines Koehn et al. (2005)'s Grow-Diag-Final-And (GDFA) with Dou and Neubig (2021)'s probability thresholding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_60",
            "start": 536,
            "end": 682,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_60@4",
            "content": "We modify the latter to account for the variable size of the probability matrix (i.e., length of source/target sentences).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_60",
            "start": 684,
            "end": 805,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_60@5",
            "content": "Our method is not limited to adding new edges to some initial bilingual alignments, a limitation of prior work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_60",
            "start": 807,
            "end": 917,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_60@6",
            "content": "As we predict each edge independently, some initial links can be discarded from the final alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_60",
            "start": 919,
            "end": 1018,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_61@0",
            "content": "We start by creating a set of forward (sourceto-target) alignment edges and a set of backward (target-to-source) alignment edges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_61",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_61@1",
            "content": "To this end, first, inspired by probability thresholding (Dou and Neubig, 2021), we apply softmax to S, and zero out probabilities below a threshold to get a sourceto-target probability matrix S XY :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_61",
            "start": 130,
            "end": 328,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_62@0",
            "content": "S XY = S * (softmax(S) > \u03b1 l ) (5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_62",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_63@0",
            "content": "Analogously, we compute the target-to-source probability matrix S Y X :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_63",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_64@0",
            "content": "S Y X = S * (softmax(S ) > \u03b1 m ) (6",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_64",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_65@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_65",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_66@0",
            "content": "where \u03b1 is a sensitivity hyperparameter, e.g., \u03b1 = 1 means that we pick edges with a probability higher than average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_66",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_66@1",
            "content": "We experimentally set \u03b1 = 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_66",
            "start": 118,
            "end": 145,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_66@2",
            "content": "Next, from each row of S XY (S Y X ), we pick the cell with the highest value (if any exists) and add this edge to the forward (backward) set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_66",
            "start": 147,
            "end": 288,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_66@3",
            "content": "We create the final set of alignment edges by applying the GDFA symmetrization method (Koehn et al., 2005) to forward and backward sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_66",
            "start": 290,
            "end": 425,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_66@4",
            "content": "The gist of GDFA is to use the intersection of forward and backward as initial alignment edges and add more edges from the union of forward and backward based on a number of heuristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_66",
            "start": 427,
            "end": 611,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_66@5",
            "content": "We call this method TGDFA (Thresholding GDFA).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_66",
            "start": 613,
            "end": 658,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_67@0",
            "content": "We also experiment with combining TGDFA with the original bilingual GDFA alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_67",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_67@1",
            "content": "We do so by adding bilingual GDFA edges to the union of forward and backward before performing the GDFA heuristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_67",
            "start": 85,
            "end": 199,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_67@2",
            "content": "We refer to these alignments as TGDFA+orig.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_67",
            "start": 201,
            "end": 243,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_68@0",
            "content": "We evaluate the resulting alignments using F 1 score and alignment error rate (AER), the standard metrics in the word alignment literature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_68",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_69@0",
            "content": "Annotation Projection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_69",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_70@0",
            "content": "Annotation projection automatically creates linguistically annotated corpora for low-resource lan-guages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_70",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_70@1",
            "content": "A model trained on data with \"annotationprojected\" labels can perform better than a completely unsupervised method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_70",
            "start": 106,
            "end": 220,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_70@2",
            "content": "Here, we focus on universal part-of-speech (UPOS) tagging (Petrov et al., 2012) for the low resource target language Yoruba; this language only has a small set of annotated sentences in Universal Dependencies (Nivre et al., 2020) and has poor POS results in unsupervised settings (Kondratyuk and Straka, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_70",
            "start": 222,
            "end": 531,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_71@0",
            "content": "The quality of the target annotated corpus depends on the quality of the annotations in the source languages and the quality of the word alignments between sources and target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_71",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_71@1",
            "content": "We use the Flair (Akbik et al., 2019) POS taggers for three high resource languages, English, German and French (Akbik et al., 2018), to annotate 30K verses whose Yoruba translations are available in PBC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_71",
            "start": 176,
            "end": 379,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_71@2",
            "content": "We then transfer the POS tags from source to target using three different approaches: (i) We directly transfer annotations from English to the target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_71",
            "start": 381,
            "end": 530,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_71@3",
            "content": "(ii) For each word in the target, we get its Eflomal bilingual alignments in the three source languages and predict the majority POS to annotate the target word. (iii) The same as in (ii), but we use our GNN (TGDFA) alignments (instead of Eflomal alignments) to project from source to target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_71",
            "start": 532,
            "end": 823,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_71@4",
            "content": "In all three approaches, we discard any target sentence from the POS tagger training data if more than 50% of its words are annotated with the \"X\" (other) tag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_71",
            "start": 825,
            "end": 983,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_72@0",
            "content": "We train a Flair SequenceTagger model on the target annotated data using mBERT embeddings (Devlin et al., 2019) Evaluation data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_72",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_72@1",
            "content": "For our main evaluation, we use the two word alignment gold datasets for PBC published by Imani Googhari et al. (2021): Blinker (Melamed, 1998) and HELFI (Yli-Jyr\u00e4 et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_72",
            "start": 129,
            "end": 306,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_72@2",
            "content": "The HELFI dataset contains the Hebrew Bible, Greek New Testament and their translations into Finnish.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_72",
            "start": 308,
            "end": 408,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_72@3",
            "content": "For HELFI, we use Imani Training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_72",
            "start": 410,
            "end": 447,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_72@4",
            "content": "The graph algorithms used by Imani Googhari et al. ( 2021) operate on each multiparallel sentence separately.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_72",
            "start": 449,
            "end": 557,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_72@5",
            "content": "In contrast, our approach allows for an inductive setting where a model is trained on a training set, accumulating knowledge from multiple multiparallel sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_72",
            "start": 559,
            "end": 721,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_72@6",
            "content": "We combine the verses in the training sets of Finnish-Hebrew and Finnish-Greek for a combined training set size of 24,159.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_72",
            "start": 723,
            "end": 844,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_72@7",
            "content": "5",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_72",
            "start": 846,
            "end": 846,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_73@0",
            "content": "Initial Word Alignments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_73",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_74@0",
            "content": "We use the Eflomal statistical word aligner to obtain bilingual alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_74",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_74@1",
            "content": "We train it for every language pair in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_74",
            "start": 76,
            "end": 130,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_74@2",
            "content": "We do not consider SimAlign (Jalili Sabet et al., 2020) since it is shown to perform poorly for languages whose representations in the multilingual pretrained language model are of low quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_74",
            "start": 132,
            "end": 324,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_74@3",
            "content": "We use Eflomal asymmetrical alignments post-processed with the intersection heuristic to get high precision bilingual alignments as input to the GNN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_74",
            "start": 326,
            "end": 474,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_74@4",
            "content": "We use the same subset of 84 languages as Imani Googhari et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_74",
            "start": 476,
            "end": 546,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_75@0",
            "content": "Training Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_75",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_76@0",
            "content": "We use PyTorch Geometric 6 to construct and train the GNN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_76",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_76@1",
            "content": "The model's hidden layer size is 512 for both GATConv and Linear layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_76",
            "start": 59,
            "end": 130,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_76@2",
            "content": "We train for one epoch on the training set -a small portion of the training set is enough to learn good embeddings (see \u00a75.1.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_76",
            "start": 132,
            "end": 259,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_76@3",
            "content": "For training, we use a batch size of 400 and learning rate of .001 with AdamW (Loshchilov and Hutter, 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_76",
            "start": 261,
            "end": 368,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_76@4",
            "content": "The whole training 5 Note that we do not use any gold alignments for training the GNN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_76",
            "start": 370,
            "end": 455,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_76@5",
            "content": "Using the verses from HELFI train split as our training set is for convenience.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_76",
            "start": 457,
            "end": 535,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_76@6",
            "content": "Our ablation experiment (Figure 3) show that a smaller subset of the training set is sufficient to achieve good performance 6 pytorch-geometric.readthedocs.io process takes less than 4 hours on a GeForce GTX 1080 Ti and the inference time is on the order of milliseconds per sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_76",
            "start": 537,
            "end": 820,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_77@0",
            "content": "5 Experiments and Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_77",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_78@0",
            "content": "Multiparallel corpus results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_78",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_79@0",
            "content": "Table 2 shows results on Blinker and HELFI for our GNNs and the baselines: bilingual alignments and two graph-based algorithms WAdAd and NMF from Imani Googhari et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_79",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_79@1",
            "content": "Our GNNs yield a better trade-off between precision and recall, most likely thanks to their ability to remove edges, and achieve the best F 1 and AER on all three datasets, outperforming WAdAd and NMF.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_79",
            "start": 176,
            "end": 376,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_79@2",
            "content": "GNN (TGDFA) achieves the best results on HELFI (FIN-HEB, FIN-GRC) while GNN (TGDFA+orig) is best on Blinker (ENG-FRA).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_79",
            "start": 378,
            "end": 495,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_79@3",
            "content": "As argued in Imani Googhari et al. (2021), this is mostly due to the different ways these two datasets were annotated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_79",
            "start": 497,
            "end": 614,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_79@4",
            "content": "Most HELFI alignments are one-to-one, while many Blinker alignments are many-to-many: phrase-level alignments where every word in a source phrase is aligned with every word in a target phrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_79",
            "start": 616,
            "end": 807,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_79@5",
            "content": "This suggests that one can choose between GNN (TGDFA) and GNN (TGDFA+orig) based on the desired characteristics of the alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_79",
            "start": 809,
            "end": 937,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_80@0",
            "content": "Effect of Training Set Size",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_80",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_81@0",
            "content": "To investigate the effect of training set size, we train the GNN on subsets of our training data with increasing sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_81",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_81@1",
            "content": "Figure 3 shows results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_81",
            "start": 120,
            "end": 142,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_81@2",
            "content": "Performance improves fast until around 2,000 verses; then it stays mostly constant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_81",
            "start": 144,
            "end": 226,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_81@3",
            "content": "Using more than 6,400 samples does not change the performance at all.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_81",
            "start": 228,
            "end": 296,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_81@4",
            "content": "Therefore, in the other experiments we use 6,400 randomly sampled verses from the training set to train GNNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_81",
            "start": 298,
            "end": 406,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_82@0",
            "content": "Ablation Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_82",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_83@0",
            "content": "To examine the importance of node features, we ablate language, position, centrality, community and word embedding features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_83",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_83@1",
            "content": "Table 3 shows that removal of graph structural features drastically reduces performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_83",
            "start": 125,
            "end": 212,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_83@2",
            "content": "Community features and language information are also important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_83",
            "start": 214,
            "end": 276,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_83@3",
            "content": "Removal of word position information and word embeddings -which store semantic information about wordshas the least effect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_83",
            "start": 278,
            "end": 400,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_83@4",
            "content": "Based on these results, it can be argued that the lexical information contained in the initial alignments and in the community features provides a strong signal regarding word relatedness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_83",
            "start": 402,
            "end": 589,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_83@5",
            "content": "The novel information that is crucial is about the overall graph structure which goes beyond the local word associations that are captured by word position and word embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_83",
            "start": 591,
            "end": 766,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_84@0",
            "content": "Effect of Word Frequency",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_84",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@0",
            "content": "We investigate the effect of word frequency on alignment performance where frequency is calculated based on the source word in the PBC; the first bin has the highest frequency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@1",
            "content": "Figure 4 shows that the performance of Eflomal drops with frequency and it struggles to align very rare words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 177,
            "end": 286,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@2",
            "content": "In contrast, GNN is not affected by word frequency as severely and its performance gains are even greater for rare words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 288,
            "end": 408,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@3",
            "content": "WAdad which is the multilingual baseline from (Imani Googhari et al., 2021) has the same trend as the GNN method, but the GNN method is more robust. , 2003), fast-align (Dyer et al., 2013) and Eflomal (\u00d6stling and Tiedemann, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 410,
            "end": 640,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@4",
            "content": "More recent work, including SimAlign (Jalili Sabet et al., 2020) and SHIFT-ATT/SHIFT-AET , uses pretrained neural language and machine translation models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 642,
            "end": 795,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@5",
            "content": "Although neural models achieve superior performance compared to statistical aligners, they can only be used for fewer than two hundred high-resource languages that are supported by multilingual language models like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 797,
            "end": 1071,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@6",
            "content": "This makes statistical models the only option for the majority of the world's languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 1073,
            "end": 1160,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@7",
            "content": "Multiparallel Corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 1162,
            "end": 1183,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@8",
            "content": "Prior applications of using multiparallel corpora include reliable translations from small datasets (Cohn and Lapata, 2007), and phrase-based machine translation (PBMT) (Kumar et al., 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 1185,
            "end": 1374,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@9",
            "content": "Multiparallel corpora are also used for language comparison (Mayer and Cysouw, 2012), typological studies (\u00d6stling, 2015;Asgari and Sch\u00fctze, 2017) and PBMT (Nakov and Ng, 2012;Bertoldi et al., 2008;Dyer et al., 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 1376,
            "end": 1592,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_85@10",
            "content": "ImaniGooghari et al. (2021) provide a tool to browse a word-aligned multiparallel corpus, which can be used for the comparative study of languages and for error analysis in machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_85",
            "start": 1594,
            "end": 1786,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_86@0",
            "content": "Annotation Projection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_86",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_87@0",
            "content": "To the best of our knowledge Lardilleux and Lepage (2008) and \u00d6stling (2014) 7 are the only word alignment methods designed for multiparallel corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_87",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_87@1",
            "content": "However, the latter method is outperformed by Eflomal (\u00d6stling and Tiedemann, 2016), a bilingual method from the same author.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_87",
            "start": 151,
            "end": 275,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_87@2",
            "content": "Recently, Imani Googhari et al. (2021) proposed MPWA, which we use as our baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_87",
            "start": 277,
            "end": 359,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_88@0",
            "content": "Graph Neural Networks (GNNs) have been used to address many problems that are inherently graph-like such as traffic networks, social networks, and physical and biological systems (Liu and Zhou, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_88",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_88@1",
            "content": "GNNs achieve impressive performance in many domains, including social networks (Wu et al., 2020) and natural science (Sanchez-Gonzalez et al., 2018) as well as NLP tasks like sentence classification (Huang et al., 2020), question generation (Pan et al., 2020), summarization (Fernandes et al., 2019) and derivational morphology (Hofmann et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_88",
            "start": 201,
            "end": 551,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_88@2",
            "content": "7 github.com/robertostling/eflomal",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_88",
            "start": 553,
            "end": 586,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_89@0",
            "content": "Conclusion and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_89",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_90@0",
            "content": "We introduced graph neural networks and community detection algorithms for multiparallel word alignment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_90",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_90@1",
            "content": "By incorporating signals from diverse sources as node features, including community features, our GNN model outperformed the baselines and prior work, establishing new state-of-the-art results on three PBC gold standard datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_90",
            "start": 105,
            "end": 333,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_90@2",
            "content": "We also showed that our GNN model improves downstream task performance in low-resource languages through annotation projection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_90",
            "start": 335,
            "end": 461,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_91@0",
            "content": "We have only used node features to provide signals to GNNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_91",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_91@1",
            "content": "In the future, other signals can be added in the form of edge features to further boost the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_91",
            "start": 60,
            "end": 163,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_92@0",
            "content": "\u017deljko Agi\u0107, Ivan Vuli\u0107, Jw300: A widecoverage parallel corpus for low-resource languages, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_92",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_93@0",
            "content": "Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, Roland Vollgraf, FLAIR: An easy-to-use framework for state-of-theart NLP, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_93",
            "start": 0,
            "end": 327,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_94@0",
            "content": "Alan Akbik, Duncan Blythe, Roland Vollgraf, Contextual string embeddings for sequence labeling, 2018, Proceedings of the 27th International Conference on Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_94",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_95@0",
            "content": "Tamer Alkhouli, Gabriel Bretschner, Jan-Thorsten Peter, Mohammed Hethnawi, Andreas Guta, Hermann Ney, Alignment-based neural machine translation, 2016, Proceedings of the First Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_95",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_96@0",
            "content": "Tamer Alkhouli, Hermann Ney, Biasing attention-based recurrent neural networks using external alignment information, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_96",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_97@0",
            "content": "Ehsaneddin Asgari, Hinrich Sch\u00fctze, Past, present, future: A computational investigation of the typology of tense in 1000 languages, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_97",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_98@0",
            "content": "UNKNOWN, None, , Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_98",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_99@0",
            "content": "Bonnie Necip Fazil Ayan,  Dorr, A maximum entropy approach to combining word alignments, 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_99",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_100@0",
            "content": "Nicola Bertoldi, Madalina Barbaiani, Phrase-based statistical machine translation with pivot languages, 2008, In International Workshop on Spoken Language Translation, IWSLT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_100",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_101@0",
            "content": "Paolo Boldi, Sebastiano Vigna, Axioms for centrality, 2014, Internet Mathematics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_101",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_102@0",
            "content": ", A faster algorithm for betweenness centrality, 2001, Journal of mathematical sociology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_102",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_103@0",
            "content": "F Peter, Stephen Brown, Vincent Pietra, Robert Della Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_103",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_104@0",
            "content": "Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, Qun Liu, Accurate word alignment induction from neural machine translation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_104",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_105@0",
            "content": "Aaron Clauset, E Mark, Cristopher Newman,  Moore, Finding community structure in very large networks, 2004, Physical review E, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_105",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_106@0",
            "content": "Trevor Cohn, Mirella Lapata, Machine translation by triangulation: Making effective use of multi-parallel corpora, 2007, Proceedings of the 45th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_106",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_107@0",
            "content": ", Czech Republic, , Annual Meeting of the Association of Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_107",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_108@0",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised cross-lingual representation learning at scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_108",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_109@0",
            "content": "Gennaro Cordasco, Luisa Gargano, Community detection via semi-synchronous label propagation algorithms, 2010, 2010 IEEE international workshop on: business applications of social network analysis (BASNA), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_109",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_110@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_110",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_111@0",
            "content": "Yi Zi, Graham Dou,  Neubig, Word alignment by fine-tuning embeddings on parallel corpora, 2021, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_111",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_112@0",
            "content": "Chris Dyer, Victor Chahuneau, Noah Smith, A simple, fast, and effective reparameterization of IBM model 2, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_112",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_113@0",
            "content": "Patrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt, Structured neural summarization, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_113",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_114@0",
            "content": "Victoria Fossum, Steven Abney, Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora, 2005, Second International Joint Conference on Natural Language Processing: Full Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_114",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_115@0",
            "content": "C Linton,  Freeman, Centrality in social networks conceptual clarification, 1978, Social networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_115",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_116@0",
            "content": "Michelle Girvan, E Mark,  Newman, Community structure in social and biological networks, 2002, Proceedings of the national academy of sciences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_116",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_117@0",
            "content": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, Meng Wang, LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation, 2020, Association for Computing Machinery, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_117",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_118@0",
            "content": "Valentin Hofmann, Hinrich Sch\u00fctze, Janet , Pierrehumbert. 2020. A graph auto-encoder model of derivational morphology, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_118",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_119@0",
            "content": "Lianzhe Huang, Xin Sun, Sujian Li, Linhao Zhang, Houfeng Wang, Syntax-aware graph attention network for aspect-level sentiment classification, 2020, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_119",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_120@0",
            "content": "Matthias Huck, Diana Dutka, Alexander Fraser, Cross-lingual annotation projection is effective for neural part-of-speech tagging, 2019, Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_120",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_121@0",
            "content": "Masoud Ayyoob Imani Googhari, Lutfi Sabet, Philipp Senel,  Dufter, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2021. Graph algorithms for multiparallel word alignment, , Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_121",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_122@0",
            "content": "Ayyoob Imanigooghari, Jalili Masoud, Philipp Sabet, Michael Dufter, Hinrich Cysou,  Sch\u00fctze, ParCourE: A parallel corpus explorer for a massively multilingual corpus, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_122",
            "start": 0,
            "end": 360,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_123@0",
            "content": "Masoud Jalili Sabet, Philipp Dufter, Fran\u00e7ois Yvon, Hinrich Sch\u00fctze, SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings, 2020, Findings of the Association for Computational Linguistics: EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_123",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_124@0",
            "content": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, Monojit Choudhury, The state and fate of linguistic diversity and inclusion in the nlp world, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_124",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_125@0",
            "content": "UNKNOWN, None, 2016, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_125",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_126@0",
            "content": "N Thomas, Max Kipf,  Welling, Semisupervised classification with graph convolutional networks, 2017, International Conference on Learning Representations (ICLR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_126",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_127@0",
            "content": "UNKNOWN, None, 2005, Edinburgh system description for the 2005 iwslt speech translation evaluation. International Workshop on Spoken Language Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_127",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_128@0",
            "content": "UNKNOWN, None, 2003, Statistical phrase-based translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_128",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_129@0",
            "content": "Dan Kondratyuk, Milan Straka, 75 languages, 1 model: Parsing Universal Dependencies universally, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_129",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_130@0",
            "content": "Shankar Kumar, Franz Och, Wolfgang Macherey, Improving word alignment with bridge languages, 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_130",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_131@0",
            "content": "Adrien Lardilleux, Yves Lepage, A truly multilingual, high coverage, accurate, yet simple, subsentential alignment method, 2008, The 8th conference of the Association for Machine Translation in the Americas (AMTA 2008), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_131",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_132@0",
            "content": "Omer Levy, Anders S\u00f8gaard, Yoav Goldberg, A strong baseline for learning cross-lingual word embeddings from sentence alignments, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_132",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_133@0",
            "content": "D William, Fei Lewis,  Xia, Automatically identifying computationally relevant typological features, 2008, Proceedings of the Third International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_133",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_134@0",
            "content": "Percy Liang, Ben Taskar, Dan Klein, Alignment by agreement, 2006, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_134",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_135@0",
            "content": "Zhiyuan Liu, Jie Zhou, Introduction to graph neural networks, 2020, Synthesis Lectures on Artificial Intelligence and Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_135",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_136@0",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_136",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_137@0",
            "content": "Thomas Mayer, Michael Cysouw, Language comparison through sparse multilingual word alignment, 2012, Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_137",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_138@0",
            "content": "Thomas Mayer, Michael Cysouw, Creating a massively parallel bible corpus, 2014, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_138",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_139@0",
            "content": "UNKNOWN, None, 1998, Manual annotation of translational equivalence: The Blinker project, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_139",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_140@0",
            "content": "Preslav Nakov, Hwee Tou Ng, Improving statistical machine translation for a resource-poor language using related resource-rich languages, 2012, Journal of Artificial Intelligence Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_140",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_141@0",
            "content": "E Mark,  Newman, Scientific collaboration networks. ii. shortest paths, weighted networks, and centrality, 2001, Physical review E, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_141",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_142@0",
            "content": "E Mark, Michelle Newman,  Girvan, Finding and evaluating community structure in networks, 2004, Physical review E, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_142",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_143@0",
            "content": "Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Jan Haji\u010d, Christopher Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, Daniel Zeman, Universal Dependencies v2: An evergrowing multilingual treebank collection, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_143",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_144@0",
            "content": "Josef Franz, Hermann Och,  Ney, A systematic comparison of various statistical alignment models, 2003, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_144",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_145@0",
            "content": "Robert \u00d6stling, Bayesian word alignment for massively parallel texts, 2014, Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_145",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_146@0",
            "content": "Robert \u00d6stling, Word order typology through multilingual word alignment, 2015, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_146",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_147@0",
            "content": "Robert \u00d6stling, J\u00f6rg Tiedemann, Efficient word alignment with Markov Chain Monte Carlo, 2016, The Prague Bulletin of Mathematical Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_147",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_148@0",
            "content": "Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, Min-Yen Kan, Semantic graphs for generating deep questions, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_148",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_149@0",
            "content": "Slav Petrov, Dipanjan Das, Ryan Mcdonald, A universal part-of-speech tagset, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_149",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_150@0",
            "content": "Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia, Graph networks as learnable physics engines for inference and control, 2018, Proceedings of the 35th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_150",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_151@0",
            "content": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, Gabriele Monfardini, The graph neural network model, 2009, IEEE Transactions on Neural Networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_151",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_152@0",
            "content": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, Yoshua Bengio, Graph attention networks, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_152",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_153@0",
            "content": "Guillaume Wisniewski, Nicolas P\u00e9cheux, Souhir Gahbiche-Braham, Fran\u00e7ois Yvon, Crosslingual part-of-speech tagging through ambiguous learning, 2014, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_153",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_154@0",
            "content": "Yongji Wu, Defu Lian, Yiheng Xu, Le Wu, Enhong Chen, Graph convolutional networks with markov random field reasoning for social spammer detection, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_154",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_155@0",
            "content": "David Yarowsky, Grace Ngai, Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora, 2001, Second Meeting of the North American Chapter of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_155",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "184-ARR_v2_156@0",
            "content": "Anssi Yli-Jyr\u00e4, Josi Purhonen, Matti Liljeqvist, Arto Antturi, Pekka Nieminen, Kari R\u00e4ntil\u00e4, Valtter Luoto, HELFI: a Hebrew-Greek-Finnish parallel Bible corpus with cross-lingual morpheme alignment, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "184-ARR_v2_156",
            "start": 0,
            "end": 275,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_1",
            "tgt_ix": "184-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_1",
            "tgt_ix": "184-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_2",
            "tgt_ix": "184-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_4",
            "tgt_ix": "184-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_5",
            "tgt_ix": "184-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_6",
            "tgt_ix": "184-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_7",
            "tgt_ix": "184-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_8",
            "tgt_ix": "184-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_9",
            "tgt_ix": "184-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_10",
            "tgt_ix": "184-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_3",
            "tgt_ix": "184-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_3",
            "tgt_ix": "184-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_3",
            "tgt_ix": "184-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_3",
            "tgt_ix": "184-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_3",
            "tgt_ix": "184-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_3",
            "tgt_ix": "184-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_3",
            "tgt_ix": "184-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_3",
            "tgt_ix": "184-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_3",
            "tgt_ix": "184-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_11",
            "tgt_ix": "184-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_12",
            "tgt_ix": "184-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_12",
            "tgt_ix": "184-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_14",
            "tgt_ix": "184-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_15",
            "tgt_ix": "184-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_16",
            "tgt_ix": "184-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_17",
            "tgt_ix": "184-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_13",
            "tgt_ix": "184-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_13",
            "tgt_ix": "184-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_13",
            "tgt_ix": "184-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_13",
            "tgt_ix": "184-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_13",
            "tgt_ix": "184-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_13",
            "tgt_ix": "184-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_12",
            "tgt_ix": "184-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_18",
            "tgt_ix": "184-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_20",
            "tgt_ix": "184-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_21",
            "tgt_ix": "184-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_22",
            "tgt_ix": "184-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_23",
            "tgt_ix": "184-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_24",
            "tgt_ix": "184-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_26",
            "tgt_ix": "184-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_19",
            "tgt_ix": "184-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_19",
            "tgt_ix": "184-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_19",
            "tgt_ix": "184-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_19",
            "tgt_ix": "184-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_19",
            "tgt_ix": "184-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_19",
            "tgt_ix": "184-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_19",
            "tgt_ix": "184-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_19",
            "tgt_ix": "184-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_19",
            "tgt_ix": "184-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_27",
            "tgt_ix": "184-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_28",
            "tgt_ix": "184-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_28",
            "tgt_ix": "184-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_28",
            "tgt_ix": "184-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_29",
            "tgt_ix": "184-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_31",
            "tgt_ix": "184-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_32",
            "tgt_ix": "184-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_33",
            "tgt_ix": "184-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_34",
            "tgt_ix": "184-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_35",
            "tgt_ix": "184-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_36",
            "tgt_ix": "184-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_30",
            "tgt_ix": "184-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_30",
            "tgt_ix": "184-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_30",
            "tgt_ix": "184-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_30",
            "tgt_ix": "184-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_30",
            "tgt_ix": "184-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_30",
            "tgt_ix": "184-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_30",
            "tgt_ix": "184-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_30",
            "tgt_ix": "184-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_28",
            "tgt_ix": "184-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_37",
            "tgt_ix": "184-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_39",
            "tgt_ix": "184-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_40",
            "tgt_ix": "184-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_41",
            "tgt_ix": "184-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_42",
            "tgt_ix": "184-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_43",
            "tgt_ix": "184-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_44",
            "tgt_ix": "184-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_45",
            "tgt_ix": "184-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_38",
            "tgt_ix": "184-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_38",
            "tgt_ix": "184-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_38",
            "tgt_ix": "184-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_38",
            "tgt_ix": "184-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_38",
            "tgt_ix": "184-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_38",
            "tgt_ix": "184-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_38",
            "tgt_ix": "184-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_38",
            "tgt_ix": "184-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_38",
            "tgt_ix": "184-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_28",
            "tgt_ix": "184-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_46",
            "tgt_ix": "184-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_48",
            "tgt_ix": "184-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_49",
            "tgt_ix": "184-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_50",
            "tgt_ix": "184-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_51",
            "tgt_ix": "184-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_47",
            "tgt_ix": "184-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_47",
            "tgt_ix": "184-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_47",
            "tgt_ix": "184-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_47",
            "tgt_ix": "184-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_47",
            "tgt_ix": "184-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_47",
            "tgt_ix": "184-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_28",
            "tgt_ix": "184-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_52",
            "tgt_ix": "184-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_54",
            "tgt_ix": "184-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_55",
            "tgt_ix": "184-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_56",
            "tgt_ix": "184-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_57",
            "tgt_ix": "184-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_53",
            "tgt_ix": "184-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_53",
            "tgt_ix": "184-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_53",
            "tgt_ix": "184-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_53",
            "tgt_ix": "184-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_53",
            "tgt_ix": "184-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_53",
            "tgt_ix": "184-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_28",
            "tgt_ix": "184-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_58",
            "tgt_ix": "184-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_60",
            "tgt_ix": "184-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_61",
            "tgt_ix": "184-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_62",
            "tgt_ix": "184-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_63",
            "tgt_ix": "184-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_64",
            "tgt_ix": "184-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_65",
            "tgt_ix": "184-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_66",
            "tgt_ix": "184-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_67",
            "tgt_ix": "184-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_28",
            "tgt_ix": "184-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_68",
            "tgt_ix": "184-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_70",
            "tgt_ix": "184-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_71",
            "tgt_ix": "184-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_69",
            "tgt_ix": "184-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_69",
            "tgt_ix": "184-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_69",
            "tgt_ix": "184-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_69",
            "tgt_ix": "184-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_72",
            "tgt_ix": "184-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_73",
            "tgt_ix": "184-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_73",
            "tgt_ix": "184-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_74",
            "tgt_ix": "184-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_76",
            "tgt_ix": "184-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_75",
            "tgt_ix": "184-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_75",
            "tgt_ix": "184-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_75",
            "tgt_ix": "184-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_77",
            "tgt_ix": "184-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_78",
            "tgt_ix": "184-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_78",
            "tgt_ix": "184-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_78",
            "tgt_ix": "184-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_79",
            "tgt_ix": "184-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_80",
            "tgt_ix": "184-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_80",
            "tgt_ix": "184-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_78",
            "tgt_ix": "184-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_81",
            "tgt_ix": "184-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_82",
            "tgt_ix": "184-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_82",
            "tgt_ix": "184-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_78",
            "tgt_ix": "184-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_83",
            "tgt_ix": "184-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_84",
            "tgt_ix": "184-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_84",
            "tgt_ix": "184-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_87",
            "tgt_ix": "184-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_86",
            "tgt_ix": "184-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_86",
            "tgt_ix": "184-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_86",
            "tgt_ix": "184-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_88",
            "tgt_ix": "184-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_90",
            "tgt_ix": "184-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_89",
            "tgt_ix": "184-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_89",
            "tgt_ix": "184-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_89",
            "tgt_ix": "184-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "184-ARR_v2_0",
            "tgt_ix": "184-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_1",
            "tgt_ix": "184-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_2",
            "tgt_ix": "184-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_2",
            "tgt_ix": "184-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_2",
            "tgt_ix": "184-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_2",
            "tgt_ix": "184-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_2",
            "tgt_ix": "184-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_2",
            "tgt_ix": "184-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_2",
            "tgt_ix": "184-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_2",
            "tgt_ix": "184-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_3",
            "tgt_ix": "184-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_4",
            "tgt_ix": "184-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_5",
            "tgt_ix": "184-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_6",
            "tgt_ix": "184-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_6",
            "tgt_ix": "184-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_6",
            "tgt_ix": "184-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_6",
            "tgt_ix": "184-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_6",
            "tgt_ix": "184-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_7",
            "tgt_ix": "184-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_7",
            "tgt_ix": "184-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_7",
            "tgt_ix": "184-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_7",
            "tgt_ix": "184-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_8",
            "tgt_ix": "184-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_8",
            "tgt_ix": "184-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_8",
            "tgt_ix": "184-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_9",
            "tgt_ix": "184-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_9",
            "tgt_ix": "184-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_9",
            "tgt_ix": "184-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_10",
            "tgt_ix": "184-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_10",
            "tgt_ix": "184-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_10",
            "tgt_ix": "184-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_11",
            "tgt_ix": "184-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_11",
            "tgt_ix": "184-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_11",
            "tgt_ix": "184-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_11",
            "tgt_ix": "184-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_11",
            "tgt_ix": "184-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_11",
            "tgt_ix": "184-ARR_v2_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_12",
            "tgt_ix": "184-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_13",
            "tgt_ix": "184-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_14",
            "tgt_ix": "184-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_14",
            "tgt_ix": "184-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_14",
            "tgt_ix": "184-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_15",
            "tgt_ix": "184-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_16",
            "tgt_ix": "184-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_17",
            "tgt_ix": "184-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_17",
            "tgt_ix": "184-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_17",
            "tgt_ix": "184-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_17",
            "tgt_ix": "184-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_17",
            "tgt_ix": "184-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_17",
            "tgt_ix": "184-ARR_v2_17@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_18",
            "tgt_ix": "184-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_19",
            "tgt_ix": "184-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_20",
            "tgt_ix": "184-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_20",
            "tgt_ix": "184-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_20",
            "tgt_ix": "184-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_20",
            "tgt_ix": "184-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_20",
            "tgt_ix": "184-ARR_v2_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_20",
            "tgt_ix": "184-ARR_v2_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_21",
            "tgt_ix": "184-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_21",
            "tgt_ix": "184-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_21",
            "tgt_ix": "184-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_21",
            "tgt_ix": "184-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_21",
            "tgt_ix": "184-ARR_v2_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_22",
            "tgt_ix": "184-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_22",
            "tgt_ix": "184-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_22",
            "tgt_ix": "184-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_22",
            "tgt_ix": "184-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_23",
            "tgt_ix": "184-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_24",
            "tgt_ix": "184-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_24",
            "tgt_ix": "184-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_25",
            "tgt_ix": "184-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_26",
            "tgt_ix": "184-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_26",
            "tgt_ix": "184-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_26",
            "tgt_ix": "184-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_26",
            "tgt_ix": "184-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_26",
            "tgt_ix": "184-ARR_v2_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_26",
            "tgt_ix": "184-ARR_v2_26@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_27",
            "tgt_ix": "184-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_27",
            "tgt_ix": "184-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_28",
            "tgt_ix": "184-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_29",
            "tgt_ix": "184-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_30",
            "tgt_ix": "184-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_31",
            "tgt_ix": "184-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_31",
            "tgt_ix": "184-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_31",
            "tgt_ix": "184-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_32",
            "tgt_ix": "184-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_33",
            "tgt_ix": "184-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_34",
            "tgt_ix": "184-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_35",
            "tgt_ix": "184-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_36",
            "tgt_ix": "184-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_37",
            "tgt_ix": "184-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_37",
            "tgt_ix": "184-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_37",
            "tgt_ix": "184-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_38",
            "tgt_ix": "184-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_39",
            "tgt_ix": "184-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_39",
            "tgt_ix": "184-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_39",
            "tgt_ix": "184-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_39",
            "tgt_ix": "184-ARR_v2_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_39",
            "tgt_ix": "184-ARR_v2_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_40",
            "tgt_ix": "184-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_40",
            "tgt_ix": "184-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_40",
            "tgt_ix": "184-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_41",
            "tgt_ix": "184-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_41",
            "tgt_ix": "184-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_41",
            "tgt_ix": "184-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_42",
            "tgt_ix": "184-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_43",
            "tgt_ix": "184-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_44",
            "tgt_ix": "184-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_45",
            "tgt_ix": "184-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_45",
            "tgt_ix": "184-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_46",
            "tgt_ix": "184-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_46",
            "tgt_ix": "184-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_47",
            "tgt_ix": "184-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_48",
            "tgt_ix": "184-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_48",
            "tgt_ix": "184-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_48",
            "tgt_ix": "184-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_48",
            "tgt_ix": "184-ARR_v2_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_48",
            "tgt_ix": "184-ARR_v2_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_49",
            "tgt_ix": "184-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_49",
            "tgt_ix": "184-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_49",
            "tgt_ix": "184-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_49",
            "tgt_ix": "184-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_49",
            "tgt_ix": "184-ARR_v2_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_49",
            "tgt_ix": "184-ARR_v2_49@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_50",
            "tgt_ix": "184-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_50",
            "tgt_ix": "184-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_50",
            "tgt_ix": "184-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_50",
            "tgt_ix": "184-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_51",
            "tgt_ix": "184-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_52",
            "tgt_ix": "184-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_52",
            "tgt_ix": "184-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_53",
            "tgt_ix": "184-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_54",
            "tgt_ix": "184-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_55",
            "tgt_ix": "184-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_55",
            "tgt_ix": "184-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_55",
            "tgt_ix": "184-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_55",
            "tgt_ix": "184-ARR_v2_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_55",
            "tgt_ix": "184-ARR_v2_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_55",
            "tgt_ix": "184-ARR_v2_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_56",
            "tgt_ix": "184-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_56",
            "tgt_ix": "184-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_56",
            "tgt_ix": "184-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_56",
            "tgt_ix": "184-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_56",
            "tgt_ix": "184-ARR_v2_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_56",
            "tgt_ix": "184-ARR_v2_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_57",
            "tgt_ix": "184-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_57",
            "tgt_ix": "184-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_57",
            "tgt_ix": "184-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_57",
            "tgt_ix": "184-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_58",
            "tgt_ix": "184-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_59",
            "tgt_ix": "184-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_60",
            "tgt_ix": "184-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_60",
            "tgt_ix": "184-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_60",
            "tgt_ix": "184-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_60",
            "tgt_ix": "184-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_60",
            "tgt_ix": "184-ARR_v2_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_60",
            "tgt_ix": "184-ARR_v2_60@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_60",
            "tgt_ix": "184-ARR_v2_60@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_61",
            "tgt_ix": "184-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_61",
            "tgt_ix": "184-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_62",
            "tgt_ix": "184-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_63",
            "tgt_ix": "184-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_64",
            "tgt_ix": "184-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_65",
            "tgt_ix": "184-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_66",
            "tgt_ix": "184-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_66",
            "tgt_ix": "184-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_66",
            "tgt_ix": "184-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_66",
            "tgt_ix": "184-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_66",
            "tgt_ix": "184-ARR_v2_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_66",
            "tgt_ix": "184-ARR_v2_66@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_67",
            "tgt_ix": "184-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_67",
            "tgt_ix": "184-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_67",
            "tgt_ix": "184-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_68",
            "tgt_ix": "184-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_69",
            "tgt_ix": "184-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_70",
            "tgt_ix": "184-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_70",
            "tgt_ix": "184-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_70",
            "tgt_ix": "184-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_71",
            "tgt_ix": "184-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_71",
            "tgt_ix": "184-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_71",
            "tgt_ix": "184-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_71",
            "tgt_ix": "184-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_71",
            "tgt_ix": "184-ARR_v2_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_72",
            "tgt_ix": "184-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_72",
            "tgt_ix": "184-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_72",
            "tgt_ix": "184-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_72",
            "tgt_ix": "184-ARR_v2_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_72",
            "tgt_ix": "184-ARR_v2_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_72",
            "tgt_ix": "184-ARR_v2_72@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_72",
            "tgt_ix": "184-ARR_v2_72@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_72",
            "tgt_ix": "184-ARR_v2_72@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_73",
            "tgt_ix": "184-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_74",
            "tgt_ix": "184-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_74",
            "tgt_ix": "184-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_74",
            "tgt_ix": "184-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_74",
            "tgt_ix": "184-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_74",
            "tgt_ix": "184-ARR_v2_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_75",
            "tgt_ix": "184-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_76",
            "tgt_ix": "184-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_76",
            "tgt_ix": "184-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_76",
            "tgt_ix": "184-ARR_v2_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_76",
            "tgt_ix": "184-ARR_v2_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_76",
            "tgt_ix": "184-ARR_v2_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_76",
            "tgt_ix": "184-ARR_v2_76@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_76",
            "tgt_ix": "184-ARR_v2_76@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_77",
            "tgt_ix": "184-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_78",
            "tgt_ix": "184-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_79",
            "tgt_ix": "184-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_79",
            "tgt_ix": "184-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_79",
            "tgt_ix": "184-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_79",
            "tgt_ix": "184-ARR_v2_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_79",
            "tgt_ix": "184-ARR_v2_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_79",
            "tgt_ix": "184-ARR_v2_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_80",
            "tgt_ix": "184-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_81",
            "tgt_ix": "184-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_81",
            "tgt_ix": "184-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_81",
            "tgt_ix": "184-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_81",
            "tgt_ix": "184-ARR_v2_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_81",
            "tgt_ix": "184-ARR_v2_81@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_82",
            "tgt_ix": "184-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_83",
            "tgt_ix": "184-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_83",
            "tgt_ix": "184-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_83",
            "tgt_ix": "184-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_83",
            "tgt_ix": "184-ARR_v2_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_83",
            "tgt_ix": "184-ARR_v2_83@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_83",
            "tgt_ix": "184-ARR_v2_83@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_84",
            "tgt_ix": "184-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_85",
            "tgt_ix": "184-ARR_v2_85@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_86",
            "tgt_ix": "184-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_87",
            "tgt_ix": "184-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_87",
            "tgt_ix": "184-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_87",
            "tgt_ix": "184-ARR_v2_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_88",
            "tgt_ix": "184-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_88",
            "tgt_ix": "184-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_88",
            "tgt_ix": "184-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_89",
            "tgt_ix": "184-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_90",
            "tgt_ix": "184-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_90",
            "tgt_ix": "184-ARR_v2_90@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_90",
            "tgt_ix": "184-ARR_v2_90@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_91",
            "tgt_ix": "184-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_91",
            "tgt_ix": "184-ARR_v2_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_92",
            "tgt_ix": "184-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_93",
            "tgt_ix": "184-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_94",
            "tgt_ix": "184-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_95",
            "tgt_ix": "184-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_96",
            "tgt_ix": "184-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_97",
            "tgt_ix": "184-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_98",
            "tgt_ix": "184-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_99",
            "tgt_ix": "184-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_100",
            "tgt_ix": "184-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_101",
            "tgt_ix": "184-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_102",
            "tgt_ix": "184-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_103",
            "tgt_ix": "184-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_104",
            "tgt_ix": "184-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_105",
            "tgt_ix": "184-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_106",
            "tgt_ix": "184-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_107",
            "tgt_ix": "184-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_108",
            "tgt_ix": "184-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_109",
            "tgt_ix": "184-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_110",
            "tgt_ix": "184-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_111",
            "tgt_ix": "184-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_112",
            "tgt_ix": "184-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_113",
            "tgt_ix": "184-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_114",
            "tgt_ix": "184-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_115",
            "tgt_ix": "184-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_116",
            "tgt_ix": "184-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_117",
            "tgt_ix": "184-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_118",
            "tgt_ix": "184-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_119",
            "tgt_ix": "184-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_120",
            "tgt_ix": "184-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_121",
            "tgt_ix": "184-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_122",
            "tgt_ix": "184-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_123",
            "tgt_ix": "184-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_124",
            "tgt_ix": "184-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_125",
            "tgt_ix": "184-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_126",
            "tgt_ix": "184-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_127",
            "tgt_ix": "184-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_128",
            "tgt_ix": "184-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_129",
            "tgt_ix": "184-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_130",
            "tgt_ix": "184-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_131",
            "tgt_ix": "184-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_132",
            "tgt_ix": "184-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_133",
            "tgt_ix": "184-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_134",
            "tgt_ix": "184-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_135",
            "tgt_ix": "184-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_136",
            "tgt_ix": "184-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_137",
            "tgt_ix": "184-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_138",
            "tgt_ix": "184-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_139",
            "tgt_ix": "184-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_140",
            "tgt_ix": "184-ARR_v2_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_141",
            "tgt_ix": "184-ARR_v2_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_142",
            "tgt_ix": "184-ARR_v2_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_143",
            "tgt_ix": "184-ARR_v2_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_144",
            "tgt_ix": "184-ARR_v2_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_145",
            "tgt_ix": "184-ARR_v2_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_146",
            "tgt_ix": "184-ARR_v2_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_147",
            "tgt_ix": "184-ARR_v2_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_148",
            "tgt_ix": "184-ARR_v2_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_149",
            "tgt_ix": "184-ARR_v2_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_150",
            "tgt_ix": "184-ARR_v2_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_151",
            "tgt_ix": "184-ARR_v2_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_152",
            "tgt_ix": "184-ARR_v2_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_153",
            "tgt_ix": "184-ARR_v2_153@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_154",
            "tgt_ix": "184-ARR_v2_154@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_155",
            "tgt_ix": "184-ARR_v2_155@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "184-ARR_v2_156",
            "tgt_ix": "184-ARR_v2_156@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 999,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "184-ARR",
        "version": 2
    }
}