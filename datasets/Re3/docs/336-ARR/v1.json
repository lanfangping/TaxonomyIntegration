{
    "nodes": [
        {
            "ix": "336-ARR_v1_0",
            "content": "Consistent Representation Learning for Continual Relation Extraction",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_2",
            "content": "Continual relation extraction (CRE) aims to continuously train a model on data with new relations while avoiding forgetting old ones. Some previous work has proved that storing a few typical samples of old relations and replaying them when learning new relations can effectively avoid forgetting. However, these memory-based methods tend to overfit the memory samples and perform poorly on imbalanced datasets. To solve these challenges, a consistent representation learning method is proposed, which maintains the stability of the relation embedding by adopting contrastive learning and knowledge distillation when replaying memory. Specifically, supervised contrastive learning based on a memory bank is first used to train each new task so that the model can effectively learn the relation representation. Then, contrastive replay is conducted of the samples in memory and makes the model retain the knowledge of historical relations through memory knowledge distillation to prevent the catastrophic forgetting of the old task. The proposed method can better learn consistent representations to alleviate forgetting effectively. Extensive experiments on FewRel and TACRED datasets show that our method significantly outperforms state-of-theart baselines and yield strong robustness on the imbalanced dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "336-ARR_v1_4",
            "content": "Relation extraction (RE) is an essential issue in information extraction (IE), which can apply to many downstream NLP tasks, such as information retrieval (Xiong et al., 2017) and question and answer (Tao et al., 2018). For example, given a sentence x with the annotated entities pairs e 1 and e 2 , the RE aims to identify the relations between e 1 and e 2 . However, traditional relation extraction models (Zhou et al., 2016;Soares et al., 2019a) always assume a fixed set of predefined relations and train on a fixed dataset, which cannot handle the growing relation types in real life well.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_5",
            "content": "To solve this situation, continual relation extraction (CRE) is introduced (Wang et al., 2019;Han et al., 2020;Wu et al., 2021;Cui et al., 2021). Compared with traditional relation extraction, CRE aims to help the model learn new relations while maintaining accurate classification of old ones. Wang et al. (2019) shows that continual relation learning needs to alleviate the catastrophic forgetting of old tasks when the model learns new tasks. Because neural networks need to retrain a fixed set of parameters with each training, the most efficient solution to the problem of catastrophic forgetting is to store all the historical data and retrain the model with all the data each time a new relational instance appears. This method can achieve the best effect in continual relation learning, but it is not adopted in real life due to the time and computing power costs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_6",
            "content": "Some recent works have proposed a variety of methods to alleviate the catastrophic forgetting problem in continual learning, including regularization methods (Kirkpatrick et al., 2017;Zenke et al., 2017;, dynamic architecture methods (Chen et al., 2015;Fernando et al., 2017), and memory-based methods (Lopez-Paz and Ranzato, 2017;Chaudhry et al., 2018). Although these methods have been verified in simple image classification tasks, previous works have proved that memory-based methods are the most effective in natural language processing applications (Wang et al., 2019;d'Autume et al., 2019). In recent years, the memory-based continual relation extraction model has made significant progress in alleviating the problem of catastrophic forgetting (Han et al., 2020;Wu et al., 2021;Cui et al., 2021). Wang et al. (2019) proposes a mechanism for embedding sentence alignment in memory maintenance to ensure the stability of the embedding space. Han et al. (2020) introduces a multi-round joint training process for memory consolidation. But these two methods only explore the problem of catastrophic forgetting in the overall performance of the task sequence. Wu et al. (2021) proposes to integrate curriculum learning. Although it is possible to analyze the characteristics of each subtask and the performance of the corresponding model, it still fails to make full use of the saved sample information. Cui et al. (2021) introduce an attention network to refine the prototype to better recover the interruption of the embedded space. However, this method will produce a bias in the classification of the old task as the new task continues to learn the classifier, which will affect the performance of the old task. Although the above method can alleviate catastrophic forgetting to a certain extent, it does not consider the consistency of relation embedding space.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_7",
            "content": "Because the performance of the model of CRE is sensitive to the quality of sample embedding, it needs to ensure that the learning of new tasks will not damage the embedding of old tasks. Inspired by supervised contrastive Learning (Khosla et al., 2020) to explicitly constrain data embeddings, a consistent representation learning method is proposed for continual relation extraction, which constrains the embedding of old tasks not to occur significantly change through supervised contrastive learning and knowledge distillation. Specifically, the example encoder first trains on the current task data through supervised contrastive learning based on memory bank, and then uses k-means to select representative samples to storage as memory after the training is completed. To relieve catastrophic forgetting, contrastive replay is used to train memorized samples. At the same time, to ensure that the embedding of historical relations does not undergo significant changes, knowledge distillation is used to make the embedding distribution of the new and old tasks consistent. In the testing phase, the nearest class mean (NCM) classifier is used to classify the test sample, which will not be affected by the deviation of the classifier.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_8",
            "content": "In summary, our contributions in this paper are summarized as follows: First, a novel CRE method is proposed, which uses supervised contrastive learning and knowledge distillation to learn consistent relation representations for continual learning. Second, consistent representation learning can ensure the stability of the relational embedding space to alleviate catastrophic forgetting and make full use of stored samples. Finally, extensive experiments results on FewRel and TACRED datasets show that the proposed method is better than the latest baseline and effectively mitigates catastrophic forgetting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_9",
            "content": "2 Related Work",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_10",
            "content": "Continual Learning",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "336-ARR_v1_11",
            "content": "Existing continual learning models mainly focus on three areas: (1) Regularization-based methods (Kirkpatrick et al., 2017;Zenke et al., 2017) impose constraints on updating neural weights important to previous tasks for relieving catastrophic forgetting. (2) Dynamic architecture methods (Chen et al., 2015;Fernando et al., 2017) extends the model architecture dynamically to learn new tasks and prevent forgetting old tasks effectively. However, these methods are unsuitable for NLP applications because the model size increases dramatically with increasing tasks. (3) Memory-based methods (Lopez-Paz and Ranzato, 2017;Aljundi et al., 2018;Chaudhry et al., 2018;Mai et al., 2021) saves some samples from old tasks and continuously learns them in new tasks to alleviate catastrophic forgetting. Dong et al. (2021) proposes a simple relational distillation incremental learning framework to balance retaining old knowledge and adapting to new knowledge. Yan et al. (2021) proposes a new two-stage learning method that uses dynamic expandable representation for more effective incremental conceptual modelling. Among these methods, memory-based methods are the most effective in NLP tasks (Wang et al., 2019;Sun et al., 2019;d'Autume et al., 2019). Inspired by the success of memory-based methods in the field of NLP, we use the framework of memory replay to learn new relations that are constantly emerging.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_12",
            "content": "Contrastive Learning",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "336-ARR_v1_13",
            "content": "Contrastive learning (CL) aims to make the representations of similar samples map closer to each other in the embedded space, while that of dissimilar samples should be farther away (Jaiswal et al., 2021). In recent years, the rise of CL has made great progress in self-supervised representation learning. He et al., 2020;Chen and He, 2021). The common point of these works is that no labels are available, so positive and negative pairs were formed through data augmentations. Recently, supervised contrastive learning (Khosla et al., 2020) has received much attention, which uses label information to extend contrastive learning. Hendrycks and Dietterich (2019) compares the supervised contrastive loss with the cross-entropy loss on the ImageNet-C dataset, and verifies that the supervised contrastive loss is not sensitive to the hyperparameter settings of the optimizer or data enhancement. Chen et al. (2020) proposed a contrastive learning framework for visual representations that does not require a special architecture or memory bank. Khosla et al. (2020) extend the self-supervised batch contrastive approach to the fully-supervised setting, which use supervised contrastive loss learning better represetation. Liu and Abbeel (2020) proposed a hybrid discriminant-generative training method based on an energy model. In this paper, contrastive learning is applied to continual relation extraction to extract better relation representation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_14",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "336-ARR_v1_15",
            "content": "Problem Formulation",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "336-ARR_v1_16",
            "content": "In continual relation extraction, given a series of K tasks {T 1 , T 2 , ..., T K }, where the k-th task has its own training set D k and relation set R k . Each task T k is a traditional supervised classification task, including a series of examples and their corresponding labels {(x i , y i )} N i=1 , where x i is the input data, including the natural language text and entity pair, and y i \u2208 R k is the relation label. The goal of continual relation learning is to train the model, which keeps learning new tasks while avoiding catastrophic forgetting of previous learning tasks. In other words, after learning the k-th task, the model can identify the relation of a given entity pair into Rk , where Rk = \u222a k i=1 R i is the relation set already observed till the k-th task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_17",
            "content": "In order to mitigate catastrophic forgetting in continual relational extraction, episodic memory modules have been used in previous work (Wang et al., 2019;Han et al., 2020;Cui et al., 2021), to store small samples in historical tasks. Inspired by (Cui et al., 2021), we store several representative samples for each relation. Therefore, the episodic memory module for the observed relations in",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_18",
            "content": "T 1 \u223c T k is Mk = \u222a r\u2208 Rk M r , where M r = {(x i , y i )} O i=1 ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_19",
            "content": "M k \u2190 M k\u22121 \u222a M ; 14: Rk \u2190 Rk\u22121 \u222a R k ; 15: if T k is not",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_20",
            "content": "M k \u2190 M k\u22121 \u222a M 26: end if 27: return E, M k , Rk ;",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_21",
            "content": "Framework",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "336-ARR_v1_22",
            "content": "The consistent representation learning (CRL) in the current task is described in Algorithm 1, which consists of three main steps: (1) r from D k . Then, the k-means algorithm is used to cluster the samples. The relation representation of the sample closest to the center is selected and stored in memory for each cluster. (3) Consistent representation learning (15 \u223c 24): In order to keep the embedding of historical relations in space consistent after learning new tasks, we perform contrastive replay and knowledge distillation constraints on the samples in memory.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_23",
            "content": "Encoder",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "336-ARR_v1_24",
            "content": "The key of CRE is to obtain a better relation representation. The pre-trained language model BERT (Devlin et al., 2019) shows a powerful ability in extracting contextual representation of text. Therefore, BERT is used to encode entity pairs and context information to get the relational representation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_25",
            "content": "Given a sentence x = [w 1 , . . . , w |x| ] and a pair of entities (E1, E2), we follow Soares et al. (2019b) augment x with four reserved word pieces to mark the begin and end of each entity mentioned in the sentence. The new token sequence is fed into BERT instead of x. To get the final relation representation between the two entities, the output corresponding to the positions of E1 and E2 are concatenated, and then map it to a high-dimensional hidden representation h \u2208 R d h , as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_26",
            "content": "h =W[h [E1] ; h [E2] ] + b,(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_27",
            "content": "where W \u2208 R 2d h \u00d7d h and b \u2208 R d h are trainable parameters. The encoder in which the abovementioned encoded sentence is a relation representation is denoted as E.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_28",
            "content": "Then, we use a projection head Proj to obtain the low-dimensional embedding:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_29",
            "content": "z =Proj(h),(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_30",
            "content": "where Proj(\u2022) = MLP(\u2022) is composed of two layers of neural networks. The normalized embedding z = z/||z|| is used for contrastive learning, and the hidden representation is used for classification.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_31",
            "content": "Inital training for new task",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "336-ARR_v1_32",
            "content": "Before training for each new task T k , we first use Encoder to extract the embedding z of the relational representation of each sentence in D k , and use them as the initialized memory bank M b :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_33",
            "content": "M b \u2190 {z i } N i=1 .(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_34",
            "content": "At the beginning of training, relation representation extraction is performed on each batch B. Then the data embedding is explicitly constrained by clustering through supervised contrastive learning (Khosla et al., 2020): After backpropagating the gradient of loss on each batch, we update the representation in the memory bank:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_35",
            "content": "L CL = i\u2208I \u22121 |P (i)| p\u2208P (i) log exp (z i \u2022 z p /\u03c4 ) j\u2208S I exp (z i \u2022 z j /\u03c4 ) ,(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_36",
            "content": "M b [ \u0128] \u2190 {z i } |B| i=1 .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_37",
            "content": "(",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_38",
            "content": ")5",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_39",
            "content": "where \u0128 is the corresponding index set of this batch of samples in M b . After epoch1 training set training, the model can learn a better relation representation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_40",
            "content": "Selecting Typical Samples for Memory",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "336-ARR_v1_41",
            "content": "In order to make the model not forget the relevant knowledge of the old task when it learns the new task, some samples need to be stored in M r . Inspired by (Han et al., 2020;Cui et al., 2021), we use k-means to cluster each relation, where the number of clusters is the number of samples that need to be stored for each class. Then, the relation representation closest to the center is selected and stored in memory for each cluster.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_42",
            "content": "Consistent Representation Learning",
            "ntype": "title",
            "meta": {
                "section": "3.6"
            }
        },
        {
            "ix": "336-ARR_v1_43",
            "content": "After learning a new task, the representation of the old relation in the space may change. In order to make the encoder not change the knowledge of the old task while learning the new task, we propose two replay strategies to learn consistent representation for alleviating this problem: contrastive replay and knowledge distillation. Figure 1 shows the main flow of consistent representation learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_44",
            "content": "Contrastive Replay with Memory Bank After the new task learning is over, we use the new task to train the encoder to further train the encoder by replaying the samples stored in memory M k . After the learning of the current task is over, we use the same method in Section 3.4 to replay the samples stored in memory M k .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_45",
            "content": "The difference here is that each batch uses all the samples in the entire memory bank for contrastive learning, as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_46",
            "content": "L CR = i\u2208I \u22121 |P (i)| p\u2208P (i) log exp (z i \u2022 z p /\u03c4 ) j\u2208 SI exp (z i \u2022 z j /\u03c4 ) ,(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_47",
            "content": "where SI represents the set of indices of all samples in Mb . Mb is the memory bank, which stores the normalized representation of all samples in M k .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_48",
            "content": "By replaying the samples in memory, the encoder can alleviate the forgetting of previously learned knowledge, and at the same time, consolidate the knowledge learned in the current task. However, contrastive replay allows the encoder to train on a small number of samples, which risks overfitting. On the other hand, it may change the distribution of relations in the previous task. Therefore, we propose knowledge distillation to make up for this shortcoming.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_49",
            "content": "We hope that the model can retain the semantic knowledge between relations in historical tasks. Therefore, before the encoder is trained on a task, we use the similarity metric between the relations in memory as Memory Knowledge. Then use the knowledge distillation to relieve the model from forgetting this knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_50",
            "content": "Specifically, the samples in the memory are encoded first, and then the prototype of each class is calculated:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_51",
            "content": "p c = O i=1 z c i ,(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_52",
            "content": "where O is the number of memory size, z c i is the relation representation belonging to class c. Then, the cosine similarity between the classes is calculated to represent the knowledge learned in the memory:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_53",
            "content": "a ij = p T i p j \u2225p i \u2225 \u2225p j \u2225 , (8",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_54",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_55",
            "content": "where a ij is the cosine similarity between prototype i and j.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_56",
            "content": "When performing memory replay, we use KL divergence to make the encoder retain the knowledge of the old task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_57",
            "content": "L KL = i KL(P i ||Q i ),(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_58",
            "content": "where",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_59",
            "content": "P i = {p ij } | Rk |",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_60",
            "content": "j=1 is the metric distribution of the prototype before training, and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_61",
            "content": "p ij = exp(a ij /\u03c4 ) j exp(a ij /\u03c4 ) . Similarly, Q i = {q ij } | Rk |",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_62",
            "content": "j=1 is the metric distribution of calculate the temporary prototype from the memory bank during training, and q ij = exp(\u00e3 ij /\u03c4 ) j exp(\u00e3 ij /\u03c4 ) . \u00e3 is the Embedding Knowledge of the memory M k , which is the cosine similarity between temporary prototypes. The temporary prototype is dynamically calculated in each batch based on the memory bank Mb .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_63",
            "content": "NCM for Prediction",
            "ntype": "title",
            "meta": {
                "section": "3.7"
            }
        },
        {
            "ix": "336-ARR_v1_64",
            "content": "To predict a label for a test sample x, the nearest class mean (NCM) compares the embedding of x with all the prototypes of memory and assigns the class label with the most similar prototype: Table 1: Accuracy (%) on all observed relations (which will continue to accumlate over time) at the stage of learning current task. The method marked by \u2020 represents the results generated from open source code 1 and the other baseline results copied from the original paper (Cui et al., 2021) 4 Experiments",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_65",
            "content": "p c = 1 n c i E (x i ) \u2022 \u22ae {y i = c} , y * =argmin c=1,...,k \u2225f (x) \u2212 p c \u2225 ,(",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_66",
            "content": "Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "336-ARR_v1_67",
            "content": "Our experiments are conducted on two benchmark datasets: in the experiment, the training-testvalidation that the split ratio is 3:1:1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_68",
            "content": "FewRel (Han et al., 2018) It is a RE dataset that contains 80 relations, each with 700 instances. Following the experimental settings by Wang et al. (2019), the original train and valid set of FewRel are used for experimental, which contains 80 classes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_69",
            "content": "TACRED (Zhang et al., 2017) It is a large-scale RE dataset containing 42 relations (including no relations) and 106,264 samples, built on news networks and online documents. Compared with FewRel, the samples in TACRED are imbalanced. Following Cui et al. (2021), the number of training samples for each relation is limited to 320 and the number of test samples of relation to 40.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_70",
            "content": "Evaluation Metrics",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "336-ARR_v1_71",
            "content": "Average accuracy is a better measure of the effect of catastrophic forgetting because it emphasizes the model's performance on earlier tasks (Han et al., 2020;Cui et al., 2021). This paper evaluates the 1 https://github.com/fd2014cl/RP-CRE model by using the average accuracy of K tasks at each step.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_72",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "336-ARR_v1_73",
            "content": "We evaluate CRL and several baselines on benchmarks for comparison:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_74",
            "content": "(1) EA-EMR (Wang et al., 2019) introduced a memory replay and embedding alignment mechanism to maintain memory and alleviate embedding distortion during training for new tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_75",
            "content": "(2) EMAR (Han et al., 2020) constructs a memory activation and reconsolidation mechanism to alleviate the catastrophic forgetting problem in CRE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_76",
            "content": "(3) CML (Wu et al., 2021) proposed a curriculum-meta learning method to alleviate the order sensitivity and catastrophic forgetting in CRE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_77",
            "content": "(4) RP-CRE (Cui et al., 2021) achieves enhanced performance by utilizing relation prototypes to refine sample embeddings, thereby effectively avoiding catastrophic forgetting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_78",
            "content": "Training Details and Parameters Setting",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "336-ARR_v1_79",
            "content": "A completely random sampling strategy at the relation level is adopted. It simulates ten tasks by randomly dividing all relations of the dataset into 10 sets to simulate 10 tasks, as suggested in (Cui et al., 2021). For a fair comparison, we set the random seed of the experiment to be the same as the seed in (Cui et al., 2021), so that the task sequence is exactly the same. Note that our reproduced model RP-CRE \u2020 and CRL use strictly the same experimental environment. In order to facilitate the reproduction of our experimental results, the proposed method source code and detailed hyperparameters are provided on https: //github.com/submitacl22/CRL.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_80",
            "content": "Results and Discussion",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "336-ARR_v1_81",
            "content": "Table 1 shows the results of the proposed methods and baselines ones compared on two datasets, where RP-CRE \u2020 is reproduced under the same conditions based on open source code. We also ablated knowledge distillation and contrastive replay for consistent representation learning. CRL (w/o KL) and CRL (w/o CR) respectively refer to removing knowledge distillation loss L KL and contrastive replay loss L CR when replaying memory. From the table, some conclusions can be drawn:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_82",
            "content": "(1) Our proposed CRL is significantly better than other baselines and achieves state-of-the-art performance. Compared with RP-CRE, our model also produces apparent advantages. It proves that CRL can learn better consistent relation representations and is more stable in the process of continual learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_83",
            "content": "(2) It is observed that all baselines perform worse on the TACRED dataset. The primary reason for this result is that TACRED is an imbalanced dataset. However, our model performs better than RP-CRE's last task on TACRED (3.4% higher than RP-CRE), which is more significant than the improvement (0.5%) on the class-balanced dataset FewRel. It shows that our model is more robust to scenarios with class-imbalanced.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_84",
            "content": "( (4) Comparing CRL and CRL (w/o CR), removing L during memory replay caused the model to drop 2.4% and 4.8% on FewRel and TACRED, respectively. The reason for the significant drop is that only adopting L KL cannot make the model review the samples of the current task, which leads to overfitting in the historical relations during replay.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_85",
            "content": "Effect of Memory Size",
            "ntype": "title",
            "meta": {
                "section": "4.6"
            }
        },
        {
            "ix": "336-ARR_v1_86",
            "content": "The memory size is the number of memory samples needed for each relation. In this section, we will study the impact of memory size on the performance of our model and RP-CRE. We compare three memory sizes: 5, 10, and 20. The experimental results are shown in Figure 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_87",
            "content": "We choose RP-CRE as the main competitor, where all configurations and task sequence remain unchanged. (1) As the size of the memory decreases, the performance of the model tends to decline, which shows that the size of the memory is a key factor that affects continuous learning and learning. But our model is more stable than RP-CRE (the performance gap in the final task), especially on the TACRED dataset. (2) On both FewRel and TACRED, CRL keeps the best performance under different memory sizes and produces obvious advantages in small memory. It indicates that utilizing consistent representation learning is a more effective way to utilize memory than the existing memory-based CRE method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_88",
            "content": "Effect of Consistent Representation Learning",
            "ntype": "title",
            "meta": {
                "section": "4.7"
            }
        },
        {
            "ix": "336-ARR_v1_89",
            "content": "In order to explore the long-term effects of consistency representation learning in continual relation extraction, we tested our model and RP-CRE on TACRED to observe the changes in the embedding space of old tasks as new tasks continue to increase.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_90",
            "content": "The model performs feature extraction on all samples in the test set in task 1 at the end of tasks 1, 4, 7, and 10. Then t-SNE is used to represent the dimensionality reduction relation representation. All samples on the test set of task 1 are drawn, where different color points represent different groundtruth labels. The visualization results are shown in Figure 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_91",
            "content": "From Figure 3, we can see that although the relation embeddings of RP-CRE are clustered and separated in each class after prototype refinement, as new tasks are continuously learned, the data embedding of task 1 is obviously scattered. In contrast, our model retains a good separation between classes, while the data embedding within classes is compact and has a certain diversity. In addition, we can see that our model has relatively stable changes in the distribution of different classes in task 1, and retains the knowledge of historical tasks with training. This is mainly because our model learns through supervised comparison, and explicitly emphasizes that the samples in historical memory are compact within the class and far away from each other. And the knowledge of historical memory is preserved through the distillation of memory knowledge. Because knowledge distillation preserves the distance distribution between classes, it can make up for the contrastive learning to over-optimize the distance between classes to prevent overfitting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_92",
            "content": "Conclusions and Future Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "336-ARR_v1_93",
            "content": "This paper proposes a novel consistent representation learning method for the CRE task, mainly through contrastive learning and knowledge distillation when replaying memory. Specifically, we use supervised comparative learning based on a memory bank to train each new task so that the model can effectively learn the feature representation. In addition, in order to prevent the catastrophic forgetting of the old task, we compare and replay the memory samples, and at the same time, make the model retain the knowledge of the relation between the historical tasks through the knowledge distillation. Our method can better learn consistent representations to alleviate catastrophic forgetting effectively. Extensive experiments on two benchmark data sets show that our method significantly improves the performance of the most advanced technology and demonstrates powerful representation learning capabilities. In the future, we will continue to study cross-domain continual relation extraction to acquire ever-increasing knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "336-ARR_v1_94",
            "content": "Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars, 2018, Proceedings of the European Conference on Computer Vision (ECCV), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Rahaf Aljundi",
                    "Francesca Babiloni"
                ],
                "title": "Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars",
                "pub_date": "2018",
                "pub_title": "Proceedings of the European Conference on Computer Vision (ECCV)",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_95",
            "content": "UNKNOWN, None, 2018, Efficient lifelong learning with a-gem, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Efficient lifelong learning with a-gem",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_96",
            "content": "UNKNOWN, None, 2015, Net2net: Accelerating learning via knowledge transfer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Net2net: Accelerating learning via knowledge transfer",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_97",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020, International conference on machine learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Ting Chen",
                    "Simon Kornblith",
                    "Mohammad Norouzi",
                    "Geoffrey Hinton"
                ],
                "title": "A simple framework for contrastive learning of visual representations",
                "pub_date": "2020",
                "pub_title": "International conference on machine learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "336-ARR_v1_98",
            "content": "Xinlei Chen, Kaiming He, Exploring simple siamese representation learning, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Xinlei Chen",
                    "Kaiming He"
                ],
                "title": "Exploring simple siamese representation learning",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_99",
            "content": "Li Cui, Deqing Yang, Jiaxin Yu, Chengwei Hu, Jiayang Cheng, Jingjie Yi, Yanghua Xiao, Refining sample embeddings with relation prototypes to enhance continual relation extraction, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Li Cui",
                    "Deqing Yang",
                    "Jiaxin Yu",
                    "Chengwei Hu",
                    "Jiayang Cheng",
                    "Jingjie Yi",
                    "Yanghua Xiao"
                ],
                "title": "Refining sample embeddings with relation prototypes to enhance continual relation extraction",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "336-ARR_v1_100",
            "content": "UNKNOWN, None, 2019, Episodic memory in lifelong language learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Episodic memory in lifelong language learning",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_101",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: pre-training of deep bidirectional transformers for language understanding, 2019-06-02, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019-06-02",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "336-ARR_v1_102",
            "content": "Songlin Dong, Xiaopeng Hong, Xiaoyu Tao, Xinyuan Chang, Xing Wei, Yihong Gong, Few-shot class-incremental learning via relation knowledge distillation, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Songlin Dong",
                    "Xiaopeng Hong",
                    "Xiaoyu Tao",
                    "Xinyuan Chang",
                    "Xing Wei",
                    "Yihong Gong"
                ],
                "title": "Few-shot class-incremental learning via relation knowledge distillation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_103",
            "content": "UNKNOWN, None, 2017, Pathnet: Evolution channels gradient descent in super neural networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Pathnet: Evolution channels gradient descent in super neural networks",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_104",
            "content": "Xu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou, Continual relation learning via episodic memory activation and reconsolidation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Xu Han",
                    "Yi Dai",
                    "Tianyu Gao",
                    "Yankai Lin",
                    "Zhiyuan Liu",
                    "Peng Li",
                    "Maosong Sun",
                    "Jie Zhou"
                ],
                "title": "Continual relation learning via episodic memory activation and reconsolidation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_105",
            "content": "Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, Maosong Sun, FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Xu Han",
                    "Hao Zhu",
                    "Pengfei Yu",
                    "Ziyun Wang",
                    "Yuan Yao",
                    "Zhiyuan Liu",
                    "Maosong Sun"
                ],
                "title": "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "336-ARR_v1_106",
            "content": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Momentum contrast for unsupervised visual representation learning, 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Kaiming He",
                    "Haoqi Fan",
                    "Yuxin Wu",
                    "Saining Xie",
                    "Ross Girshick"
                ],
                "title": "Momentum contrast for unsupervised visual representation learning",
                "pub_date": "2020",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_107",
            "content": "UNKNOWN, None, 2019, Benchmarking neural network robustness to common corruptions and perturbations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Benchmarking neural network robustness to common corruptions and perturbations",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_108",
            "content": "UNKNOWN, None, , Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. 2021. A survey on contrastive selfsupervised learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. 2021. A survey on contrastive selfsupervised learning",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_109",
            "content": "UNKNOWN, None, , Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_110",
            "content": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Overcoming catastrophic forgetting in neural networks, 2017, Proceedings of the national academy of sciences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "James Kirkpatrick",
                    "Razvan Pascanu",
                    "Neil Rabinowitz",
                    "Joel Veness",
                    "Guillaume Desjardins",
                    "Andrei Rusu",
                    "Kieran Milan",
                    "John Quan",
                    "Tiago Ramalho",
                    "Agnieszka Grabska-Barwinska"
                ],
                "title": "Overcoming catastrophic forgetting in neural networks",
                "pub_date": "2017",
                "pub_title": "Proceedings of the national academy of sciences",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_111",
            "content": "UNKNOWN, None, 2020, Prototypical contrastive learning of unsupervised representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Prototypical contrastive learning of unsupervised representations",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_112",
            "content": "UNKNOWN, None, 2020, Hybrid discriminative-generative training via contrastive learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Hybrid discriminative-generative training via contrastive learning",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_113",
            "content": "Xialei Liu, Marc Masana, Luis Herranz, Joost Van De, Antonio Weijer, Andrew Lopez,  Bagdanov, Rotate your networks: Better weight consolidation and less catastrophic forgetting, 2018, 24th International Conference on Pattern Recognition (ICPR), IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Xialei Liu",
                    "Marc Masana",
                    "Luis Herranz",
                    "Joost Van De",
                    "Antonio Weijer",
                    "Andrew Lopez",
                    " Bagdanov"
                ],
                "title": "Rotate your networks: Better weight consolidation and less catastrophic forgetting",
                "pub_date": "2018",
                "pub_title": "24th International Conference on Pattern Recognition (ICPR)",
                "pub": "IEEE"
            }
        },
        {
            "ix": "336-ARR_v1_114",
            "content": "David Lopez, - Paz, Marc'aurelio Ranzato, Gradient episodic memory for continual learning, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "David Lopez",
                    "- Paz",
                    "Marc'aurelio Ranzato"
                ],
                "title": "Gradient episodic memory for continual learning",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_115",
            "content": "Zheda Mai, Ruiwen Li, Hyunwoo Kim, Scott Sanner, Supervised contrastive replay: Revisiting the nearest class mean classifier in online classincremental continual learning, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Zheda Mai",
                    "Ruiwen Li",
                    "Hyunwoo Kim",
                    "Scott Sanner"
                ],
                "title": "Supervised contrastive replay: Revisiting the nearest class mean classifier in online classincremental continual learning",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_116",
            "content": "Livio Baldini, Nicholas Soares, Jeffrey Fitzgerald, Tom Ling,  Kwiatkowski, Matching the blanks: Distributional similarity for relation learning, 2019, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    " Livio Baldini",
                    "Nicholas Soares",
                    "Jeffrey Fitzgerald",
                    "Tom Ling",
                    " Kwiatkowski"
                ],
                "title": "Matching the blanks: Distributional similarity for relation learning",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_117",
            "content": "Livio Baldini, Nicholas Soares, Jeffrey Fitzgerald, Tom Ling,  Kwiatkowski, Matching the blanks: Distributional similarity for relation learning, 2019-07-28, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    " Livio Baldini",
                    "Nicholas Soares",
                    "Jeffrey Fitzgerald",
                    "Tom Ling",
                    " Kwiatkowski"
                ],
                "title": "Matching the blanks: Distributional similarity for relation learning",
                "pub_date": "2019-07-28",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_118",
            "content": "UNKNOWN, None, 2019, Lamol: Language modeling for lifelong language learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Lamol: Language modeling for lifelong language learning",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_119",
            "content": "Chongyang Tao, Shen Gao, Mingyue Shang, Wei Wu, Dongyan Zhao, Rui Yan, Get the point of my utterance! learning towards effective responses with multi-head attention mechanism, 2018-07-13, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Chongyang Tao",
                    "Shen Gao",
                    "Mingyue Shang",
                    "Wei Wu",
                    "Dongyan Zhao",
                    "Rui Yan"
                ],
                "title": "Get the point of my utterance! learning towards effective responses with multi-head attention mechanism",
                "pub_date": "2018-07-13",
                "pub_title": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_120",
            "content": "UNKNOWN, None, 2019, Sentence embedding alignment for lifelong relation extraction, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Sentence embedding alignment for lifelong relation extraction",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_121",
            "content": "UNKNOWN, None, 2021, Curriculum-meta learning for order-robust continual relation extraction, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Curriculum-meta learning for order-robust continual relation extraction",
                "pub": "CoRR"
            }
        },
        {
            "ix": "336-ARR_v1_122",
            "content": "Zhirong Wu, Yuanjun Xiong, X Stella, Dahua Yu,  Lin, Unsupervised feature learning via nonparametric instance discrimination, 2018, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Zhirong Wu",
                    "Yuanjun Xiong",
                    "X Stella",
                    "Dahua Yu",
                    " Lin"
                ],
                "title": "Unsupervised feature learning via nonparametric instance discrimination",
                "pub_date": "2018",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_123",
            "content": "Chenyan Xiong, Russell Power, Jamie Callan, Explicit semantic ranking for academic search via knowledge graph embedding, 2017-04-03, Proceedings of the 26th International Conference on World Wide Web, ACM.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Chenyan Xiong",
                    "Russell Power",
                    "Jamie Callan"
                ],
                "title": "Explicit semantic ranking for academic search via knowledge graph embedding",
                "pub_date": "2017-04-03",
                "pub_title": "Proceedings of the 26th International Conference on World Wide Web",
                "pub": "ACM"
            }
        },
        {
            "ix": "336-ARR_v1_124",
            "content": "Shipeng Yan, Jiangwei Xie, Xuming He, Der: Dynamically expandable representation for class incremental learning, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Shipeng Yan",
                    "Jiangwei Xie",
                    "Xuming He"
                ],
                "title": "Der: Dynamically expandable representation for class incremental learning",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_125",
            "content": "Friedemann Zenke, Ben Poole, Surya Ganguli, Continual learning through synaptic intelligence, 2017, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Friedemann Zenke",
                    "Ben Poole",
                    "Surya Ganguli"
                ],
                "title": "Continual learning through synaptic intelligence",
                "pub_date": "2017",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "336-ARR_v1_126",
            "content": "Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, Christopher D Manning, Position-aware attention and supervised data improve slot filling, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Yuhao Zhang",
                    "Victor Zhong",
                    "Danqi Chen",
                    "Gabor Angeli",
                    "Christopher D Manning"
                ],
                "title": "Position-aware attention and supervised data improve slot filling",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "336-ARR_v1_127",
            "content": "Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, Bo Xu, Attention-based bidirectional long short-term memory networks for relation classification, 2016, Proceedings of the 54th annual meeting of the association for computational linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Peng Zhou",
                    "Wei Shi",
                    "Jun Tian",
                    "Zhenyu Qi",
                    "Bingchen Li",
                    "Hongwei Hao",
                    "Bo Xu"
                ],
                "title": "Attention-based bidirectional long short-term memory networks for relation classification",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th annual meeting of the association for computational linguistics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "336-ARR_v1_0@0",
            "content": "Consistent Representation Learning for Continual Relation Extraction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_0",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_2@0",
            "content": "Continual relation extraction (CRE) aims to continuously train a model on data with new relations while avoiding forgetting old ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_2",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_2@1",
            "content": "Some previous work has proved that storing a few typical samples of old relations and replaying them when learning new relations can effectively avoid forgetting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_2",
            "start": 134,
            "end": 295,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_2@2",
            "content": "However, these memory-based methods tend to overfit the memory samples and perform poorly on imbalanced datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_2",
            "start": 297,
            "end": 409,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_2@3",
            "content": "To solve these challenges, a consistent representation learning method is proposed, which maintains the stability of the relation embedding by adopting contrastive learning and knowledge distillation when replaying memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_2",
            "start": 411,
            "end": 632,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_2@4",
            "content": "Specifically, supervised contrastive learning based on a memory bank is first used to train each new task so that the model can effectively learn the relation representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_2",
            "start": 634,
            "end": 807,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_2@5",
            "content": "Then, contrastive replay is conducted of the samples in memory and makes the model retain the knowledge of historical relations through memory knowledge distillation to prevent the catastrophic forgetting of the old task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_2",
            "start": 809,
            "end": 1029,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_2@6",
            "content": "The proposed method can better learn consistent representations to alleviate forgetting effectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_2",
            "start": 1031,
            "end": 1130,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_2@7",
            "content": "Extensive experiments on FewRel and TACRED datasets show that our method significantly outperforms state-of-theart baselines and yield strong robustness on the imbalanced dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_2",
            "start": 1132,
            "end": 1310,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_4@0",
            "content": "Relation extraction (RE) is an essential issue in information extraction (IE), which can apply to many downstream NLP tasks, such as information retrieval (Xiong et al., 2017) and question and answer (Tao et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_4",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_4@1",
            "content": "For example, given a sentence x with the annotated entities pairs e 1 and e 2 , the RE aims to identify the relations between e 1 and e 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_4",
            "start": 220,
            "end": 358,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_4@2",
            "content": "However, traditional relation extraction models (Zhou et al., 2016;Soares et al., 2019a) always assume a fixed set of predefined relations and train on a fixed dataset, which cannot handle the growing relation types in real life well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_4",
            "start": 360,
            "end": 593,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_5@0",
            "content": "To solve this situation, continual relation extraction (CRE) is introduced (Wang et al., 2019;Han et al., 2020;Wu et al., 2021;Cui et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_5",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_5@1",
            "content": "Compared with traditional relation extraction, CRE aims to help the model learn new relations while maintaining accurate classification of old ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_5",
            "start": 146,
            "end": 293,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_5@2",
            "content": "Wang et al. (2019) shows that continual relation learning needs to alleviate the catastrophic forgetting of old tasks when the model learns new tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_5",
            "start": 295,
            "end": 444,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_5@3",
            "content": "Because neural networks need to retrain a fixed set of parameters with each training, the most efficient solution to the problem of catastrophic forgetting is to store all the historical data and retrain the model with all the data each time a new relational instance appears.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_5",
            "start": 446,
            "end": 721,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_5@4",
            "content": "This method can achieve the best effect in continual relation learning, but it is not adopted in real life due to the time and computing power costs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_5",
            "start": 723,
            "end": 871,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_6@0",
            "content": "Some recent works have proposed a variety of methods to alleviate the catastrophic forgetting problem in continual learning, including regularization methods (Kirkpatrick et al., 2017;Zenke et al., 2017;, dynamic architecture methods (Chen et al., 2015;Fernando et al., 2017), and memory-based methods (Lopez-Paz and Ranzato, 2017;Chaudhry et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_6",
            "start": 0,
            "end": 353,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_6@1",
            "content": "Although these methods have been verified in simple image classification tasks, previous works have proved that memory-based methods are the most effective in natural language processing applications (Wang et al., 2019;d'Autume et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_6",
            "start": 355,
            "end": 596,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_6@2",
            "content": "In recent years, the memory-based continual relation extraction model has made significant progress in alleviating the problem of catastrophic forgetting (Han et al., 2020;Wu et al., 2021;Cui et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_6",
            "start": 598,
            "end": 803,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_6@3",
            "content": "Wang et al. (2019) proposes a mechanism for embedding sentence alignment in memory maintenance to ensure the stability of the embedding space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_6",
            "start": 805,
            "end": 946,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_6@4",
            "content": "Han et al. (2020) introduces a multi-round joint training process for memory consolidation. But these two methods only explore the problem of catastrophic forgetting in the overall performance of the task sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_6",
            "start": 948,
            "end": 1161,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_6@5",
            "content": "Wu et al. (2021) proposes to integrate curriculum learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_6",
            "start": 1163,
            "end": 1221,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_6@6",
            "content": "Although it is possible to analyze the characteristics of each subtask and the performance of the corresponding model, it still fails to make full use of the saved sample information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_6",
            "start": 1223,
            "end": 1405,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_6@7",
            "content": "Cui et al. (2021) introduce an attention network to refine the prototype to better recover the interruption of the embedded space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_6",
            "start": 1407,
            "end": 1536,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_6@8",
            "content": "However, this method will produce a bias in the classification of the old task as the new task continues to learn the classifier, which will affect the performance of the old task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_6",
            "start": 1538,
            "end": 1717,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_6@9",
            "content": "Although the above method can alleviate catastrophic forgetting to a certain extent, it does not consider the consistency of relation embedding space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_6",
            "start": 1719,
            "end": 1868,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_7@0",
            "content": "Because the performance of the model of CRE is sensitive to the quality of sample embedding, it needs to ensure that the learning of new tasks will not damage the embedding of old tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_7",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_7@1",
            "content": "Inspired by supervised contrastive Learning (Khosla et al., 2020) to explicitly constrain data embeddings, a consistent representation learning method is proposed for continual relation extraction, which constrains the embedding of old tasks not to occur significantly change through supervised contrastive learning and knowledge distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_7",
            "start": 187,
            "end": 529,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_7@2",
            "content": "Specifically, the example encoder first trains on the current task data through supervised contrastive learning based on memory bank, and then uses k-means to select representative samples to storage as memory after the training is completed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_7",
            "start": 531,
            "end": 772,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_7@3",
            "content": "To relieve catastrophic forgetting, contrastive replay is used to train memorized samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_7",
            "start": 774,
            "end": 863,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_7@4",
            "content": "At the same time, to ensure that the embedding of historical relations does not undergo significant changes, knowledge distillation is used to make the embedding distribution of the new and old tasks consistent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_7",
            "start": 865,
            "end": 1075,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_7@5",
            "content": "In the testing phase, the nearest class mean (NCM) classifier is used to classify the test sample, which will not be affected by the deviation of the classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_7",
            "start": 1077,
            "end": 1237,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_8@0",
            "content": "In summary, our contributions in this paper are summarized as follows: First, a novel CRE method is proposed, which uses supervised contrastive learning and knowledge distillation to learn consistent relation representations for continual learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_8",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_8@1",
            "content": "Second, consistent representation learning can ensure the stability of the relational embedding space to alleviate catastrophic forgetting and make full use of stored samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_8",
            "start": 249,
            "end": 423,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_8@2",
            "content": "Finally, extensive experiments results on FewRel and TACRED datasets show that the proposed method is better than the latest baseline and effectively mitigates catastrophic forgetting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_8",
            "start": 425,
            "end": 608,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_9@0",
            "content": "2 Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_9",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_10@0",
            "content": "Continual Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_10",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_11@0",
            "content": "Existing continual learning models mainly focus on three areas: (1) Regularization-based methods (Kirkpatrick et al., 2017;Zenke et al., 2017) impose constraints on updating neural weights important to previous tasks for relieving catastrophic forgetting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_11",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_11@1",
            "content": "(2) Dynamic architecture methods (Chen et al., 2015;Fernando et al., 2017) extends the model architecture dynamically to learn new tasks and prevent forgetting old tasks effectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_11",
            "start": 256,
            "end": 437,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_11@2",
            "content": "However, these methods are unsuitable for NLP applications because the model size increases dramatically with increasing tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_11",
            "start": 439,
            "end": 565,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_11@3",
            "content": "(3) Memory-based methods (Lopez-Paz and Ranzato, 2017;Aljundi et al., 2018;Chaudhry et al., 2018;Mai et al., 2021) saves some samples from old tasks and continuously learns them in new tasks to alleviate catastrophic forgetting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_11",
            "start": 567,
            "end": 794,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_11@4",
            "content": "Dong et al. (2021) proposes a simple relational distillation incremental learning framework to balance retaining old knowledge and adapting to new knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_11",
            "start": 796,
            "end": 952,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_11@5",
            "content": "Yan et al. (2021) proposes a new two-stage learning method that uses dynamic expandable representation for more effective incremental conceptual modelling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_11",
            "start": 954,
            "end": 1108,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_11@6",
            "content": "Among these methods, memory-based methods are the most effective in NLP tasks (Wang et al., 2019;Sun et al., 2019;d'Autume et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_11",
            "start": 1110,
            "end": 1246,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_11@7",
            "content": "Inspired by the success of memory-based methods in the field of NLP, we use the framework of memory replay to learn new relations that are constantly emerging.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_11",
            "start": 1248,
            "end": 1406,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_12@0",
            "content": "Contrastive Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_12",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_13@0",
            "content": "Contrastive learning (CL) aims to make the representations of similar samples map closer to each other in the embedded space, while that of dissimilar samples should be farther away (Jaiswal et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_13",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_13@1",
            "content": "In recent years, the rise of CL has made great progress in self-supervised representation learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_13",
            "start": 206,
            "end": 304,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_13@2",
            "content": "He et al., 2020;Chen and He, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_13",
            "start": 306,
            "end": 340,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_13@3",
            "content": "The common point of these works is that no labels are available, so positive and negative pairs were formed through data augmentations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_13",
            "start": 342,
            "end": 476,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_13@4",
            "content": "Recently, supervised contrastive learning (Khosla et al., 2020) has received much attention, which uses label information to extend contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_13",
            "start": 478,
            "end": 630,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_13@5",
            "content": "Hendrycks and Dietterich (2019) compares the supervised contrastive loss with the cross-entropy loss on the ImageNet-C dataset, and verifies that the supervised contrastive loss is not sensitive to the hyperparameter settings of the optimizer or data enhancement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_13",
            "start": 632,
            "end": 894,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_13@6",
            "content": "Chen et al. (2020) proposed a contrastive learning framework for visual representations that does not require a special architecture or memory bank.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_13",
            "start": 896,
            "end": 1043,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_13@7",
            "content": "Khosla et al. (2020) extend the self-supervised batch contrastive approach to the fully-supervised setting, which use supervised contrastive loss learning better represetation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_13",
            "start": 1045,
            "end": 1220,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_13@8",
            "content": "Liu and Abbeel (2020) proposed a hybrid discriminant-generative training method based on an energy model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_13",
            "start": 1222,
            "end": 1326,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_13@9",
            "content": "In this paper, contrastive learning is applied to continual relation extraction to extract better relation representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_13",
            "start": 1328,
            "end": 1449,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_14@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_14",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_15@0",
            "content": "Problem Formulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_15",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_16@0",
            "content": "In continual relation extraction, given a series of K tasks {T 1 , T 2 , ..., T K }, where the k-th task has its own training set D k and relation set R k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_16",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_16@1",
            "content": "Each task T k is a traditional supervised classification task, including a series of examples and their corresponding labels {(x i , y i )} N i=1 , where x i is the input data, including the natural language text and entity pair, and y i \u2208 R k is the relation label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_16",
            "start": 157,
            "end": 422,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_16@2",
            "content": "The goal of continual relation learning is to train the model, which keeps learning new tasks while avoiding catastrophic forgetting of previous learning tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_16",
            "start": 424,
            "end": 583,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_16@3",
            "content": "In other words, after learning the k-th task, the model can identify the relation of a given entity pair into Rk , where Rk = \u222a k i=1 R i is the relation set already observed till the k-th task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_16",
            "start": 585,
            "end": 778,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_17@0",
            "content": "In order to mitigate catastrophic forgetting in continual relational extraction, episodic memory modules have been used in previous work (Wang et al., 2019;Han et al., 2020;Cui et al., 2021), to store small samples in historical tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_17",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_17@1",
            "content": "Inspired by (Cui et al., 2021), we store several representative samples for each relation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_17",
            "start": 236,
            "end": 325,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_17@2",
            "content": "Therefore, the episodic memory module for the observed relations in",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_17",
            "start": 327,
            "end": 393,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_18@0",
            "content": "T 1 \u223c T k is Mk = \u222a r\u2208 Rk M r , where M r = {(x i , y i )} O i=1 ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_18",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_19@0",
            "content": "M k \u2190 M k\u22121 \u222a M ; 14: Rk \u2190 Rk\u22121 \u222a R k ; 15: if T k is not",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_19",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_20@0",
            "content": "M k \u2190 M k\u22121 \u222a M 26: end if 27: return E, M k , Rk ;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_20",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_21@0",
            "content": "Framework",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_21",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_22@0",
            "content": "The consistent representation learning (CRL) in the current task is described in Algorithm 1, which consists of three main steps: (1) r from D k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_22",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_22@1",
            "content": "Then, the k-means algorithm is used to cluster the samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_22",
            "start": 147,
            "end": 205,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_22@2",
            "content": "The relation representation of the sample closest to the center is selected and stored in memory for each cluster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_22",
            "start": 207,
            "end": 320,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_22@3",
            "content": "(3) Consistent representation learning (15 \u223c 24): In order to keep the embedding of historical relations in space consistent after learning new tasks, we perform contrastive replay and knowledge distillation constraints on the samples in memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_22",
            "start": 322,
            "end": 566,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_23@0",
            "content": "Encoder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_23",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_24@0",
            "content": "The key of CRE is to obtain a better relation representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_24",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_24@1",
            "content": "The pre-trained language model BERT (Devlin et al., 2019) shows a powerful ability in extracting contextual representation of text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_24",
            "start": 62,
            "end": 192,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_24@2",
            "content": "Therefore, BERT is used to encode entity pairs and context information to get the relational representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_24",
            "start": 194,
            "end": 301,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_25@0",
            "content": "Given a sentence x = [w 1 , . . . , w |x| ] and a pair of entities (E1, E2), we follow Soares et al. (2019b) augment x with four reserved word pieces to mark the begin and end of each entity mentioned in the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_25",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_25@1",
            "content": "The new token sequence is fed into BERT instead of x. To get the final relation representation between the two entities, the output corresponding to the positions of E1 and E2 are concatenated, and then map it to a high-dimensional hidden representation h \u2208 R d h , as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_25",
            "start": 218,
            "end": 494,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_26@0",
            "content": "h =W[h [E1] ; h [E2] ] + b,(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_26",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_27@0",
            "content": "where W \u2208 R 2d h \u00d7d h and b \u2208 R d h are trainable parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_27",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_27@1",
            "content": "The encoder in which the abovementioned encoded sentence is a relation representation is denoted as E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_27",
            "start": 62,
            "end": 163,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_28@0",
            "content": "Then, we use a projection head Proj to obtain the low-dimensional embedding:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_28",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_29@0",
            "content": "z =Proj(h),(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_29",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_30@0",
            "content": "where Proj(\u2022) = MLP(\u2022) is composed of two layers of neural networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_30",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_30@1",
            "content": "The normalized embedding z = z/||z|| is used for contrastive learning, and the hidden representation is used for classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_30",
            "start": 69,
            "end": 196,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_31@0",
            "content": "Inital training for new task",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_31",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_32@0",
            "content": "Before training for each new task T k , we first use Encoder to extract the embedding z of the relational representation of each sentence in D k , and use them as the initialized memory bank M b :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_32",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_33@0",
            "content": "M b \u2190 {z i } N i=1 .(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_33",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_34@0",
            "content": "At the beginning of training, relation representation extraction is performed on each batch B. Then the data embedding is explicitly constrained by clustering through supervised contrastive learning (Khosla et al., 2020): After backpropagating the gradient of loss on each batch, we update the representation in the memory bank:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_34",
            "start": 0,
            "end": 327,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_35@0",
            "content": "L CL = i\u2208I \u22121 |P (i)| p\u2208P (i) log exp (z i \u2022 z p /\u03c4 ) j\u2208S I exp (z i \u2022 z j /\u03c4 ) ,(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_35",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_36@0",
            "content": "M b [ \u0128] \u2190 {z i } |B| i=1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_36",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_37@0",
            "content": "(",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_37",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_38@0",
            "content": ")5",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_38",
            "start": 0,
            "end": 1,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_39@0",
            "content": "where \u0128 is the corresponding index set of this batch of samples in M b .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_39",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_39@1",
            "content": "After epoch1 training set training, the model can learn a better relation representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_39",
            "start": 73,
            "end": 161,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_40@0",
            "content": "Selecting Typical Samples for Memory",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_40",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_41@0",
            "content": "In order to make the model not forget the relevant knowledge of the old task when it learns the new task, some samples need to be stored in M r .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_41",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_41@1",
            "content": "Inspired by (Han et al., 2020;Cui et al., 2021), we use k-means to cluster each relation, where the number of clusters is the number of samples that need to be stored for each class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_41",
            "start": 146,
            "end": 327,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_41@2",
            "content": "Then, the relation representation closest to the center is selected and stored in memory for each cluster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_41",
            "start": 329,
            "end": 434,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_42@0",
            "content": "Consistent Representation Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_42",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_43@0",
            "content": "After learning a new task, the representation of the old relation in the space may change.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_43",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_43@1",
            "content": "In order to make the encoder not change the knowledge of the old task while learning the new task, we propose two replay strategies to learn consistent representation for alleviating this problem: contrastive replay and knowledge distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_43",
            "start": 91,
            "end": 333,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_43@2",
            "content": "Figure 1 shows the main flow of consistent representation learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_43",
            "start": 335,
            "end": 401,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_44@0",
            "content": "Contrastive Replay with Memory Bank After the new task learning is over, we use the new task to train the encoder to further train the encoder by replaying the samples stored in memory M k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_44",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_44@1",
            "content": "After the learning of the current task is over, we use the same method in Section 3.4 to replay the samples stored in memory M k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_44",
            "start": 191,
            "end": 320,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_45@0",
            "content": "The difference here is that each batch uses all the samples in the entire memory bank for contrastive learning, as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_45",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_46@0",
            "content": "L CR = i\u2208I \u22121 |P (i)| p\u2208P (i) log exp (z i \u2022 z p /\u03c4 ) j\u2208 SI exp (z i \u2022 z j /\u03c4 ) ,(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_46",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_47@0",
            "content": "where SI represents the set of indices of all samples in Mb .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_47",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_47@1",
            "content": "Mb is the memory bank, which stores the normalized representation of all samples in M k .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_47",
            "start": 62,
            "end": 150,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_48@0",
            "content": "By replaying the samples in memory, the encoder can alleviate the forgetting of previously learned knowledge, and at the same time, consolidate the knowledge learned in the current task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_48",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_48@1",
            "content": "However, contrastive replay allows the encoder to train on a small number of samples, which risks overfitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_48",
            "start": 187,
            "end": 296,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_48@2",
            "content": "On the other hand, it may change the distribution of relations in the previous task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_48",
            "start": 298,
            "end": 381,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_48@3",
            "content": "Therefore, we propose knowledge distillation to make up for this shortcoming.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_48",
            "start": 383,
            "end": 459,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_49@0",
            "content": "We hope that the model can retain the semantic knowledge between relations in historical tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_49",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_49@1",
            "content": "Therefore, before the encoder is trained on a task, we use the similarity metric between the relations in memory as Memory Knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_49",
            "start": 96,
            "end": 228,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_49@2",
            "content": "Then use the knowledge distillation to relieve the model from forgetting this knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_49",
            "start": 230,
            "end": 317,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_50@0",
            "content": "Specifically, the samples in the memory are encoded first, and then the prototype of each class is calculated:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_50",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_51@0",
            "content": "p c = O i=1 z c i ,(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_51",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_52@0",
            "content": "where O is the number of memory size, z c i is the relation representation belonging to class c. Then, the cosine similarity between the classes is calculated to represent the knowledge learned in the memory:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_52",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_53@0",
            "content": "a ij = p T i p j \u2225p i \u2225 \u2225p j \u2225 , (8",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_53",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_54@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_54",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_55@0",
            "content": "where a ij is the cosine similarity between prototype i and j.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_55",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_56@0",
            "content": "When performing memory replay, we use KL divergence to make the encoder retain the knowledge of the old task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_56",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_57@0",
            "content": "L KL = i KL(P i ||Q i ),(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_57",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_58@0",
            "content": "where",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_58",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_59@0",
            "content": "P i = {p ij } | Rk |",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_59",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_60@0",
            "content": "j=1 is the metric distribution of the prototype before training, and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_60",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_61@0",
            "content": "p ij = exp(a ij /\u03c4 ) j exp(a ij /\u03c4 ) . Similarly, Q i = {q ij } | Rk |",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_61",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_62@0",
            "content": "j=1 is the metric distribution of calculate the temporary prototype from the memory bank during training, and q ij = exp(\u00e3 ij /\u03c4 ) j exp(\u00e3 ij /\u03c4 ) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_62",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_62@1",
            "content": "\u00e3 is the Embedding Knowledge of the memory M k , which is the cosine similarity between temporary prototypes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_62",
            "start": 149,
            "end": 257,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_62@2",
            "content": "The temporary prototype is dynamically calculated in each batch based on the memory bank Mb .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_62",
            "start": 259,
            "end": 351,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_63@0",
            "content": "NCM for Prediction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_63",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_64@0",
            "content": "To predict a label for a test sample x, the nearest class mean (NCM) compares the embedding of x with all the prototypes of memory and assigns the class label with the most similar prototype: Table 1: Accuracy (%) on all observed relations (which will continue to accumlate over time) at the stage of learning current task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_64",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_64@1",
            "content": "The method marked by \u2020 represents the results generated from open source code 1 and the other baseline results copied from the original paper (Cui et al., 2021) 4 Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_64",
            "start": 324,
            "end": 497,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_65@0",
            "content": "p c = 1 n c i E (x i ) \u2022 \u22ae {y i = c} , y * =argmin c=1,...,k \u2225f (x) \u2212 p c \u2225 ,(",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_65",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_66@0",
            "content": "Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_66",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_67@0",
            "content": "Our experiments are conducted on two benchmark datasets: in the experiment, the training-testvalidation that the split ratio is 3:1:1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_67",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_68@0",
            "content": "FewRel (Han et al., 2018) It is a RE dataset that contains 80 relations, each with 700 instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_68",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_68@1",
            "content": "Following the experimental settings by Wang et al. (2019), the original train and valid set of FewRel are used for experimental, which contains 80 classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_68",
            "start": 98,
            "end": 252,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_69@0",
            "content": "TACRED (Zhang et al., 2017) It is a large-scale RE dataset containing 42 relations (including no relations) and 106,264 samples, built on news networks and online documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_69",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_69@1",
            "content": "Compared with FewRel, the samples in TACRED are imbalanced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_69",
            "start": 174,
            "end": 232,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_69@2",
            "content": "Following Cui et al. (2021), the number of training samples for each relation is limited to 320 and the number of test samples of relation to 40.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_69",
            "start": 234,
            "end": 378,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_70@0",
            "content": "Evaluation Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_70",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_71@0",
            "content": "Average accuracy is a better measure of the effect of catastrophic forgetting because it emphasizes the model's performance on earlier tasks (Han et al., 2020;Cui et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_71",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_71@1",
            "content": "This paper evaluates the 1 https://github.com/fd2014cl/RP-CRE model by using the average accuracy of K tasks at each step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_71",
            "start": 178,
            "end": 299,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_72@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_72",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_73@0",
            "content": "We evaluate CRL and several baselines on benchmarks for comparison:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_73",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_74@0",
            "content": "(1) EA-EMR (Wang et al., 2019) introduced a memory replay and embedding alignment mechanism to maintain memory and alleviate embedding distortion during training for new tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_74",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_75@0",
            "content": "(2) EMAR (Han et al., 2020) constructs a memory activation and reconsolidation mechanism to alleviate the catastrophic forgetting problem in CRE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_75",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_76@0",
            "content": "(3) CML (Wu et al., 2021) proposed a curriculum-meta learning method to alleviate the order sensitivity and catastrophic forgetting in CRE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_76",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_77@0",
            "content": "(4) RP-CRE (Cui et al., 2021) achieves enhanced performance by utilizing relation prototypes to refine sample embeddings, thereby effectively avoiding catastrophic forgetting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_77",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_78@0",
            "content": "Training Details and Parameters Setting",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_78",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_79@0",
            "content": "A completely random sampling strategy at the relation level is adopted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_79",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_79@1",
            "content": "It simulates ten tasks by randomly dividing all relations of the dataset into 10 sets to simulate 10 tasks, as suggested in (Cui et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_79",
            "start": 72,
            "end": 214,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_79@2",
            "content": "For a fair comparison, we set the random seed of the experiment to be the same as the seed in (Cui et al., 2021), so that the task sequence is exactly the same.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_79",
            "start": 216,
            "end": 375,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_79@3",
            "content": "Note that our reproduced model RP-CRE \u2020 and CRL use strictly the same experimental environment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_79",
            "start": 377,
            "end": 471,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_79@4",
            "content": "In order to facilitate the reproduction of our experimental results, the proposed method source code and detailed hyperparameters are provided on https: //github.com/submitacl22/CRL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_79",
            "start": 473,
            "end": 654,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_80@0",
            "content": "Results and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_80",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_81@0",
            "content": "Table 1 shows the results of the proposed methods and baselines ones compared on two datasets, where RP-CRE \u2020 is reproduced under the same conditions based on open source code.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_81",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_81@1",
            "content": "We also ablated knowledge distillation and contrastive replay for consistent representation learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_81",
            "start": 177,
            "end": 277,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_81@2",
            "content": "CRL (w/o KL) and CRL (w/o CR) respectively refer to removing knowledge distillation loss L KL and contrastive replay loss L CR when replaying memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_81",
            "start": 279,
            "end": 427,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_81@3",
            "content": "From the table, some conclusions can be drawn:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_81",
            "start": 429,
            "end": 474,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_82@0",
            "content": "(1) Our proposed CRL is significantly better than other baselines and achieves state-of-the-art performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_82",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_82@1",
            "content": "Compared with RP-CRE, our model also produces apparent advantages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_82",
            "start": 109,
            "end": 174,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_82@2",
            "content": "It proves that CRL can learn better consistent relation representations and is more stable in the process of continual learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_82",
            "start": 176,
            "end": 303,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_83@0",
            "content": "(2) It is observed that all baselines perform worse on the TACRED dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_83",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_83@1",
            "content": "The primary reason for this result is that TACRED is an imbalanced dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_83",
            "start": 75,
            "end": 149,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_83@2",
            "content": "However, our model performs better than RP-CRE's last task on TACRED (3.4% higher than RP-CRE), which is more significant than the improvement (0.5%) on the class-balanced dataset FewRel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_83",
            "start": 151,
            "end": 337,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_83@3",
            "content": "It shows that our model is more robust to scenarios with class-imbalanced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_83",
            "start": 339,
            "end": 412,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_84@0",
            "content": "( (4) Comparing CRL and CRL (w/o CR), removing L during memory replay caused the model to drop 2.4% and 4.8% on FewRel and TACRED, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_84",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_84@1",
            "content": "The reason for the significant drop is that only adopting L KL cannot make the model review the samples of the current task, which leads to overfitting in the historical relations during replay.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_84",
            "start": 145,
            "end": 338,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_85@0",
            "content": "Effect of Memory Size",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_85",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_86@0",
            "content": "The memory size is the number of memory samples needed for each relation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_86",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_86@1",
            "content": "In this section, we will study the impact of memory size on the performance of our model and RP-CRE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_86",
            "start": 74,
            "end": 173,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_86@2",
            "content": "We compare three memory sizes: 5, 10, and 20.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_86",
            "start": 175,
            "end": 219,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_86@3",
            "content": "The experimental results are shown in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_86",
            "start": 221,
            "end": 267,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_87@0",
            "content": "We choose RP-CRE as the main competitor, where all configurations and task sequence remain unchanged. (1) As the size of the memory decreases, the performance of the model tends to decline, which shows that the size of the memory is a key factor that affects continuous learning and learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_87",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_87@1",
            "content": "But our model is more stable than RP-CRE (the performance gap in the final task), especially on the TACRED dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_87",
            "start": 293,
            "end": 407,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_87@2",
            "content": "(2) On both FewRel and TACRED, CRL keeps the best performance under different memory sizes and produces obvious advantages in small memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_87",
            "start": 409,
            "end": 547,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_87@3",
            "content": "It indicates that utilizing consistent representation learning is a more effective way to utilize memory than the existing memory-based CRE method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_87",
            "start": 549,
            "end": 695,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_88@0",
            "content": "Effect of Consistent Representation Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_88",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_89@0",
            "content": "In order to explore the long-term effects of consistency representation learning in continual relation extraction, we tested our model and RP-CRE on TACRED to observe the changes in the embedding space of old tasks as new tasks continue to increase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_89",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_90@0",
            "content": "The model performs feature extraction on all samples in the test set in task 1 at the end of tasks 1, 4, 7, and 10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_90",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_90@1",
            "content": "Then t-SNE is used to represent the dimensionality reduction relation representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_90",
            "start": 116,
            "end": 200,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_90@2",
            "content": "All samples on the test set of task 1 are drawn, where different color points represent different groundtruth labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_90",
            "start": 202,
            "end": 318,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_90@3",
            "content": "The visualization results are shown in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_90",
            "start": 320,
            "end": 367,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_91@0",
            "content": "From Figure 3, we can see that although the relation embeddings of RP-CRE are clustered and separated in each class after prototype refinement, as new tasks are continuously learned, the data embedding of task 1 is obviously scattered.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_91",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_91@1",
            "content": "In contrast, our model retains a good separation between classes, while the data embedding within classes is compact and has a certain diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_91",
            "start": 236,
            "end": 380,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_91@2",
            "content": "In addition, we can see that our model has relatively stable changes in the distribution of different classes in task 1, and retains the knowledge of historical tasks with training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_91",
            "start": 382,
            "end": 562,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_91@3",
            "content": "This is mainly because our model learns through supervised comparison, and explicitly emphasizes that the samples in historical memory are compact within the class and far away from each other. And the knowledge of historical memory is preserved through the distillation of memory knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_91",
            "start": 564,
            "end": 854,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_91@4",
            "content": "Because knowledge distillation preserves the distance distribution between classes, it can make up for the contrastive learning to over-optimize the distance between classes to prevent overfitting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_91",
            "start": 856,
            "end": 1052,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_92@0",
            "content": "Conclusions and Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_92",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_93@0",
            "content": "This paper proposes a novel consistent representation learning method for the CRE task, mainly through contrastive learning and knowledge distillation when replaying memory.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_93",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_93@1",
            "content": "Specifically, we use supervised comparative learning based on a memory bank to train each new task so that the model can effectively learn the feature representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_93",
            "start": 174,
            "end": 339,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_93@2",
            "content": "In addition, in order to prevent the catastrophic forgetting of the old task, we compare and replay the memory samples, and at the same time, make the model retain the knowledge of the relation between the historical tasks through the knowledge distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_93",
            "start": 341,
            "end": 598,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_93@3",
            "content": "Our method can better learn consistent representations to alleviate catastrophic forgetting effectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_93",
            "start": 600,
            "end": 703,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_93@4",
            "content": "Extensive experiments on two benchmark data sets show that our method significantly improves the performance of the most advanced technology and demonstrates powerful representation learning capabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_93",
            "start": 705,
            "end": 908,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_93@5",
            "content": "In the future, we will continue to study cross-domain continual relation extraction to acquire ever-increasing knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_93",
            "start": 910,
            "end": 1030,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_94@0",
            "content": "Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars, 2018, Proceedings of the European Conference on Computer Vision (ECCV), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_94",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_95@0",
            "content": "UNKNOWN, None, 2018, Efficient lifelong learning with a-gem, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_95",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_96@0",
            "content": "UNKNOWN, None, 2015, Net2net: Accelerating learning via knowledge transfer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_96",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_97@0",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, A simple framework for contrastive learning of visual representations, 2020, International conference on machine learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_97",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_98@0",
            "content": "Xinlei Chen, Kaiming He, Exploring simple siamese representation learning, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_98",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_99@0",
            "content": "Li Cui, Deqing Yang, Jiaxin Yu, Chengwei Hu, Jiayang Cheng, Jingjie Yi, Yanghua Xiao, Refining sample embeddings with relation prototypes to enhance continual relation extraction, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_99",
            "start": 0,
            "end": 361,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_100@0",
            "content": "UNKNOWN, None, 2019, Episodic memory in lifelong language learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_100",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_101@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: pre-training of deep bidirectional transformers for language understanding, 2019-06-02, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_101",
            "start": 0,
            "end": 357,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_102@0",
            "content": "Songlin Dong, Xiaopeng Hong, Xiaoyu Tao, Xinyuan Chang, Xing Wei, Yihong Gong, Few-shot class-incremental learning via relation knowledge distillation, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_102",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_103@0",
            "content": "UNKNOWN, None, 2017, Pathnet: Evolution channels gradient descent in super neural networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_103",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_104@0",
            "content": "Xu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou, Continual relation learning via episodic memory activation and reconsolidation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_104",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_105@0",
            "content": "Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, Maosong Sun, FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_105",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_106@0",
            "content": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Momentum contrast for unsupervised visual representation learning, 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_106",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_107@0",
            "content": "UNKNOWN, None, 2019, Benchmarking neural network robustness to common corruptions and perturbations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_107",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_108@0",
            "content": "UNKNOWN, None, , Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. 2021. A survey on contrastive selfsupervised learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_108",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_109@0",
            "content": "UNKNOWN, None, , Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_109",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_110@0",
            "content": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Overcoming catastrophic forgetting in neural networks, 2017, Proceedings of the national academy of sciences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_110",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_111@0",
            "content": "UNKNOWN, None, 2020, Prototypical contrastive learning of unsupervised representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_111",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_112@0",
            "content": "UNKNOWN, None, 2020, Hybrid discriminative-generative training via contrastive learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_112",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_113@0",
            "content": "Xialei Liu, Marc Masana, Luis Herranz, Joost Van De, Antonio Weijer, Andrew Lopez,  Bagdanov, Rotate your networks: Better weight consolidation and less catastrophic forgetting, 2018, 24th International Conference on Pattern Recognition (ICPR), IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_113",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_114@0",
            "content": "David Lopez, - Paz, Marc'aurelio Ranzato, Gradient episodic memory for continual learning, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_114",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_115@0",
            "content": "Zheda Mai, Ruiwen Li, Hyunwoo Kim, Scott Sanner, Supervised contrastive replay: Revisiting the nearest class mean classifier in online classincremental continual learning, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_115",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_116@0",
            "content": "Livio Baldini, Nicholas Soares, Jeffrey Fitzgerald, Tom Ling,  Kwiatkowski, Matching the blanks: Distributional similarity for relation learning, 2019, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_116",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_117@0",
            "content": "Livio Baldini, Nicholas Soares, Jeffrey Fitzgerald, Tom Ling,  Kwiatkowski, Matching the blanks: Distributional similarity for relation learning, 2019-07-28, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_117",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_118@0",
            "content": "UNKNOWN, None, 2019, Lamol: Language modeling for lifelong language learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_118",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_119@0",
            "content": "Chongyang Tao, Shen Gao, Mingyue Shang, Wei Wu, Dongyan Zhao, Rui Yan, Get the point of my utterance! learning towards effective responses with multi-head attention mechanism, 2018-07-13, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_119",
            "start": 0,
            "end": 293,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_120@0",
            "content": "UNKNOWN, None, 2019, Sentence embedding alignment for lifelong relation extraction, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_120",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_121@0",
            "content": "UNKNOWN, None, 2021, Curriculum-meta learning for order-robust continual relation extraction, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_121",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_122@0",
            "content": "Zhirong Wu, Yuanjun Xiong, X Stella, Dahua Yu,  Lin, Unsupervised feature learning via nonparametric instance discrimination, 2018, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_122",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_123@0",
            "content": "Chenyan Xiong, Russell Power, Jamie Callan, Explicit semantic ranking for academic search via knowledge graph embedding, 2017-04-03, Proceedings of the 26th International Conference on World Wide Web, ACM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_123",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_124@0",
            "content": "Shipeng Yan, Jiangwei Xie, Xuming He, Der: Dynamically expandable representation for class incremental learning, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_124",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_125@0",
            "content": "Friedemann Zenke, Ben Poole, Surya Ganguli, Continual learning through synaptic intelligence, 2017, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_125",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_126@0",
            "content": "Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, Christopher D Manning, Position-aware attention and supervised data improve slot filling, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_126",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "336-ARR_v1_127@0",
            "content": "Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, Bo Xu, Attention-based bidirectional long short-term memory networks for relation classification, 2016, Proceedings of the 54th annual meeting of the association for computational linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "336-ARR_v1_127",
            "start": 0,
            "end": 260,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_1",
            "tgt_ix": "336-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_1",
            "tgt_ix": "336-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_2",
            "tgt_ix": "336-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_4",
            "tgt_ix": "336-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_5",
            "tgt_ix": "336-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_7",
            "tgt_ix": "336-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_8",
            "tgt_ix": "336-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_3",
            "tgt_ix": "336-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_3",
            "tgt_ix": "336-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_3",
            "tgt_ix": "336-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_3",
            "tgt_ix": "336-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_3",
            "tgt_ix": "336-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_3",
            "tgt_ix": "336-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_3",
            "tgt_ix": "336-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_9",
            "tgt_ix": "336-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_10",
            "tgt_ix": "336-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_10",
            "tgt_ix": "336-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_11",
            "tgt_ix": "336-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_12",
            "tgt_ix": "336-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_12",
            "tgt_ix": "336-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_14",
            "tgt_ix": "336-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_14",
            "tgt_ix": "336-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_16",
            "tgt_ix": "336-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_17",
            "tgt_ix": "336-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_18",
            "tgt_ix": "336-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_19",
            "tgt_ix": "336-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_15",
            "tgt_ix": "336-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_15",
            "tgt_ix": "336-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_15",
            "tgt_ix": "336-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_15",
            "tgt_ix": "336-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_15",
            "tgt_ix": "336-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_15",
            "tgt_ix": "336-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_14",
            "tgt_ix": "336-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_20",
            "tgt_ix": "336-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_21",
            "tgt_ix": "336-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_21",
            "tgt_ix": "336-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_14",
            "tgt_ix": "336-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_22",
            "tgt_ix": "336-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_24",
            "tgt_ix": "336-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_25",
            "tgt_ix": "336-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_26",
            "tgt_ix": "336-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_27",
            "tgt_ix": "336-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_28",
            "tgt_ix": "336-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_29",
            "tgt_ix": "336-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_23",
            "tgt_ix": "336-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_23",
            "tgt_ix": "336-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_23",
            "tgt_ix": "336-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_23",
            "tgt_ix": "336-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_23",
            "tgt_ix": "336-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_23",
            "tgt_ix": "336-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_23",
            "tgt_ix": "336-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_23",
            "tgt_ix": "336-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_14",
            "tgt_ix": "336-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_30",
            "tgt_ix": "336-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_32",
            "tgt_ix": "336-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_33",
            "tgt_ix": "336-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_34",
            "tgt_ix": "336-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_35",
            "tgt_ix": "336-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_36",
            "tgt_ix": "336-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_37",
            "tgt_ix": "336-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_38",
            "tgt_ix": "336-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_31",
            "tgt_ix": "336-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_31",
            "tgt_ix": "336-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_31",
            "tgt_ix": "336-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_31",
            "tgt_ix": "336-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_31",
            "tgt_ix": "336-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_31",
            "tgt_ix": "336-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_31",
            "tgt_ix": "336-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_31",
            "tgt_ix": "336-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_31",
            "tgt_ix": "336-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_14",
            "tgt_ix": "336-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_39",
            "tgt_ix": "336-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_40",
            "tgt_ix": "336-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_40",
            "tgt_ix": "336-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_14",
            "tgt_ix": "336-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_41",
            "tgt_ix": "336-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_43",
            "tgt_ix": "336-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_44",
            "tgt_ix": "336-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_45",
            "tgt_ix": "336-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_46",
            "tgt_ix": "336-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_47",
            "tgt_ix": "336-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_49",
            "tgt_ix": "336-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_50",
            "tgt_ix": "336-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_51",
            "tgt_ix": "336-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_52",
            "tgt_ix": "336-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_53",
            "tgt_ix": "336-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_54",
            "tgt_ix": "336-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_55",
            "tgt_ix": "336-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_56",
            "tgt_ix": "336-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_57",
            "tgt_ix": "336-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_58",
            "tgt_ix": "336-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_59",
            "tgt_ix": "336-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_60",
            "tgt_ix": "336-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_61",
            "tgt_ix": "336-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_48",
            "tgt_ix": "336-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_14",
            "tgt_ix": "336-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_62",
            "tgt_ix": "336-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_64",
            "tgt_ix": "336-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_63",
            "tgt_ix": "336-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_63",
            "tgt_ix": "336-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_63",
            "tgt_ix": "336-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_65",
            "tgt_ix": "336-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_67",
            "tgt_ix": "336-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_68",
            "tgt_ix": "336-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_66",
            "tgt_ix": "336-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_66",
            "tgt_ix": "336-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_66",
            "tgt_ix": "336-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_66",
            "tgt_ix": "336-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_69",
            "tgt_ix": "336-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_70",
            "tgt_ix": "336-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_70",
            "tgt_ix": "336-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_71",
            "tgt_ix": "336-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_73",
            "tgt_ix": "336-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_74",
            "tgt_ix": "336-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_75",
            "tgt_ix": "336-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_76",
            "tgt_ix": "336-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_72",
            "tgt_ix": "336-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_72",
            "tgt_ix": "336-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_72",
            "tgt_ix": "336-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_72",
            "tgt_ix": "336-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_72",
            "tgt_ix": "336-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_72",
            "tgt_ix": "336-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_77",
            "tgt_ix": "336-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_78",
            "tgt_ix": "336-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_78",
            "tgt_ix": "336-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_79",
            "tgt_ix": "336-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_81",
            "tgt_ix": "336-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_82",
            "tgt_ix": "336-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_83",
            "tgt_ix": "336-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_80",
            "tgt_ix": "336-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_80",
            "tgt_ix": "336-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_80",
            "tgt_ix": "336-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_80",
            "tgt_ix": "336-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_80",
            "tgt_ix": "336-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_84",
            "tgt_ix": "336-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_86",
            "tgt_ix": "336-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_85",
            "tgt_ix": "336-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_85",
            "tgt_ix": "336-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_85",
            "tgt_ix": "336-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_87",
            "tgt_ix": "336-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_89",
            "tgt_ix": "336-ARR_v1_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_90",
            "tgt_ix": "336-ARR_v1_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_88",
            "tgt_ix": "336-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_88",
            "tgt_ix": "336-ARR_v1_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_88",
            "tgt_ix": "336-ARR_v1_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_88",
            "tgt_ix": "336-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_91",
            "tgt_ix": "336-ARR_v1_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_92",
            "tgt_ix": "336-ARR_v1_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_92",
            "tgt_ix": "336-ARR_v1_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "336-ARR_v1_0",
            "tgt_ix": "336-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_1",
            "tgt_ix": "336-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_2",
            "tgt_ix": "336-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_2",
            "tgt_ix": "336-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_2",
            "tgt_ix": "336-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_2",
            "tgt_ix": "336-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_2",
            "tgt_ix": "336-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_2",
            "tgt_ix": "336-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_2",
            "tgt_ix": "336-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_2",
            "tgt_ix": "336-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_3",
            "tgt_ix": "336-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_4",
            "tgt_ix": "336-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_4",
            "tgt_ix": "336-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_4",
            "tgt_ix": "336-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_5",
            "tgt_ix": "336-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_5",
            "tgt_ix": "336-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_5",
            "tgt_ix": "336-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_5",
            "tgt_ix": "336-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_5",
            "tgt_ix": "336-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_6@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_6",
            "tgt_ix": "336-ARR_v1_6@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_7",
            "tgt_ix": "336-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_7",
            "tgt_ix": "336-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_7",
            "tgt_ix": "336-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_7",
            "tgt_ix": "336-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_7",
            "tgt_ix": "336-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_7",
            "tgt_ix": "336-ARR_v1_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_8",
            "tgt_ix": "336-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_8",
            "tgt_ix": "336-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_8",
            "tgt_ix": "336-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_9",
            "tgt_ix": "336-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_10",
            "tgt_ix": "336-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_11",
            "tgt_ix": "336-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_11",
            "tgt_ix": "336-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_11",
            "tgt_ix": "336-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_11",
            "tgt_ix": "336-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_11",
            "tgt_ix": "336-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_11",
            "tgt_ix": "336-ARR_v1_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_11",
            "tgt_ix": "336-ARR_v1_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_11",
            "tgt_ix": "336-ARR_v1_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_12",
            "tgt_ix": "336-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_13@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_13@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_13",
            "tgt_ix": "336-ARR_v1_13@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_14",
            "tgt_ix": "336-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_15",
            "tgt_ix": "336-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_16",
            "tgt_ix": "336-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_16",
            "tgt_ix": "336-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_16",
            "tgt_ix": "336-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_16",
            "tgt_ix": "336-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_17",
            "tgt_ix": "336-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_17",
            "tgt_ix": "336-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_17",
            "tgt_ix": "336-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_18",
            "tgt_ix": "336-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_19",
            "tgt_ix": "336-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_20",
            "tgt_ix": "336-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_21",
            "tgt_ix": "336-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_22",
            "tgt_ix": "336-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_22",
            "tgt_ix": "336-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_22",
            "tgt_ix": "336-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_22",
            "tgt_ix": "336-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_23",
            "tgt_ix": "336-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_24",
            "tgt_ix": "336-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_24",
            "tgt_ix": "336-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_24",
            "tgt_ix": "336-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_25",
            "tgt_ix": "336-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_25",
            "tgt_ix": "336-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_26",
            "tgt_ix": "336-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_27",
            "tgt_ix": "336-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_27",
            "tgt_ix": "336-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_28",
            "tgt_ix": "336-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_29",
            "tgt_ix": "336-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_30",
            "tgt_ix": "336-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_30",
            "tgt_ix": "336-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_31",
            "tgt_ix": "336-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_32",
            "tgt_ix": "336-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_33",
            "tgt_ix": "336-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_34",
            "tgt_ix": "336-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_35",
            "tgt_ix": "336-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_36",
            "tgt_ix": "336-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_37",
            "tgt_ix": "336-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_38",
            "tgt_ix": "336-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_39",
            "tgt_ix": "336-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_39",
            "tgt_ix": "336-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_40",
            "tgt_ix": "336-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_41",
            "tgt_ix": "336-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_41",
            "tgt_ix": "336-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_41",
            "tgt_ix": "336-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_42",
            "tgt_ix": "336-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_43",
            "tgt_ix": "336-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_43",
            "tgt_ix": "336-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_43",
            "tgt_ix": "336-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_44",
            "tgt_ix": "336-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_44",
            "tgt_ix": "336-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_45",
            "tgt_ix": "336-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_46",
            "tgt_ix": "336-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_47",
            "tgt_ix": "336-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_47",
            "tgt_ix": "336-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_48",
            "tgt_ix": "336-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_48",
            "tgt_ix": "336-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_48",
            "tgt_ix": "336-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_48",
            "tgt_ix": "336-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_49",
            "tgt_ix": "336-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_49",
            "tgt_ix": "336-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_49",
            "tgt_ix": "336-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_50",
            "tgt_ix": "336-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_51",
            "tgt_ix": "336-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_52",
            "tgt_ix": "336-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_53",
            "tgt_ix": "336-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_54",
            "tgt_ix": "336-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_55",
            "tgt_ix": "336-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_56",
            "tgt_ix": "336-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_57",
            "tgt_ix": "336-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_58",
            "tgt_ix": "336-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_59",
            "tgt_ix": "336-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_60",
            "tgt_ix": "336-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_61",
            "tgt_ix": "336-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_62",
            "tgt_ix": "336-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_62",
            "tgt_ix": "336-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_62",
            "tgt_ix": "336-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_63",
            "tgt_ix": "336-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_64",
            "tgt_ix": "336-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_64",
            "tgt_ix": "336-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_65",
            "tgt_ix": "336-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_66",
            "tgt_ix": "336-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_67",
            "tgt_ix": "336-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_68",
            "tgt_ix": "336-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_68",
            "tgt_ix": "336-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_69",
            "tgt_ix": "336-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_69",
            "tgt_ix": "336-ARR_v1_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_69",
            "tgt_ix": "336-ARR_v1_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_70",
            "tgt_ix": "336-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_71",
            "tgt_ix": "336-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_71",
            "tgt_ix": "336-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_72",
            "tgt_ix": "336-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_73",
            "tgt_ix": "336-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_74",
            "tgt_ix": "336-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_75",
            "tgt_ix": "336-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_76",
            "tgt_ix": "336-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_77",
            "tgt_ix": "336-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_78",
            "tgt_ix": "336-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_79",
            "tgt_ix": "336-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_79",
            "tgt_ix": "336-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_79",
            "tgt_ix": "336-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_79",
            "tgt_ix": "336-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_79",
            "tgt_ix": "336-ARR_v1_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_80",
            "tgt_ix": "336-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_81",
            "tgt_ix": "336-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_81",
            "tgt_ix": "336-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_81",
            "tgt_ix": "336-ARR_v1_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_81",
            "tgt_ix": "336-ARR_v1_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_82",
            "tgt_ix": "336-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_82",
            "tgt_ix": "336-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_82",
            "tgt_ix": "336-ARR_v1_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_83",
            "tgt_ix": "336-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_83",
            "tgt_ix": "336-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_83",
            "tgt_ix": "336-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_83",
            "tgt_ix": "336-ARR_v1_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_84",
            "tgt_ix": "336-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_84",
            "tgt_ix": "336-ARR_v1_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_85",
            "tgt_ix": "336-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_86",
            "tgt_ix": "336-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_86",
            "tgt_ix": "336-ARR_v1_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_86",
            "tgt_ix": "336-ARR_v1_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_86",
            "tgt_ix": "336-ARR_v1_86@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_87",
            "tgt_ix": "336-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_87",
            "tgt_ix": "336-ARR_v1_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_87",
            "tgt_ix": "336-ARR_v1_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_87",
            "tgt_ix": "336-ARR_v1_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_88",
            "tgt_ix": "336-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_89",
            "tgt_ix": "336-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_90",
            "tgt_ix": "336-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_90",
            "tgt_ix": "336-ARR_v1_90@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_90",
            "tgt_ix": "336-ARR_v1_90@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_90",
            "tgt_ix": "336-ARR_v1_90@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_91",
            "tgt_ix": "336-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_91",
            "tgt_ix": "336-ARR_v1_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_91",
            "tgt_ix": "336-ARR_v1_91@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_91",
            "tgt_ix": "336-ARR_v1_91@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_91",
            "tgt_ix": "336-ARR_v1_91@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_92",
            "tgt_ix": "336-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_93",
            "tgt_ix": "336-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_93",
            "tgt_ix": "336-ARR_v1_93@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_93",
            "tgt_ix": "336-ARR_v1_93@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_93",
            "tgt_ix": "336-ARR_v1_93@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_93",
            "tgt_ix": "336-ARR_v1_93@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_93",
            "tgt_ix": "336-ARR_v1_93@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_94",
            "tgt_ix": "336-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_95",
            "tgt_ix": "336-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_96",
            "tgt_ix": "336-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_97",
            "tgt_ix": "336-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_98",
            "tgt_ix": "336-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_99",
            "tgt_ix": "336-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_100",
            "tgt_ix": "336-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_101",
            "tgt_ix": "336-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_102",
            "tgt_ix": "336-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_103",
            "tgt_ix": "336-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_104",
            "tgt_ix": "336-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_105",
            "tgt_ix": "336-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_106",
            "tgt_ix": "336-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_107",
            "tgt_ix": "336-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_108",
            "tgt_ix": "336-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_109",
            "tgt_ix": "336-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_110",
            "tgt_ix": "336-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_111",
            "tgt_ix": "336-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_112",
            "tgt_ix": "336-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_113",
            "tgt_ix": "336-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_114",
            "tgt_ix": "336-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_115",
            "tgt_ix": "336-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_116",
            "tgt_ix": "336-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_117",
            "tgt_ix": "336-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_118",
            "tgt_ix": "336-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_119",
            "tgt_ix": "336-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_120",
            "tgt_ix": "336-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_121",
            "tgt_ix": "336-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_122",
            "tgt_ix": "336-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_123",
            "tgt_ix": "336-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_124",
            "tgt_ix": "336-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_125",
            "tgt_ix": "336-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_126",
            "tgt_ix": "336-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "336-ARR_v1_127",
            "tgt_ix": "336-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1192,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "336-ARR",
        "version": 1
    }
}