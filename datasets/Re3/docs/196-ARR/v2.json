{
    "nodes": [
        {
            "ix": "196-ARR_v2_0",
            "content": "An Analysis of Negation in Natural Language Understanding Corpora",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_2",
            "content": "This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks. We show that these corpora have few negations compared to generalpurpose English, and that the few negations in them are often unimportant. Indeed, one can often ignore negations and still make the right predictions. Additionally, experimental results show that state-of-the-art transformers trained with these corpora obtain substantially worse results with instances that contain negation, especially if the negations are important. We conclude that new corpora accounting for negation are needed to solve natural language understanding tasks when negation is present.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "196-ARR_v2_4",
            "content": "Natural language understanding (NLU) is an umbrella term used to refer to any task that requires text understanding. For example, question answering (Rajpurkar et al., 2016), information extraction (Stanovsky et al., 2018), coreference resolution , and machine reading (Yang et al., 2019), among many others, are tasks that fall under natural language understanding. The threshold for claiming that a system understands natural language is ever-moving. New corpora are often justified by pointing out that state-of-the-art models do not obtain good results. After years of steady improvements, more powerful models eventually obtain so-called human performance, and at that point new, more challenging corpora are created.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_5",
            "content": "Many corpora for natural language understanding tasks contain language generated by annotators rather than retrieved from texts written independently of the corpus creation process. These corpora are certainly useful and have facilitated tremendous progress. Annotator-generated examples, however, carry the risk of evaluating systems with synthetic language that is not representative of language in the wild. For example, annotators are likely to use negation when asked to write a text that contradicts something despite contradictions in the wild need not have a negation (Gururangan et al., 2018). Recently, Kwiatkowski et al. (2019) present a large corpus for question answering that consists of natural questions (i.e., asked by somebody with a real information need) in order to encourage research in a more realistic scenario. This contrasts with previous corpora, where the questions were written by annotators after being told the answer (Rajpurkar et al., 2016).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_6",
            "content": "In this paper, we explore the role of negation in eight corpora for six popular natural language understanding tasks. Our goal is to check whether negation plays the role it deserves in these tasks. To our surprise, we conclude that negation is virtually ignored by answering the following questions: 1 1. Do NLU corpora contain as many negations as general-purpose texts? (they don't); 2. Do the (few) negations in NLU corpora play a role in solving the tasks? (they don't); and 3. Do state-of-the-art transformers trained with NLU corpora face challenges with instances that contain negation? (they do, especially if the negation is important).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_7",
            "content": "Background and Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "196-ARR_v2_8",
            "content": "We work with the eight corpora covering six tasks summarized below and exemplified in Table 2. We select two corpora for question answering: CommonsenseQA (Talmor et al., 2019) and COPA (Roemmele et al., 2011). CommonsenseQA consists of multi-choice questions (5 candidate answers) that require some degree of commonsense. COPA presents a premise (e.g., The man broke his toe) and a question (e.g., What was the cause of this?) and the system must choose between two plausible alternatives (e.g. He got a hole in his sock or He dropped a hammer on his foot).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_9",
            "content": "For textual similarity and paraphrasing, we select QQP 2 and STS-B (Cer et al., 2017). QQP consists of pairs of questions and the task is to determine whether they are paraphrases. STS-B consists of pairs of texts and the task is to determine how semantically similar they are with a score from 0 to 5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_10",
            "content": "We select one corpus for the remaining tasks. For inference, we work with QNLI (Rajpurkar et al., 2016), which consists in determining whether a text is a valid answer to a question. We use WiC (Pilehvar and Camacho-Collados, 2019) for word sense disambiguation. WiC consists in determining whether two instances of the same word (in two sentences; italicized in Table 2) are used with the same meaning. For coreference resolution, we choose WSC (Levesque et al., 2012), which consists in determining whether a pronoun and a noun phrase are co-referential (italicized in Table 2). Finally, we work with SST-2 (Socher et al., 2013) for sentiment analysis. The task consists in determining whether a sentence from a collection of movie reviews has positive or negative sentiment.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_11",
            "content": "For convenience, we work with the formatted versions of these corpora in the GLUE (Wang et al., 2018) and SuperGLUE benchmarks. The only exception is CommonsenseQA, which is not part of these benchmarks. Related Work Previous work has shown that SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) have annotation artifacts (e.g., negation is a strong indicator of contradictions) (Gururangan et al., 2018). The literature has also shown that simple adversarial attacks including negation cues are very effective (Naik et al., 2018;Wallace et al., 2019). Kovatchev et al. (2019) analyze 11 paraphrasing systems and show that they obtain substantially worse results when negation is present.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_12",
            "content": "More recently, Ribeiro et al. (2020) show that negation is one of the linguistic phenomena commercial sentiment analysis struggle with. Several previous works have investigated the (lack of) ability of transformers to make inferences when negation is present. For example, Ettinger (2020) conclude that BERT is unable to complete sentences when negation is present. BERT also faces challenges solving the task of natural language inference (i.e., identifying entailments and contradictions) with monotonicity and negation (Geiger et al., 2020;Yanaka et al., 2019) (2019) show the limitations of BERT making acceptability judgments with sentences that contain negative polarity items. Most related to out work, Hossain et al. ( 2020) analyze the role of negation in three natural language inference corpora: RTE Bar-Haim et al., 2006;Giampiccolo et al., 2007;Bentivogli et al., 2009), SNLI and MNLI. In this paper, we present a similar analysis, but we move beyond natural language inference and work with eight corpora spanning six natural language understanding tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_13",
            "content": "3 Research Questions and Analysis Q1: Do natural language understanding corpora contain as many negations as generalpurpose English texts? In order to automatically identify negation cues, we train a negation cue detector with the largest corpus available, ConanDoyle-neg (Morante and Daelemans, 2012). The cue detector is based on the RoBERTa pretrained language model Well for one a being could have a non-physical existance and yet not even be in your mind.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_14",
            "content": "3.4 \u2717 The difference is huge, as not all non-physical things exist in minds.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_15",
            "content": "Who did BSkyB team up with as it was not part of consortium? yes \u2713 While BSkyB had been excluded from being a part of the [. . . ], BSkyB was able to join ITV Digital's free-to-air replacement, Freeview, in which it holds an equal stake [. . . ] In what year did Lavoisier publish his work on combustion? no \u2717 In one experiment, Lavoisier observed that there was no overall increase in weight when tin and air were heated in a closed container.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_16",
            "content": "It's not the ultimate depression-era gangster movie. neg. \u2713",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_17",
            "content": "Whaley's determination to immerse you in sheer, unrelenting wretchedness is exhausting. neg. \u2717",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_18",
            "content": "The intention of this legislation is to boost the economy. same \u2717 Good intentions are not enough.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_19",
            "content": "Sam and Amy are passionately in love, but Amy's parents are unhappy about it, because they are only fifteen. yes \u2717",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_20",
            "content": "Table 2: Examples containing negation (underlined) from the validation datasets of the natural language understanding corpora we work with. The third column presents the expected answer for the example (a choice, judgment, or score depending on the task). The last column indicates whether the negation is important.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_21",
            "content": "Table 1 presents the percentage of sentences that contain negation in (a) the eight corpora we work with and (b) general-purpose English. We take the latter percentage (all sentences) from Hossain et al. ( 2020), who run a negation cue detector in online reviews, conversations, and books. Additionally, we also present the percentages in questions. Negation is much less common in all natural language understanding corpora but WSC (0.8%-16%) than in general-purpose English (22.6%-29.9%). Note that negation is also underrepresented in corpora that primarily contain questions (general-purpose: 15.8%-20.2%; COPA: 0.8%, QQP: 8.1%).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_22",
            "content": "Q2: Do the (few) negations in natural language understanding corpora play a role in solving the tasks? After showing that negation in underrepresented in natural language understanding corpora, we explore whether the few negations they contain are important. Given an instance from any of the corpora, we consider a negation important if removing it changes the ground truth. In other words, a negation is unimportant if one can ignore it and still solve the task at hand. Table 2 presents examples of important and unimportant negations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_23",
            "content": "We manually examine the negations in all instances containing negation from the validation split of each corpus except QQP, for which we examine 1,000 (out of 5,196). Note that COPA does not have any negations in the validation split, and many corpora have few instances containing negation (CommonsenseQA: 184, STS-B: 225, QNLI: 852, WiC: 99, WSC: 52, and SST-2: 263). We choose to work with the validation set because we want to compare results when negation is and is not important (Q3), and the ground truth for the test splits of some corpora are not publicly available.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_24",
            "content": "We observe that (a) all negations in WiC and WSC are unimportant, and (b) the percentages of unimportant negations in CommonsenseQA, SST-2, QQP, STS-B, and QNLI are substantial: 45.1%, 63%, 97.4%, 95.6%, and 97.7%, respectively. These percentages indicate that one can safely ignore (almost) all negations and still solve the benchmarks. Despite the fact that negations are not important in WSC and WiC, they do affect the experimental results (details in Q3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_25",
            "content": "We also analyze the role of two major types of negation: syntactic (not, no, never, etc.) and morphological (i.e., affixes such as un-, im-, and -less). To this end, we work with CommonsenseQA and SST-2, which have lower percentages of unimportant negations (45.1% and 63%) than the other corpora we use (97.4%-100%). Table 3 provides examples of these two negation types. Perhaps unsurprisingly, syntactic negations are much more common than morphological negations (Common-senseQA: 88.6% vs 11.4%, SST-2: 71.9% vs 28.1%). More importantly, syntactic negations are more often important in SST-2 (42.3% vs 23%), but both syntactic and morphological negation are roughly equaly important in CommonsenseQA (55.2% vs 52.4%). Q3: Do state-of-the-art transformers trained with NLU corpora face challenges with instances that contain negation? We conduct experiments with RoBERTa . More specifically, we use the implementation by Phang et al. (2020) and train a model with the training split of each corpus. We refer the readers to the Appendix B for the details about these models and hyperparameters. We chose RoBERTa over other transformers because 4 out of the 10 best submissions to the SuperGLUE benchmark use it. 3 Table 4 presents the results evaluating the models with the corresponding validation splits. RoBERTa obtains slightly worse results with the validation instances that have negation in all corpora; the only exception is QQP (F1: 0.90 vs. 0.91). These results lead to the conclusion that negation may only pose a small challenge to state-of-the-art transformers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_26",
            "content": "The results obtained evaluating with the important and unimportant negations from the samples analyzed in Question 2, however, provide a different picture. Indeed, we observe substantial drops in results in all tasks that have both kinds of negations. More specifically, we obtain 27% lower results with instances containing important negations in QNLI (F1: 0.92 vs. 0.67), 33%/26% lower in STS-B, 24% lower in CommonsenseQA, 21% lower in QQP, and 9% lower in SST. Further, even though all negations are unimportant in WiC and WSC, we observe a drop in performance for the instances with negation compared to the instances without negation (WiC: 0.64 vs 0.67 and WSC: 0.59 vs 0.63). We conclude that transformers trained with existing NLU corpora face challenges with instances that contain negation. These results raise two important questions for future research: Is negation an inherently challenging phenomenon for RoBERTa? How many instances with negation are required to solve a natural language understanding task?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_27",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "196-ARR_v2_28",
            "content": "We have analyzed the role of negation in eight natural language understanding corpora covering six tasks. Our analyses show that (a) all but WSC conalmost no negations or around 31%-54% of the negations found in general-purpose texts, (b) the few negations in these corpora are usually unimportant, and (c) RoBERTa obtains substantially worse results when negation is important.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_29",
            "content": "Our analyses also provide some evidence that creating models to properly deal with negation may require both new corpora and more powerful models. The need for new corpora stems from the answers to Questions 1 and 2. The justification for powerful models is more subtle. We point out that the percentage of unimportant negations (Section 3) is only a weak indicator of the drop in results with important negations (Table 4). For example, we observe a 24% and 21% drop in results with important negations from CommonsenseQA and QQP despite 45% and 97% of negations are unimportant.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_30",
            "content": "Negation reverses truth values thus solutions to any natural language understanding task should be robust when negation is present and important. To this end, our future work includes two lines of research. First, we plan to create benchmarks for the six tasks consisting of instances containing negation (50/50 split important/unimportant). Second, we plan to conduct probing experiments to investigate whether (and where) pretrained transformers capture the meaning of negation. Doing so may help us discover potential solutions to understand negation and make inferences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_31",
            "content": "We develop a negation cue detector (Section 3 in the paper) by utilizing the RoBERTa (base architecture; 12 layers) pre-trained model . We fine-tune the system on ConanDoyleneg (Morante and Daelemans, 2012) corpus. While fine-training, the negation cues are marked with BIO (B: Beginning of cue, I: Inside of cue, O: Outside of cue) tagging scheme. The contextualized representations from the last layer of RoBERTa are passed to a fully connected (FC) layer. Finally, a conditional random field (CRF) layer produces the output sequence for the labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_32",
            "content": "Our model yields the following results on the test set: 93.26 Precision, 94.32 Recall, and 93.79 F1. The neural model takes about two hours on average to train on a single GPU of NVIDIA Tesla K80. A list of the tuned hyperparameters that the model requires to achieve the above results is provided in Table 6: Hyperparameters used to fine-tune RoBERTa individually for each corpus. Hp-1, Hp-2, and Hp-3 refer to the number of epochs, batch size, and learning rate used in the training procedure. We use default settings for the other hyperparameters when we use the implementation by Phang et al. (2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_33",
            "content": "B Hyperparameters to Fine-tune the System for Each of the NLU Tasks",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_34",
            "content": "We use an implementation by Phang et al. (2020) and fine-tune RoBERTa (base architecture; 12 layers) model separately for each of the eight corpora. We use the default settings of the hyperparameters, except for a few, when fine-tuning the model on each benchmark. Table 6 shows tuned hyperparameters for each benchmark.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "196-ARR_v2_35",
            "content": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, Idan Szpektor, The second pascal recognising textual entailment challenge, 2006, Proceedings of the second PASCAL challenges workshop on recognising textual entailment, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Roy Bar-Haim",
                    "Ido Dagan",
                    "Bill Dolan",
                    "Lisa Ferro",
                    "Danilo Giampiccolo",
                    "Bernardo Magnini",
                    "Idan Szpektor"
                ],
                "title": "The second pascal recognising textual entailment challenge",
                "pub_date": "2006",
                "pub_title": "Proceedings of the second PASCAL challenges workshop on recognising textual entailment",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_36",
            "content": "UNKNOWN, None, 2009, The fifth pascal recognizing textual entailment challenge, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "The fifth pascal recognizing textual entailment challenge",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_37",
            "content": "R Samuel, Gabor Bowman, Christopher Angeli, Christopher Potts,  Manning, A large annotated corpus for learning natural language inference, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "R Samuel",
                    "Gabor Bowman",
                    "Christopher Angeli",
                    "Christopher Potts",
                    " Manning"
                ],
                "title": "A large annotated corpus for learning natural language inference",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_38",
            "content": "Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, Lucia Specia, SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation, 2017, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Daniel Cer",
                    "Mona Diab",
                    "Eneko Agirre",
                    "I\u00f1igo Lopez-Gazpio",
                    "Lucia Specia"
                ],
                "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "196-ARR_v2_39",
            "content": "Oren Ido Dagan, Bernardo Glickman,  Magnini, The pascal recognising textual entailment challenge, 2006, Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05, Springer-Verlag.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Oren Ido Dagan",
                    "Bernardo Glickman",
                    " Magnini"
                ],
                "title": "The pascal recognising textual entailment challenge",
                "pub_date": "2006",
                "pub_title": "Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05",
                "pub": "Springer-Verlag"
            }
        },
        {
            "ix": "196-ARR_v2_40",
            "content": "Allyson Ettinger, What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Allyson Ettinger"
                ],
                "title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_41",
            "content": "Atticus Geiger, Kyle Richardson, Christopher Potts, Neural natural language inference models partially embed theories of lexical entailment and negation, 2020, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Atticus Geiger",
                    "Kyle Richardson",
                    "Christopher Potts"
                ],
                "title": "Neural natural language inference models partially embed theories of lexical entailment and negation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "196-ARR_v2_42",
            "content": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, Bill Dolan, The third PASCAL recognizing textual entailment challenge, 2007, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Danilo Giampiccolo",
                    "Bernardo Magnini",
                    "Ido Dagan",
                    "Bill Dolan"
                ],
                "title": "The third PASCAL recognizing textual entailment challenge",
                "pub_date": "2007",
                "pub_title": "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "196-ARR_v2_43",
            "content": "Swabha Suchin Gururangan, Omer Swayamdipta, Roy Levy, Samuel Schwartz, Noah Bowman,  Smith, Annotation artifacts in natural language inference data, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Swabha Suchin Gururangan",
                    "Omer Swayamdipta",
                    "Roy Levy",
                    "Samuel Schwartz",
                    "Noah Bowman",
                    " Smith"
                ],
                "title": "Annotation artifacts in natural language inference data",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_44",
            "content": "Venelin Md Mosharaf Hossain, Pranoy Kovatchev, Tiffany Dutta, Elizabeth Kao, Eduardo Wei,  Blanco, An analysis of natural language inference benchmarks through the lens of negation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Venelin Md Mosharaf Hossain",
                    "Pranoy Kovatchev",
                    "Tiffany Dutta",
                    "Elizabeth Kao",
                    "Eduardo Wei",
                    " Blanco"
                ],
                "title": "An analysis of natural language inference benchmarks through the lens of negation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_45",
            "content": "Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi Gunawi, Cody Hammock, Joe Mambretti, Lessons learned from the chameleon testbed, 2020, Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC '20). USENIX Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Kate Keahey",
                    "Jason Anderson",
                    "Zhuo Zhen",
                    "Pierre Riteau",
                    "Paul Ruth",
                    "Dan Stanzione",
                    "Mert Cevik",
                    "Jacob Colleran",
                    "Haryadi Gunawi",
                    "Cody Hammock",
                    "Joe Mambretti"
                ],
                "title": "Lessons learned from the chameleon testbed",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC '20). USENIX Association",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_46",
            "content": "Aditya Khandelwal, Suraj Sawant, NegBERT: A transfer learning approach for negation detection and scope resolution, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, European Language Resources Association.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Aditya Khandelwal",
                    "Suraj Sawant"
                ],
                "title": "NegBERT: A transfer learning approach for negation detection and scope resolution",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 12th Language Resources and Evaluation Conference",
                "pub": "European Language Resources Association"
            }
        },
        {
            "ix": "196-ARR_v2_47",
            "content": "M Venelin Kovatchev, Maria Marti, Javier Salamo,  Beltran, A qualitative evaluation framework for paraphrase identification, 2019, Proceedings of the International Conference on Recent Advances in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "M Venelin Kovatchev",
                    "Maria Marti",
                    "Javier Salamo",
                    " Beltran"
                ],
                "title": "A qualitative evaluation framework for paraphrase identification",
                "pub_date": "2019",
                "pub_title": "Proceedings of the International Conference on Recent Advances in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_48",
            "content": "UNKNOWN, None, 2019, Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_49",
            "content": "Hector Levesque, Ernest Davis, Leora Morgenstern, The winograd schema challenge, 2012, Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Hector Levesque",
                    "Ernest Davis",
                    "Leora Morgenstern"
                ],
                "title": "The winograd schema challenge",
                "pub_date": "2012",
                "pub_title": "Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "196-ARR_v2_50",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_51",
            "content": "Roser Morante, Walter Daelemans, ConanDoyle-neg: Annotation of negation cues and their scope in conan doyle stories, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Roser Morante",
                    "Walter Daelemans"
                ],
                "title": "ConanDoyle-neg: Annotation of negation cues and their scope in conan doyle stories",
                "pub_date": "2012",
                "pub_title": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_52",
            "content": "Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, Graham Neubig, Stress test evaluation for natural language inference, 2018, Proceedings of the 27th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Aakanksha Naik",
                    "Abhilasha Ravichander",
                    "Norman Sadeh",
                    "Carolyn Rose",
                    "Graham Neubig"
                ],
                "title": "Stress test evaluation for natural language inference",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 27th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_53",
            "content": "UNKNOWN, None, , 2020. jiant 2.0: A software toolkit for research on general-purpose text understanding models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "2020. jiant 2.0: A software toolkit for research on general-purpose text understanding models",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_54",
            "content": "Mohammad Taher Pilehvar, Jose Camacho-Collados, WiC: the word-in-context dataset for evaluating context-sensitive meaning representations, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Mohammad Taher Pilehvar",
                    "Jose Camacho-Collados"
                ],
                "title": "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "196-ARR_v2_55",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100,000+ questions for machine comprehension of text, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Pranav Rajpurkar",
                    "Jian Zhang",
                    "Konstantin Lopyrev",
                    "Percy Liang"
                ],
                "title": "SQuAD: 100,000+ questions for machine comprehension of text",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "196-ARR_v2_56",
            "content": "Tongshuang Marco Tulio Ribeiro, Carlos Wu, Sameer Guestrin,  Singh, Beyond accuracy: Behavioral testing of NLP models with CheckList, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Tongshuang Marco Tulio Ribeiro",
                    "Carlos Wu",
                    "Sameer Guestrin",
                    " Singh"
                ],
                "title": "Beyond accuracy: Behavioral testing of NLP models with CheckList",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_57",
            "content": "Melissa Roemmele, Andrew S Cosmin Adrian Bejan,  Gordon, Choice of plausible alternatives: An evaluation of commonsense causal reasoning, 2011, AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Melissa Roemmele",
                    "Andrew S Cosmin Adrian Bejan",
                    " Gordon"
                ],
                "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
                "pub_date": "2011",
                "pub_title": "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_58",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "Christopher Manning",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "196-ARR_v2_59",
            "content": "Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, Ido Dagan, Supervised open information extraction, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Gabriel Stanovsky",
                    "Julian Michael",
                    "Luke Zettlemoyer",
                    "Ido Dagan"
                ],
                "title": "Supervised open information extraction",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "196-ARR_v2_60",
            "content": "Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, CommonsenseQA: A question answering challenge targeting commonsense knowledge, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Alon Talmor",
                    "Jonathan Herzig",
                    "Nicholas Lourie",
                    "Jonathan Berant"
                ],
                "title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "196-ARR_v2_61",
            "content": "Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh, Universal adversarial triggers for attacking and analyzing NLP, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Eric Wallace",
                    "Shi Feng",
                    "Nikhil Kandpal",
                    "Matt Gardner",
                    "Sameer Singh"
                ],
                "title": "Universal adversarial triggers for attacking and analyzing NLP",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_62",
            "content": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Superglue: A stickier benchmark for general-purpose language understanding systems, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Alex Wang",
                    "Yada Pruksachatkun",
                    "Nikita Nangia",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel Bowman"
                ],
                "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "196-ARR_v2_63",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2018, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Alex Wang",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel Bowman"
                ],
                "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_64",
            "content": "Alex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Hagen Blix, Yining Nie, Anna Alsop, Shikha Bordia, Haokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason Phang, Anhad Mohananey, Paloma Phu Mon Htut, Samuel Jeretic,  Bowman, Investigating BERT's knowledge of language: Five analysis methods with NPIs, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Alex Warstadt",
                    "Yu Cao",
                    "Ioana Grosu",
                    "Wei Peng",
                    "Hagen Blix",
                    "Yining Nie",
                    "Anna Alsop",
                    "Shikha Bordia",
                    "Haokun Liu",
                    "Alicia Parrish",
                    "Sheng-Fu Wang",
                    "Jason Phang",
                    "Anhad Mohananey",
                    "Paloma Phu Mon Htut",
                    "Samuel Jeretic",
                    " Bowman"
                ],
                "title": "Investigating BERT's knowledge of language: Five analysis methods with NPIs",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "196-ARR_v2_65",
            "content": "Adina Williams, Nikita Nangia, Samuel Bowman, A broad-coverage challenge corpus for sentence understanding through inference, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Adina Williams",
                    "Nikita Nangia",
                    "Samuel Bowman"
                ],
                "title": "A broad-coverage challenge corpus for sentence understanding through inference",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "196-ARR_v2_66",
            "content": "Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, Jiwei Li, CorefQA: Coreference resolution as querybased span prediction, 2020, Proceedings of the 58th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Wei Wu",
                    "Fei Wang",
                    "Arianna Yuan",
                    "Fei Wu",
                    "Jiwei Li"
                ],
                "title": "CorefQA: Coreference resolution as querybased span prediction",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_67",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "196-ARR_v2_68",
            "content": "Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze, Johan Bos, HELP: A dataset for identifying shortcomings of neural models in monotonicity reasoning, 2019, Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Hitomi Yanaka",
                    "Koji Mineshima",
                    "Daisuke Bekki",
                    "Kentaro Inui",
                    "Satoshi Sekine",
                    "Lasha Abzianidze",
                    "Johan Bos"
                ],
                "title": "HELP: A dataset for identifying shortcomings of neural models in monotonicity reasoning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)",
                "pub": null
            }
        },
        {
            "ix": "196-ARR_v2_69",
            "content": "An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, Hua Wu, Qiaoqiao She, Sujian Li, Enhancing pre-trained language representations with rich knowledge for machine reading comprehension, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "An Yang",
                    "Quan Wang",
                    "Jing Liu",
                    "Kai Liu",
                    "Yajuan Lyu",
                    "Hua Wu",
                    "Qiaoqiao She",
                    "Sujian Li"
                ],
                "title": "Enhancing pre-trained language representations with rich knowledge for machine reading comprehension",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "196-ARR_v2_0@0",
            "content": "An Analysis of Negation in Natural Language Understanding Corpora",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_0",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_2@0",
            "content": "This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_2",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_2@1",
            "content": "We show that these corpora have few negations compared to generalpurpose English, and that the few negations in them are often unimportant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_2",
            "start": 105,
            "end": 243,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_2@2",
            "content": "Indeed, one can often ignore negations and still make the right predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_2",
            "start": 245,
            "end": 320,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_2@3",
            "content": "Additionally, experimental results show that state-of-the-art transformers trained with these corpora obtain substantially worse results with instances that contain negation, especially if the negations are important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_2",
            "start": 322,
            "end": 538,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_2@4",
            "content": "We conclude that new corpora accounting for negation are needed to solve natural language understanding tasks when negation is present.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_2",
            "start": 540,
            "end": 674,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_4@0",
            "content": "Natural language understanding (NLU) is an umbrella term used to refer to any task that requires text understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_4",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_4@1",
            "content": "For example, question answering (Rajpurkar et al., 2016), information extraction (Stanovsky et al., 2018), coreference resolution , and machine reading (Yang et al., 2019), among many others, are tasks that fall under natural language understanding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_4",
            "start": 117,
            "end": 365,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_4@2",
            "content": "The threshold for claiming that a system understands natural language is ever-moving.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_4",
            "start": 367,
            "end": 451,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_4@3",
            "content": "New corpora are often justified by pointing out that state-of-the-art models do not obtain good results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_4",
            "start": 453,
            "end": 556,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_4@4",
            "content": "After years of steady improvements, more powerful models eventually obtain so-called human performance, and at that point new, more challenging corpora are created.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_4",
            "start": 558,
            "end": 721,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_5@0",
            "content": "Many corpora for natural language understanding tasks contain language generated by annotators rather than retrieved from texts written independently of the corpus creation process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_5",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_5@1",
            "content": "These corpora are certainly useful and have facilitated tremendous progress.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_5",
            "start": 182,
            "end": 257,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_5@2",
            "content": "Annotator-generated examples, however, carry the risk of evaluating systems with synthetic language that is not representative of language in the wild.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_5",
            "start": 259,
            "end": 409,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_5@3",
            "content": "For example, annotators are likely to use negation when asked to write a text that contradicts something despite contradictions in the wild need not have a negation (Gururangan et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_5",
            "start": 411,
            "end": 601,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_5@4",
            "content": "Recently, Kwiatkowski et al. (2019) present a large corpus for question answering that consists of natural questions (i.e., asked by somebody with a real information need) in order to encourage research in a more realistic scenario.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_5",
            "start": 603,
            "end": 834,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_5@5",
            "content": "This contrasts with previous corpora, where the questions were written by annotators after being told the answer (Rajpurkar et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_5",
            "start": 836,
            "end": 973,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_6@0",
            "content": "In this paper, we explore the role of negation in eight corpora for six popular natural language understanding tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_6",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_6@1",
            "content": "Our goal is to check whether negation plays the role it deserves in these tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_6",
            "start": 118,
            "end": 197,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_6@2",
            "content": "To our surprise, we conclude that negation is virtually ignored by answering the following questions: 1 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_6",
            "start": 199,
            "end": 304,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_6@3",
            "content": "Do NLU corpora contain as many negations as general-purpose texts? (they don't); 2. Do the (few) negations in NLU corpora play a role in solving the tasks? (they don't); and 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_6",
            "start": 306,
            "end": 481,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_6@4",
            "content": "Do state-of-the-art transformers trained with NLU corpora face challenges with instances that contain negation? (they do, especially if the negation is important).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_6",
            "start": 483,
            "end": 645,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_7@0",
            "content": "Background and Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_7",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_8@0",
            "content": "We work with the eight corpora covering six tasks summarized below and exemplified in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_8",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_8@1",
            "content": "We select two corpora for question answering: CommonsenseQA (Talmor et al., 2019) and COPA (Roemmele et al., 2011).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_8",
            "start": 95,
            "end": 209,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_8@2",
            "content": "CommonsenseQA consists of multi-choice questions (5 candidate answers) that require some degree of commonsense.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_8",
            "start": 211,
            "end": 321,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_8@3",
            "content": "COPA presents a premise (e.g., The man broke his toe) and a question (e.g., What was the cause of this?) and the system must choose between two plausible alternatives (e.g. He got a hole in his sock or He dropped a hammer on his foot).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_8",
            "start": 323,
            "end": 557,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_9@0",
            "content": "For textual similarity and paraphrasing, we select QQP 2 and STS-B (Cer et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_9",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_9@1",
            "content": "QQP consists of pairs of questions and the task is to determine whether they are paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_9",
            "start": 87,
            "end": 179,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_9@2",
            "content": "STS-B consists of pairs of texts and the task is to determine how semantically similar they are with a score from 0 to 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_9",
            "start": 181,
            "end": 301,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_10@0",
            "content": "We select one corpus for the remaining tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_10",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_10@1",
            "content": "For inference, we work with QNLI (Rajpurkar et al., 2016), which consists in determining whether a text is a valid answer to a question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_10",
            "start": 46,
            "end": 181,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_10@2",
            "content": "We use WiC (Pilehvar and Camacho-Collados, 2019) for word sense disambiguation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_10",
            "start": 183,
            "end": 261,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_10@3",
            "content": "WiC consists in determining whether two instances of the same word (in two sentences; italicized in Table 2) are used with the same meaning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_10",
            "start": 263,
            "end": 402,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_10@4",
            "content": "For coreference resolution, we choose WSC (Levesque et al., 2012), which consists in determining whether a pronoun and a noun phrase are co-referential (italicized in Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_10",
            "start": 404,
            "end": 579,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_10@5",
            "content": "Finally, we work with SST-2 (Socher et al., 2013) for sentiment analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_10",
            "start": 581,
            "end": 653,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_10@6",
            "content": "The task consists in determining whether a sentence from a collection of movie reviews has positive or negative sentiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_10",
            "start": 655,
            "end": 776,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_11@0",
            "content": "For convenience, we work with the formatted versions of these corpora in the GLUE (Wang et al., 2018) and SuperGLUE benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_11",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_11@1",
            "content": "The only exception is CommonsenseQA, which is not part of these benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_11",
            "start": 128,
            "end": 202,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_11@2",
            "content": "Related Work Previous work has shown that SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) have annotation artifacts (e.g., negation is a strong indicator of contradictions) (Gururangan et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_11",
            "start": 204,
            "end": 414,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_11@3",
            "content": "The literature has also shown that simple adversarial attacks including negation cues are very effective (Naik et al., 2018;Wallace et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_11",
            "start": 416,
            "end": 561,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_11@4",
            "content": "Kovatchev et al. (2019) analyze 11 paraphrasing systems and show that they obtain substantially worse results when negation is present.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_11",
            "start": 563,
            "end": 697,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_12@0",
            "content": "More recently, Ribeiro et al. (2020) show that negation is one of the linguistic phenomena commercial sentiment analysis struggle with.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_12",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_12@1",
            "content": "Several previous works have investigated the (lack of) ability of transformers to make inferences when negation is present.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_12",
            "start": 136,
            "end": 258,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_12@2",
            "content": "For example, Ettinger (2020) conclude that BERT is unable to complete sentences when negation is present.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_12",
            "start": 260,
            "end": 364,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_12@3",
            "content": "BERT also faces challenges solving the task of natural language inference (i.e., identifying entailments and contradictions) with monotonicity and negation (Geiger et al., 2020;Yanaka et al., 2019) (2019) show the limitations of BERT making acceptability judgments with sentences that contain negative polarity items.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_12",
            "start": 366,
            "end": 682,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_12@4",
            "content": "Most related to out work, Hossain et al. ( 2020) analyze the role of negation in three natural language inference corpora: RTE Bar-Haim et al., 2006;Giampiccolo et al., 2007;Bentivogli et al., 2009), SNLI and MNLI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_12",
            "start": 684,
            "end": 897,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_12@5",
            "content": "In this paper, we present a similar analysis, but we move beyond natural language inference and work with eight corpora spanning six natural language understanding tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_12",
            "start": 899,
            "end": 1068,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_13@0",
            "content": "3 Research Questions and Analysis Q1: Do natural language understanding corpora contain as many negations as generalpurpose English texts?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_13",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_13@1",
            "content": "In order to automatically identify negation cues, we train a negation cue detector with the largest corpus available, ConanDoyle-neg (Morante and Daelemans, 2012).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_13",
            "start": 139,
            "end": 301,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_13@2",
            "content": "The cue detector is based on the RoBERTa pretrained language model Well for one a being could have a non-physical existance and yet not even be in your mind.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_13",
            "start": 303,
            "end": 459,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_14@0",
            "content": "3.4 \u2717 The difference is huge, as not all non-physical things exist in minds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_14",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_15@0",
            "content": "Who did BSkyB team up with as it was not part of consortium? yes \u2713 While BSkyB had been excluded from being a part of the [. . . ], BSkyB was able to join ITV Digital's free-to-air replacement, Freeview, in which it holds an equal stake [. . . ] In what year did Lavoisier publish his work on combustion? no \u2717 In one experiment, Lavoisier observed that there was no overall increase in weight when tin and air were heated in a closed container.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_15",
            "start": 0,
            "end": 443,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_16@0",
            "content": "It's not the ultimate depression-era gangster movie.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_16",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_16@1",
            "content": "neg.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_16",
            "start": 53,
            "end": 56,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_16@2",
            "content": "\u2713",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_16",
            "start": 58,
            "end": 58,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_17@0",
            "content": "Whaley's determination to immerse you in sheer, unrelenting wretchedness is exhausting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_17",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_17@1",
            "content": "neg.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_17",
            "start": 88,
            "end": 91,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_17@2",
            "content": "\u2717",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_17",
            "start": 93,
            "end": 93,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_18@0",
            "content": "The intention of this legislation is to boost the economy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_18",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_18@1",
            "content": "same \u2717 Good intentions are not enough.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_18",
            "start": 59,
            "end": 96,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_19@0",
            "content": "Sam and Amy are passionately in love, but Amy's parents are unhappy about it, because they are only fifteen. yes \u2717",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_19",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_20@0",
            "content": "Table 2: Examples containing negation (underlined) from the validation datasets of the natural language understanding corpora we work with.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_20",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_20@1",
            "content": "The third column presents the expected answer for the example (a choice, judgment, or score depending on the task).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_20",
            "start": 140,
            "end": 254,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_20@2",
            "content": "The last column indicates whether the negation is important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_20",
            "start": 256,
            "end": 315,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_21@0",
            "content": "Table 1 presents the percentage of sentences that contain negation in (a) the eight corpora we work with and (b) general-purpose English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_21",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_21@1",
            "content": "We take the latter percentage (all sentences) from Hossain et al. ( 2020), who run a negation cue detector in online reviews, conversations, and books.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_21",
            "start": 138,
            "end": 288,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_21@2",
            "content": "Additionally, we also present the percentages in questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_21",
            "start": 290,
            "end": 348,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_21@3",
            "content": "Negation is much less common in all natural language understanding corpora but WSC (0.8%-16%) than in general-purpose English (22.6%-29.9%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_21",
            "start": 350,
            "end": 489,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_21@4",
            "content": "Note that negation is also underrepresented in corpora that primarily contain questions (general-purpose: 15.8%-20.2%; COPA: 0.8%, QQP: 8.1%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_21",
            "start": 491,
            "end": 632,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_22@0",
            "content": "Q2: Do the (few) negations in natural language understanding corpora play a role in solving the tasks? After showing that negation in underrepresented in natural language understanding corpora, we explore whether the few negations they contain are important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_22",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_22@1",
            "content": "Given an instance from any of the corpora, we consider a negation important if removing it changes the ground truth.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_22",
            "start": 259,
            "end": 374,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_22@2",
            "content": "In other words, a negation is unimportant if one can ignore it and still solve the task at hand.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_22",
            "start": 376,
            "end": 471,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_22@3",
            "content": "Table 2 presents examples of important and unimportant negations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_22",
            "start": 473,
            "end": 537,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_23@0",
            "content": "We manually examine the negations in all instances containing negation from the validation split of each corpus except QQP, for which we examine 1,000 (out of 5,196).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_23",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_23@1",
            "content": "Note that COPA does not have any negations in the validation split, and many corpora have few instances containing negation (CommonsenseQA: 184, STS-B: 225, QNLI: 852, WiC: 99, WSC: 52, and SST-2: 263).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_23",
            "start": 167,
            "end": 368,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_23@2",
            "content": "We choose to work with the validation set because we want to compare results when negation is and is not important (Q3), and the ground truth for the test splits of some corpora are not publicly available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_23",
            "start": 370,
            "end": 574,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_24@0",
            "content": "We observe that (a) all negations in WiC and WSC are unimportant, and (b) the percentages of unimportant negations in CommonsenseQA, SST-2, QQP, STS-B, and QNLI are substantial: 45.1%, 63%, 97.4%, 95.6%, and 97.7%, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_24",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_24@1",
            "content": "These percentages indicate that one can safely ignore (almost) all negations and still solve the benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_24",
            "start": 229,
            "end": 336,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_24@2",
            "content": "Despite the fact that negations are not important in WSC and WiC, they do affect the experimental results (details in Q3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_24",
            "start": 338,
            "end": 459,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@0",
            "content": "We also analyze the role of two major types of negation: syntactic (not, no, never, etc.) and morphological (i.e., affixes such as un-, im-, and -less).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@1",
            "content": "To this end, we work with CommonsenseQA and SST-2, which have lower percentages of unimportant negations (45.1% and 63%) than the other corpora we use (97.4%-100%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 153,
            "end": 316,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@2",
            "content": "Table 3 provides examples of these two negation types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 318,
            "end": 371,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@3",
            "content": "Perhaps unsurprisingly, syntactic negations are much more common than morphological negations (Common-senseQA: 88.6% vs 11.4%, SST-2: 71.9% vs 28.1%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 373,
            "end": 522,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@4",
            "content": "More importantly, syntactic negations are more often important in SST-2 (42.3% vs 23%), but both syntactic and morphological negation are roughly equaly important in CommonsenseQA (55.2% vs 52.4%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 524,
            "end": 720,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@5",
            "content": "Q3: Do state-of-the-art transformers trained with NLU corpora face challenges with instances that contain negation?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 722,
            "end": 836,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@6",
            "content": "We conduct experiments with RoBERTa .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 838,
            "end": 874,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@7",
            "content": "More specifically, we use the implementation by Phang et al. (2020) and train a model with the training split of each corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 876,
            "end": 1000,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@8",
            "content": "We refer the readers to the Appendix B for the details about these models and hyperparameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 1002,
            "end": 1095,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@9",
            "content": "We chose RoBERTa over other transformers because 4 out of the 10 best submissions to the SuperGLUE benchmark use it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 1097,
            "end": 1212,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@10",
            "content": "3 Table 4 presents the results evaluating the models with the corresponding validation splits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 1214,
            "end": 1307,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@11",
            "content": "RoBERTa obtains slightly worse results with the validation instances that have negation in all corpora; the only exception is QQP (F1: 0.90 vs. 0.91).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 1309,
            "end": 1458,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_25@12",
            "content": "These results lead to the conclusion that negation may only pose a small challenge to state-of-the-art transformers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_25",
            "start": 1460,
            "end": 1575,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_26@0",
            "content": "The results obtained evaluating with the important and unimportant negations from the samples analyzed in Question 2, however, provide a different picture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_26",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_26@1",
            "content": "Indeed, we observe substantial drops in results in all tasks that have both kinds of negations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_26",
            "start": 156,
            "end": 250,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_26@2",
            "content": "More specifically, we obtain 27% lower results with instances containing important negations in QNLI (F1: 0.92 vs. 0.67), 33%/26% lower in STS-B, 24% lower in CommonsenseQA, 21% lower in QQP, and 9% lower in SST.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_26",
            "start": 252,
            "end": 463,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_26@3",
            "content": "Further, even though all negations are unimportant in WiC and WSC, we observe a drop in performance for the instances with negation compared to the instances without negation (WiC: 0.64 vs 0.67 and WSC: 0.59 vs 0.63).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_26",
            "start": 465,
            "end": 681,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_26@4",
            "content": "We conclude that transformers trained with existing NLU corpora face challenges with instances that contain negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_26",
            "start": 683,
            "end": 799,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_26@5",
            "content": "These results raise two important questions for future research: Is negation an inherently challenging phenomenon for RoBERTa?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_26",
            "start": 801,
            "end": 926,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_26@6",
            "content": "How many instances with negation are required to solve a natural language understanding task?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_26",
            "start": 928,
            "end": 1020,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_27@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_27",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_28@0",
            "content": "We have analyzed the role of negation in eight natural language understanding corpora covering six tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_28",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_28@1",
            "content": "Our analyses show that (a) all but WSC conalmost no negations or around 31%-54% of the negations found in general-purpose texts, (b) the few negations in these corpora are usually unimportant, and (c) RoBERTa obtains substantially worse results when negation is important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_28",
            "start": 106,
            "end": 377,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_29@0",
            "content": "Our analyses also provide some evidence that creating models to properly deal with negation may require both new corpora and more powerful models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_29",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_29@1",
            "content": "The need for new corpora stems from the answers to Questions 1 and 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_29",
            "start": 147,
            "end": 215,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_29@2",
            "content": "The justification for powerful models is more subtle.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_29",
            "start": 217,
            "end": 269,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_29@3",
            "content": "We point out that the percentage of unimportant negations (Section 3) is only a weak indicator of the drop in results with important negations (Table 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_29",
            "start": 271,
            "end": 423,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_29@4",
            "content": "For example, we observe a 24% and 21% drop in results with important negations from CommonsenseQA and QQP despite 45% and 97% of negations are unimportant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_29",
            "start": 425,
            "end": 579,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_30@0",
            "content": "Negation reverses truth values thus solutions to any natural language understanding task should be robust when negation is present and important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_30",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_30@1",
            "content": "To this end, our future work includes two lines of research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_30",
            "start": 146,
            "end": 205,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_30@2",
            "content": "First, we plan to create benchmarks for the six tasks consisting of instances containing negation (50/50 split important/unimportant).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_30",
            "start": 207,
            "end": 340,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_30@3",
            "content": "Second, we plan to conduct probing experiments to investigate whether (and where) pretrained transformers capture the meaning of negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_30",
            "start": 342,
            "end": 479,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_30@4",
            "content": "Doing so may help us discover potential solutions to understand negation and make inferences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_30",
            "start": 481,
            "end": 573,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_31@0",
            "content": "We develop a negation cue detector (Section 3 in the paper) by utilizing the RoBERTa (base architecture; 12 layers) pre-trained model .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_31",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_31@1",
            "content": "We fine-tune the system on ConanDoyleneg (Morante and Daelemans, 2012) corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_31",
            "start": 136,
            "end": 213,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_31@2",
            "content": "While fine-training, the negation cues are marked with BIO (B: Beginning of cue, I: Inside of cue, O: Outside of cue) tagging scheme.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_31",
            "start": 215,
            "end": 347,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_31@3",
            "content": "The contextualized representations from the last layer of RoBERTa are passed to a fully connected (FC) layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_31",
            "start": 349,
            "end": 457,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_31@4",
            "content": "Finally, a conditional random field (CRF) layer produces the output sequence for the labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_31",
            "start": 459,
            "end": 550,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_32@0",
            "content": "Our model yields the following results on the test set: 93.26 Precision, 94.32 Recall, and 93.79 F1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_32",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_32@1",
            "content": "The neural model takes about two hours on average to train on a single GPU of NVIDIA Tesla K80.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_32",
            "start": 101,
            "end": 195,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_32@2",
            "content": "A list of the tuned hyperparameters that the model requires to achieve the above results is provided in Table 6: Hyperparameters used to fine-tune RoBERTa individually for each corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_32",
            "start": 197,
            "end": 380,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_32@3",
            "content": "Hp-1, Hp-2, and Hp-3 refer to the number of epochs, batch size, and learning rate used in the training procedure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_32",
            "start": 382,
            "end": 494,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_32@4",
            "content": "We use default settings for the other hyperparameters when we use the implementation by Phang et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_32",
            "start": 496,
            "end": 603,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_33@0",
            "content": "B Hyperparameters to Fine-tune the System for Each of the NLU Tasks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_33",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_34@0",
            "content": "We use an implementation by Phang et al. (2020) and fine-tune RoBERTa (base architecture; 12 layers) model separately for each of the eight corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_34",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_34@1",
            "content": "We use the default settings of the hyperparameters, except for a few, when fine-tuning the model on each benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_34",
            "start": 149,
            "end": 263,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_34@2",
            "content": "Table 6 shows tuned hyperparameters for each benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_34",
            "start": 265,
            "end": 319,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_35@0",
            "content": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, Idan Szpektor, The second pascal recognising textual entailment challenge, 2006, Proceedings of the second PASCAL challenges workshop on recognising textual entailment, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_35",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_36@0",
            "content": "UNKNOWN, None, 2009, The fifth pascal recognizing textual entailment challenge, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_36",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_37@0",
            "content": "R Samuel, Gabor Bowman, Christopher Angeli, Christopher Potts,  Manning, A large annotated corpus for learning natural language inference, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_37",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_38@0",
            "content": "Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, Lucia Specia, SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation, 2017, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_38",
            "start": 0,
            "end": 303,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_39@0",
            "content": "Oren Ido Dagan, Bernardo Glickman,  Magnini, The pascal recognising textual entailment challenge, 2006, Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05, Springer-Verlag.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_39",
            "start": 0,
            "end": 310,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_40@0",
            "content": "Allyson Ettinger, What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_40",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_41@0",
            "content": "Atticus Geiger, Kyle Richardson, Christopher Potts, Neural natural language inference models partially embed theories of lexical entailment and negation, 2020, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_41",
            "start": 0,
            "end": 310,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_42@0",
            "content": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, Bill Dolan, The third PASCAL recognizing textual entailment challenge, 2007, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_42",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_43@0",
            "content": "Swabha Suchin Gururangan, Omer Swayamdipta, Roy Levy, Samuel Schwartz, Noah Bowman,  Smith, Annotation artifacts in natural language inference data, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_43",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_44@0",
            "content": "Venelin Md Mosharaf Hossain, Pranoy Kovatchev, Tiffany Dutta, Elizabeth Kao, Eduardo Wei,  Blanco, An analysis of natural language inference benchmarks through the lens of negation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_44",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_45@0",
            "content": "Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi Gunawi, Cody Hammock, Joe Mambretti, Lessons learned from the chameleon testbed, 2020, Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC '20). USENIX Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_45",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_46@0",
            "content": "Aditya Khandelwal, Suraj Sawant, NegBERT: A transfer learning approach for negation detection and scope resolution, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, European Language Resources Association.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_46",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_47@0",
            "content": "M Venelin Kovatchev, Maria Marti, Javier Salamo,  Beltran, A qualitative evaluation framework for paraphrase identification, 2019, Proceedings of the International Conference on Recent Advances in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_47",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_48@0",
            "content": "UNKNOWN, None, 2019, Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_48",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_49@0",
            "content": "Hector Levesque, Ernest Davis, Leora Morgenstern, The winograd schema challenge, 2012, Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_49",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_50@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_50",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_51@0",
            "content": "Roser Morante, Walter Daelemans, ConanDoyle-neg: Annotation of negation cues and their scope in conan doyle stories, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_51",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_52@0",
            "content": "Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, Graham Neubig, Stress test evaluation for natural language inference, 2018, Proceedings of the 27th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_52",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_53@0",
            "content": "UNKNOWN, None, , 2020. jiant 2.0: A software toolkit for research on general-purpose text understanding models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_53",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_54@0",
            "content": "Mohammad Taher Pilehvar, Jose Camacho-Collados, WiC: the word-in-context dataset for evaluating context-sensitive meaning representations, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_54",
            "start": 0,
            "end": 310,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_55@0",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100,000+ questions for machine comprehension of text, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_55",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_56@0",
            "content": "Tongshuang Marco Tulio Ribeiro, Carlos Wu, Sameer Guestrin,  Singh, Beyond accuracy: Behavioral testing of NLP models with CheckList, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_56",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_57@0",
            "content": "Melissa Roemmele, Andrew S Cosmin Adrian Bejan,  Gordon, Choice of plausible alternatives: An evaluation of commonsense causal reasoning, 2011, AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_57",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_58@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_58",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_59@0",
            "content": "Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, Ido Dagan, Supervised open information extraction, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_59",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_60@0",
            "content": "Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, CommonsenseQA: A question answering challenge targeting commonsense knowledge, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_60",
            "start": 0,
            "end": 334,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_61@0",
            "content": "Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh, Universal adversarial triggers for attacking and analyzing NLP, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_61",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_62@0",
            "content": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Superglue: A stickier benchmark for general-purpose language understanding systems, 2019, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_62",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_63@0",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2018, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_63",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_64@0",
            "content": "Alex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Hagen Blix, Yining Nie, Anna Alsop, Shikha Bordia, Haokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason Phang, Anhad Mohananey, Paloma Phu Mon Htut, Samuel Jeretic,  Bowman, Investigating BERT's knowledge of language: Five analysis methods with NPIs, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_64",
            "start": 0,
            "end": 517,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_65@0",
            "content": "Adina Williams, Nikita Nangia, Samuel Bowman, A broad-coverage challenge corpus for sentence understanding through inference, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_65",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_66@0",
            "content": "Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, Jiwei Li, CorefQA: Coreference resolution as querybased span prediction, 2020, Proceedings of the 58th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_66",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_67@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_67",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_68@0",
            "content": "Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze, Johan Bos, HELP: A dataset for identifying shortcomings of neural models in monotonicity reasoning, 2019, Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_68",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "196-ARR_v2_69@0",
            "content": "An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, Hua Wu, Qiaoqiao She, Sujian Li, Enhancing pre-trained language representations with rich knowledge for machine reading comprehension, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "196-ARR_v2_69",
            "start": 0,
            "end": 322,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "196-ARR_v2_0",
            "tgt_ix": "196-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_0",
            "tgt_ix": "196-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_1",
            "tgt_ix": "196-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_1",
            "tgt_ix": "196-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_0",
            "tgt_ix": "196-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_2",
            "tgt_ix": "196-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_4",
            "tgt_ix": "196-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_5",
            "tgt_ix": "196-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_3",
            "tgt_ix": "196-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_3",
            "tgt_ix": "196-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_3",
            "tgt_ix": "196-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_3",
            "tgt_ix": "196-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_0",
            "tgt_ix": "196-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_6",
            "tgt_ix": "196-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_8",
            "tgt_ix": "196-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_9",
            "tgt_ix": "196-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_10",
            "tgt_ix": "196-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_11",
            "tgt_ix": "196-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_12",
            "tgt_ix": "196-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_13",
            "tgt_ix": "196-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_14",
            "tgt_ix": "196-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_16",
            "tgt_ix": "196-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_15",
            "tgt_ix": "196-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_17",
            "tgt_ix": "196-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_19",
            "tgt_ix": "196-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_20",
            "tgt_ix": "196-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_21",
            "tgt_ix": "196-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_22",
            "tgt_ix": "196-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_23",
            "tgt_ix": "196-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_24",
            "tgt_ix": "196-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_18",
            "tgt_ix": "196-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_0",
            "tgt_ix": "196-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_26",
            "tgt_ix": "196-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_28",
            "tgt_ix": "196-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_29",
            "tgt_ix": "196-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_27",
            "tgt_ix": "196-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_27",
            "tgt_ix": "196-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_27",
            "tgt_ix": "196-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_27",
            "tgt_ix": "196-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_31",
            "tgt_ix": "196-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_32",
            "tgt_ix": "196-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_33",
            "tgt_ix": "196-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_27",
            "tgt_ix": "196-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_27",
            "tgt_ix": "196-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_27",
            "tgt_ix": "196-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_27",
            "tgt_ix": "196-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_30",
            "tgt_ix": "196-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "196-ARR_v2_0",
            "tgt_ix": "196-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_1",
            "tgt_ix": "196-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_2",
            "tgt_ix": "196-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_2",
            "tgt_ix": "196-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_2",
            "tgt_ix": "196-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_2",
            "tgt_ix": "196-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_2",
            "tgt_ix": "196-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_3",
            "tgt_ix": "196-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_4",
            "tgt_ix": "196-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_4",
            "tgt_ix": "196-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_4",
            "tgt_ix": "196-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_4",
            "tgt_ix": "196-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_4",
            "tgt_ix": "196-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_5",
            "tgt_ix": "196-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_5",
            "tgt_ix": "196-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_5",
            "tgt_ix": "196-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_5",
            "tgt_ix": "196-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_5",
            "tgt_ix": "196-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_5",
            "tgt_ix": "196-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_6",
            "tgt_ix": "196-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_6",
            "tgt_ix": "196-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_6",
            "tgt_ix": "196-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_6",
            "tgt_ix": "196-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_6",
            "tgt_ix": "196-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_7",
            "tgt_ix": "196-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_8",
            "tgt_ix": "196-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_8",
            "tgt_ix": "196-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_8",
            "tgt_ix": "196-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_8",
            "tgt_ix": "196-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_9",
            "tgt_ix": "196-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_9",
            "tgt_ix": "196-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_9",
            "tgt_ix": "196-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_10",
            "tgt_ix": "196-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_10",
            "tgt_ix": "196-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_10",
            "tgt_ix": "196-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_10",
            "tgt_ix": "196-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_10",
            "tgt_ix": "196-ARR_v2_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_10",
            "tgt_ix": "196-ARR_v2_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_10",
            "tgt_ix": "196-ARR_v2_10@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_11",
            "tgt_ix": "196-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_11",
            "tgt_ix": "196-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_11",
            "tgt_ix": "196-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_11",
            "tgt_ix": "196-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_11",
            "tgt_ix": "196-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_12",
            "tgt_ix": "196-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_12",
            "tgt_ix": "196-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_12",
            "tgt_ix": "196-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_12",
            "tgt_ix": "196-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_12",
            "tgt_ix": "196-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_12",
            "tgt_ix": "196-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_13",
            "tgt_ix": "196-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_13",
            "tgt_ix": "196-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_13",
            "tgt_ix": "196-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_14",
            "tgt_ix": "196-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_15",
            "tgt_ix": "196-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_16",
            "tgt_ix": "196-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_16",
            "tgt_ix": "196-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_16",
            "tgt_ix": "196-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_17",
            "tgt_ix": "196-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_17",
            "tgt_ix": "196-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_17",
            "tgt_ix": "196-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_18",
            "tgt_ix": "196-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_18",
            "tgt_ix": "196-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_19",
            "tgt_ix": "196-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_20",
            "tgt_ix": "196-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_20",
            "tgt_ix": "196-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_20",
            "tgt_ix": "196-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_21",
            "tgt_ix": "196-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_21",
            "tgt_ix": "196-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_21",
            "tgt_ix": "196-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_21",
            "tgt_ix": "196-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_21",
            "tgt_ix": "196-ARR_v2_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_22",
            "tgt_ix": "196-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_22",
            "tgt_ix": "196-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_22",
            "tgt_ix": "196-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_22",
            "tgt_ix": "196-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_23",
            "tgt_ix": "196-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_23",
            "tgt_ix": "196-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_23",
            "tgt_ix": "196-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_24",
            "tgt_ix": "196-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_24",
            "tgt_ix": "196-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_24",
            "tgt_ix": "196-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_25",
            "tgt_ix": "196-ARR_v2_25@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_26",
            "tgt_ix": "196-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_26",
            "tgt_ix": "196-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_26",
            "tgt_ix": "196-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_26",
            "tgt_ix": "196-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_26",
            "tgt_ix": "196-ARR_v2_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_26",
            "tgt_ix": "196-ARR_v2_26@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_26",
            "tgt_ix": "196-ARR_v2_26@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_27",
            "tgt_ix": "196-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_28",
            "tgt_ix": "196-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_28",
            "tgt_ix": "196-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_29",
            "tgt_ix": "196-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_29",
            "tgt_ix": "196-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_29",
            "tgt_ix": "196-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_29",
            "tgt_ix": "196-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_29",
            "tgt_ix": "196-ARR_v2_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_30",
            "tgt_ix": "196-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_30",
            "tgt_ix": "196-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_30",
            "tgt_ix": "196-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_30",
            "tgt_ix": "196-ARR_v2_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_30",
            "tgt_ix": "196-ARR_v2_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_31",
            "tgt_ix": "196-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_31",
            "tgt_ix": "196-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_31",
            "tgt_ix": "196-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_31",
            "tgt_ix": "196-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_31",
            "tgt_ix": "196-ARR_v2_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_32",
            "tgt_ix": "196-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_32",
            "tgt_ix": "196-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_32",
            "tgt_ix": "196-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_32",
            "tgt_ix": "196-ARR_v2_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_32",
            "tgt_ix": "196-ARR_v2_32@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_33",
            "tgt_ix": "196-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_34",
            "tgt_ix": "196-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_34",
            "tgt_ix": "196-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_34",
            "tgt_ix": "196-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_35",
            "tgt_ix": "196-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_36",
            "tgt_ix": "196-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_37",
            "tgt_ix": "196-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_38",
            "tgt_ix": "196-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_39",
            "tgt_ix": "196-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_40",
            "tgt_ix": "196-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_41",
            "tgt_ix": "196-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_42",
            "tgt_ix": "196-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_43",
            "tgt_ix": "196-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_44",
            "tgt_ix": "196-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_45",
            "tgt_ix": "196-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_46",
            "tgt_ix": "196-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_47",
            "tgt_ix": "196-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_48",
            "tgt_ix": "196-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_49",
            "tgt_ix": "196-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_50",
            "tgt_ix": "196-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_51",
            "tgt_ix": "196-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_52",
            "tgt_ix": "196-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_53",
            "tgt_ix": "196-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_54",
            "tgt_ix": "196-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_55",
            "tgt_ix": "196-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_56",
            "tgt_ix": "196-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_57",
            "tgt_ix": "196-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_58",
            "tgt_ix": "196-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_59",
            "tgt_ix": "196-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_60",
            "tgt_ix": "196-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_61",
            "tgt_ix": "196-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_62",
            "tgt_ix": "196-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_63",
            "tgt_ix": "196-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_64",
            "tgt_ix": "196-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_65",
            "tgt_ix": "196-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_66",
            "tgt_ix": "196-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_67",
            "tgt_ix": "196-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_68",
            "tgt_ix": "196-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "196-ARR_v2_69",
            "tgt_ix": "196-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 481,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "196-ARR",
        "version": 2
    }
}