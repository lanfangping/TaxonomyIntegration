{
    "nodes": [
        {
            "ix": "21-ARR_v2_0",
            "content": "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_2",
            "content": "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models. 1 * Equal contribution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "21-ARR_v2_4",
            "content": "With the prevalence of large neural networks with millions or billions of parameters, model compression is gaining prominence for facilitating efficient, eco-friendly deployment for machine learning applications. Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015b) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015b;Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Ahn et al., 2019;Park et al., 2019;Passalis & Tefas, 2018;Heo et al., 2019;Kim et al., 2018;Shi et al., 2021;Sanh et al., 2019;Jiao et al., 2019;Wang et al., 2020b). Previous works often train a large model as the \"teacher\"; then they fix the teacher and train a \"student\" model to mimic the behavior of the teacher, in order to transfer the knowledge from the teacher to the student.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_5",
            "content": "However, this paradigm has the following drawbacks: (1) The teacher is unaware of the student's capacity. Recent studies in pedagogy suggest student-centered learning, which considers students' characteristics and learning capability, has shown effectiveness improving students' performance (Cornelius-White, 2007;Wright, 2011). However, in conventional knowledge distillation, the student passively accepts knowledge from the teacher, without regard for the student model's learning capability and performance. Recent works (Park et al., 2021;Shi et al., 2021) introduce student-aware distillation by jointly training the teacher and the student with task-specific objectives. However, there is still space for improvement since: (2) The teacher is not optimized for distillation. In previous works, the teacher is often trained to optimize its own inference performance. However, the teacher is not aware of the need to transfer its knowledge to a student and thus usually does so suboptimally. A real-world analogy is that a PhD student may have enough knowledge to solve problems themselves, but requires additional teaching training to qualify as a professor.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_6",
            "content": "To address these two drawbacks, we propose Knowledge Distillation with Meta Learning (MetaDistil), a new teacher-student distillation framework using meta learning (Finn et al., 2017) to exploit feedback about the student's learning progress to improve the teacher's knowledge transfer ability throughout the distillation process. On the basis of previous formulations of bi-level optimization based meta learning (Finn et al., 2017), we propose a new mechanism called pilot update that aligns the learning of the bi-level learners (i.e., the teacher and the student). We illustrate the workflow of MetaDistil in Figure 1. The teacher in MetaDistil is trainable, which enables the teacher to adjust to its student network and also improves its L CE < l a t e x i t s h a 1 _ b a s e 6 4 = \" c W z Y M h 3 A e R J k O + v s c t A Q M H 7 i 1 8 8 = \" > A A A C A n i c b V B N S 8 N A E N 3 U r 1 q / o p 7 E S 7 A I n k p S B T 0 W i + D B Q w X b C m 0 I m + 2 2 X b r Z h N 2 J U E L w 4 l / x 4 k E R r / 4 K b / 4 b N 2 k O 2 v p g 4 e 1 7 M 8 z M 8 y P O F N j 2 t 1 F a W l 5 Z X S u v V z Y 2 t 7 Z 3 z N 2 9 j g p j S W i b h D y U 9 z 5 W l D N B 2 8 C A 0 / t I U h z 4 n H b 9 S T P z u w 9 U K h a K O 5 h G 1 A 3 w S L A h I x i 0 5 J k H / Q D D m G C e 3 K R e k n 8 Y J M 2 r N P X M q l 2 z c 1 Pilot update L 0 KD < l a t e x i t s h a 1 _ b a s e 6 4 = \" u z i w H V w 4 q q K f w s W y y 8 k 1 We call this process a \"teaching experiment.\" In this way, we can obtain an experimental student S that can be quizzed. Then, we sample from the quiz set, and calculate the loss of S on these samples. We use this loss as a feedback signal to meta-update the teacher by calculating second derivatives and performing gradient descent (Finn et al., 2017). Finally, we discard the experimental subject S and use the updated teacher to distill into the student S on the same training batches. The use of meta learning allows the teacher model to receive feedback from the student in a completely differentiable way. We provide a simple and intuitive approach to explicitly optimize the teacher using the student's quiz performance as a proxy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_7",
            "content": "i L x C l I F R V o e e Z X f x C S O K A C C M d K 9 R w 7 A j f B E h j h N K 3 0 Y 0 U j T C Z 4 R H u a C h x Q 5 S b 5 C a l 1 r J W B N Q y l f g K s X P 3 d k e B A q W n g 6 8 p s R z X v Z e J / X i + G 4 Y W b M B H F Q A W Z D R r G 3 I L Q y v K w B k x S A n y q C S a S 6 V 0 t M s Y S E 9 C p V X Q I z v z J i 6 R T r z m n t f r t W b V x W c R R R o f o C J 0 g B 5 2 j B r p G L d R G B D 2 i Z / S K 3 o w n 4 8 V 4 N z 5 m p S W j 6 N l H f 2 B 8 / g A T 8 J f d < / l a t e x i t > T S",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_8",
            "content": "x 7 M u D t 4 = \" > A A A C A 3 i c b V D L S s N A F L 3 x W e s r 6 k 4 3 w S K 6 K k k V d F n Q h a C L C v Y B b Q i T 6 b Q d O p m E m Y l Q Q s C N v + L G h S J u / Q l 3 / o 2 T N A t t P T B w 5 p x 7 u f c e P 2 J U K t v + N h Y W l 5 Z X V k t r 5 f W N z a 1 t c 2 e 3 J c N Y Y N L E I Q t F x 0 e S M M p J U 1 H F S C c S B A U + I 2 1 / f J n 5 7 Q c i J A 3 5 v Z p E x A 3 Q k N M B x U h p y T P 3 e w F S I 4 x Y c n u c e k n + o y q 5 u U p T z 6 z Y V T u H N U + c g l S g Q M M z v 3 r 9 E M c B 4 Q o z J G X X s S P l J k g o i h l J y 7 1 Y k g j h M R q S r q Y c B U S 6 S X 5 D a h 1 p p W 8 N Q q E f V 1 a u / u 5 I U C D l J P B 1 Z b a j n P U y 8 T + v G 6 v B h Z t Q H s W K c D w d N I i Z p U I r C 8 T q U 0 G w Y h N N E B Z U 7 2 r h E R I I K x 1 b W Y f g z J 4 8 T 1 q 1 q n N a r d 2 d V e q 1 I o 4 S H M A h n I A D 5 1 C H a 2 h A E z A 8 w j O 8 w p v x Z L w Y 7 8 b H t H T B K H r 2 4 A + M z x + A Z 5 g F < / l a t e x i t > L 0 KD < l a t e x i t s h a 1 _ b a s e 6 4 = \" u z i w H V w 4 q q K f w s W y y 8 k 1 x 7 M u D t 4 = \" > A A A C A 3 i c b V D L S s N A F L 3 x W e s r 6 k 4 3 w S K 6 K k k V d F n Q h a C L C v Y B b Q i T 6 b Q d O p m E m Y l Q Q s C N v + L G h S J u / Q l 3 / o 2 T N A t t P T B w 5 p x 7 u f c e P 2 J U K t v + N h Y W l 5 Z X V k t r 5 f W N z a 1 t c 2 e 3 J c N Y Y N L E I Q t F x 0 e S M M p J U 1 H F S C c S B A U + I 2 1 / f J n 5 7 Q c i J A 3 5 v Z p E x A 3 Q k N M B x U h p y T P 3 e w F S I 4 x Y c n u c e k n + o y q 5 u U p T z 6 z Y V T u H N U + c g l S g Q M M z v 3 r 9 E M c B 4 Q o z J G X X s S P l J k g o i h l J y 7 1 Y k g j h M R q S r q Y c B U S 6 S X 5 D a h 1 p p W 8 N Q q E f V 1 a u / u 5 I U C D l J P B 1 Z b a j n P U y 8 T + v G 6 v B h Z t Q H s W K c D w d N I i Z p U I r C 8 T q U 0 G w Y h N N E B Z U 7 2 r h E R I I K x 1 b W Y f g z J 4 8 T 1 q 1 q n N a r d 2 d V e q 1 I o 4 S H M A h n I A D 5 1 C H a 2 h A E z A 8 w j O 8 w p v x Z L w Y 7 8 b H t H T B K H r 2 4 A + M z x + A Z 5 g F < / l a t e x i t > L KD < l a t e x i t s h a 1 _ b a s e 6 4 = \" O k / / 7 Q 4 U X i 0 + y v I X 1 p X 7 v f y q M Z s = \" > A A A C A n i c b V D L S s N A F J 3 U V 6 2 v q C t x E y y C q 5 J U Q Z c F X Q i 6 q G A f 0 I Y w m U 7 a o Z N J m L k R S g h u / B U 3 L h R x 6 1 e 4 8 2 + c t F l o 6 4 G B M + f c y 7 3 3 + D F n C m z 7 2 y g t L a + s r p X X K x u b W 9 s 7 5 u 5 e W 0 W J J L R F I h 7 J r o 8 V 5 U z Q F j D g t B t L i k O f 0 4 4 / v s z 9 z g O V i k X i H i Y x d U M 8 F C x g B I O W P P O g H 2 I Y E c z T 2 8 x L p x 8 G 6 c 1 V l n l m 1 a 7 Z U 1 i L x C l I F R V o e u Z X f x C R J K Q C C M d K 9 R w 7 B j f F E h j h N K v 0 E 0 V j T M Z 4 S H u a C h x S 5 a b T E z L r W C s D K 4 i k f g K s q f q 7 I 8 W h U p P Q 1 5 X 5 j m r e y 8 X / v F 4 C w Y W b M h E n Q A W Z D Q o S b k F k 5 X l Y A y Y p A T 7 R B B P J 9 K 4 W G W G J C e j U K j o E Z / 7 k R d K u 1 5 z T W v 3 u r N q o F 3 G U 0 S E 6 Q i f I Q e e o g a 5 R E 7 U Q Q Y / o G b 2 i N + P J e D H e j Y 9 Z a c k o e v b R H x i f P x n S l 9 Q = < / l a t",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_9",
            "content": "To test the effectiveness of MetaDistil, we conduct extensive experiments on text and image classification tasks. MetaDistil outperforms knowledge distillation by a large margin, verifying the effectiveness and versatility of our method. Also, our method achieves state-of-the-art performance compressing BERT (Devlin et al., 2019) on the GLUE benchmark and shows competitive results compressing ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) on CIFAR-100 (Krizhevsky et al., 2009). Additionally, we design experiments to analyze and explain the improvement. Ablation studies show the effectiveness of our proposed pilot update and dynamic distillation. Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably because of its ability to adjust the parameters of the teacher model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "21-ARR_v2_11",
            "content": "Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., , 2021bXu & McAuley, 2022). Knowledge distillation is a prominent method for training compact networks to achieve comparable performance to a deep network. Hinton et al. (2015b) first introduced the idea of knowledge distillation to exploit the \"dark knowledge\" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model. Since its introduction, several works (Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Park et al., 2019;Sun et al., 2019;Jiao et al., 2019) have investigated methods that align different latent representations between the student and teacher models for better knowledge transfer. In the context of knowledge distillation, MetaDistil shares some common ideas with the line of work that utilizes a sequence of intermediate teacher models to make the teacher network better adapt to the capacity of the student model throughout the training process, including teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020) and route constraint optimization (RCO) . However, the intermediate teachers are heuristically selected independently of the training process and the evolution of the teacher network is discrete. In contrast, MetaDistil employs meta learning to make the teacher model adapt to the current state of the student model and provide a continuously evolving meta-teacher that can better teach the student. Concurrently, Park et al. (2021) and Shi et al. (2021) propose to update the teacher model jointly with the student model with task specific objectives (e.g., cross-entropy loss) during the KD process and add constraints to keep student and teacher similar to each other. Their approaches makes the teacher model aware of the student model by constraining the teacher model's capacity. However, the teacher models in their methods are still not optimized for knowledge transfer. In addition, Zhang et al. (2018) introduced deep mutual learning where multiple models learn collaboratively and teach each other throughout the training process. While it is focused on a different setting where different models have approximately the same capacity and are learned from scratch, it also encourages the teacher model to behave similarly to the student model. Different from all aforementioned methods, MetaDistil employs meta learning to explicitly optimize the teacher model for better knowledge transfer ability, and leads to improved performance of the resulting student model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_12",
            "content": "The core idea of meta learning is \"learning to learn,\" which means taking the optimization process of a learning algorithm into consideration when optimizing the learning algorithm itself. Meta learning typically involves a bi-level optimization process where the inner-learner provides feedback for optimization of the meta-learner. Successful applications of meta learning include learning better initialization (Finn et al., 2017), architecture search , learning to optimize the learning rate schedule (Baydin et al., 2018), and learning to optimize (Andrychowicz et al., 2016). These works typically aim to obtain an optimized meta-learner (i.e., the teacher model in MetaDistil), while the optimization of the inner-learner (i.e., the student model in MetaDistil), is mainly used to provide learning signal for the meta optimization process. This is different from the objective of knowledge distillation where an optimized student model is the goal. Recently, there have been a few works investigating using this bi-level optimization framework to obtain a better inner-learner. For example, meta pseudo labels (Pham et al., 2020) use meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model. These methods adapt the same bi-level optimization process as previous works where the goal is to obtain an optimized meta-learner. In these approaches, during each iteration, the meta-learner is optimized for the original inner-learner and then applied to the updated inner-learner in the next iteration. This leads to a mismatch between the meta-learner and the inner-learner, and is therefore suboptimal for learning a good inner-learner. In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problems, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_13",
            "content": "Meta Knowledge Distillation Recently, some works on KD take a meta approach. Pan et al. (2020) proposed a framework to train a metateacher across domains that can better fit new domains with meta-learning. Then, traditional KD is performed to transfer the knowledge from the meta-teacher to the student. Liu et al. (2020) proposed a self-distillation network which utilizes meta-learning to train a label-generator as a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers. Different from the above, MetaDistil is a general knowledge distillation method that exploits meta-learning to allow the teacher to learn to teach dynamically. Instead of merely training a meta-teacher, our method uses meta-learning throughout the procedure of knowledge transfer, making the teacher model compatible for the student model for every training example during each training stage.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_14",
            "content": "Knowledge Distillation with Meta Learning",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "21-ARR_v2_15",
            "content": "An overview of MetaDistil is presented in Figure 1. MetaDistil includes two major components. First, the meta update enables the teacher model to receive the student model's feedback on the distillation process, allowing the teacher model to \"learn to teach\" and provide distillation signals that are more suitable for the student model's current capacity. The pilot update mechanism ensures a finergrained match between the student model and the meta-updated teacher model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_16",
            "content": "Background",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "21-ARR_v2_17",
            "content": "Knowledge Distillation",
            "ntype": "title",
            "meta": {
                "section": "3.1.1"
            }
        },
        {
            "ix": "21-ARR_v2_18",
            "content": "Knowledge distillation algorithms aim to exploit the hidden knowledge from a large teacher network, denoted as T , to guide the training of a shallow student network, denoted as S. To help transfer the knowledge from the teacher to the student, apart from the original task-specific objective (e.g., crossentropy loss), a knowledge distillation objective which aligns the behavior of the student and the teacher is included to train the student network. Formally, given a labeled dataset D of N samples D = {(x 1 , y 1 ) , . . . , (x N , y N )}, we can write the loss function of the student network as follows,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_19",
            "content": "L S (D; \u03b8 S ; \u03b8 T ) = 1 N N i=1 [\u03b1L T (y i , S (x i ; \u03b8 S )) + (1 \u2212 \u03b1) L KD (T (x i ; \u03b8 T ) , S (x i ; \u03b8 S ))] (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_20",
            "content": "where \u03b1 is a hyper-parameter to control the relative importance of the two terms; \u03b8 T and \u03b8 S are the parameters of the teacher T and student S, respectively. L T refers to the task-specific loss and L KD refers to the knowledge distillation loss which measures the similarity of the student and the teacher. Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error (MSE) between student and teacher logits, the similarity between the student and the teacher's attention distribution, etc. We do not specify the detailed form of the loss function because MetaDistil is a general framework that can be easily applied to various kinds of KD objectives as long as the objective is differentiable with respect to the teacher parameters. In the experiments of this paper, we use mean squared error between the hidden states of the teacher and the student for both our method and the KD baseline since recent study finds that it is more stable and slightly outperforms than KL divergence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_21",
            "content": "Meta Learning",
            "ntype": "title",
            "meta": {
                "section": "3.1.2"
            }
        },
        {
            "ix": "21-ARR_v2_22",
            "content": "In meta learning algorithms that involve a bi-level optimization problem (Finn et al., 2017), there exists an inner-learner f i and a meta-learner f m . The inner-learner is trained to accomplish a task T or a distribution of tasks with help from the metalearner. The training process of f i on T with the help of f m is typically called inner-loop, and we can denote f i (f m ) as the updated inner-learner after the inner-loop. We can express f i as a function of f m because learning f i depends on f m . In return, the meta-learner is optimized with a meta objective, which is generally the maximization of expected performance of the inner-learner after the innerloop, i.e., f i (f m ). This learning process is called a meta-loop and is often accomplished by gradient descent with derivatives of L(f i (f m )), the loss of updated inner-leaner on some held-out support set (i.e., the quiz set in our paper).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_23",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "21-ARR_v2_24",
            "content": "Pilot Update",
            "ntype": "title",
            "meta": {
                "section": "3.2.1"
            }
        },
        {
            "ix": "21-ARR_v2_25",
            "content": "In the original formulation of meta learning (Finn et al., 2017), the purpose is to learn a good metalearner f m that can generalize to different innerlearners f i for different tasks. In their approach, the meta-learner is optimized for the \"original\" innerlearner at the beginning of each iteration and the current batch of training data. The updated metalearner is then applied to the updated inner-learner and a different batch of data in the next iteration. This behavior is reasonable if the purpose is to optimize the meta-learner. However, in MetaDistil, we only care about the performance of the only innerlearner, i.e., the student. In this case, this behavior leads to a mismatch between the meta-learner and the inner-learner, and is therefore suboptimal for learning a good inner-learner. Therefore, we need a way to align and synchronize the learning of the meta-and inner-learner, in order to allow an update step of the meta-learner to have an instant effect on the inner-learner. This instant reflection prevents the meta-learner from catastrophic forgetting (McCloskey & Cohen, 1989). To achieve this, we design a pilot update mechanism. For a batch of training data x, we first make a temporary copy of the inner-learner f i and update both the copy f i and the meta learner f m on x. Then, we discard f i and update f i again with the updated f m on the same data x. This mechanism can apply the impact of data x to both f m and f i at the same time, thus aligns the training process. Pilot update is a general technique that can potentially be applied to any meta learning application that optimizes the inner-learner performance. We will describe how we apply this mechanism to MetaDistil shortly and empirically verify the effectiveness of pilot update in Section 4.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_26",
            "content": "Learning to Teach",
            "ntype": "title",
            "meta": {
                "section": "3.2.2"
            }
        },
        {
            "ix": "21-ARR_v2_27",
            "content": "In MetaDistil, we would like to optimize the teacher model, which is fixed in traditional KD frameworks. Different from previous deep mutual learning (Zhang et al., 2018) methods that switch the role between the student and teacher",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_28",
            "content": "Algorithm 1 Knowledge Distillation with Meta Learning (MetaDistil)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_29",
            "content": "Require: student \u03b8S, teacher \u03b8T , train set D, quiz set Q Require: \u03bb, \u00b5: learning rate for the student and the teacher 1: while not done do 2: Sample batch of training data x \u223c D 3:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_30",
            "content": "Copy student parameter \u03b8S to student \u03b8 S 4:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_31",
            "content": "Update \u03b8 S with x and \u03b8T : \u03b8 S \u2190 \u03b8 S \u2212 \u03bb\u2207 \u03b8 S LS(x; \u03b8S; \u03b8T ) 5:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_32",
            "content": "Sample a batch of quiz data q \u223c Q 6:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_33",
            "content": "Update \u03b8T with q and \u03b8 S : \u03b8T \u2190 \u03b8T \u2212 \u00b5\u2207 \u03b8 T LT (q, \u03b8 S (\u03b8T )) 7:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_34",
            "content": "Update original \u03b8S with x and the updated \u03b8T : \u03b8S \u2190 \u03b8S \u2212 \u03bb\u2207 \u03b8 S LS(x; \u03b8S; \u03b8T ) 8: end while network and train the original teacher model with soft labels generated by the student model, or recent works (Shi et al., 2021;Park et al., 2021) that update the teacher model with a task-specific loss during the KD process, MetaDistil explicitly optimizes the teacher model in a \"learning to teach\" fashion, so that it can better transfer its knowledge to the student model. Concretely, the optimization objective of the teacher model in the MetaDistil framework is the performance of the student model after distilling from the teacher model. This \"learning to teach\" paradigm naturally fits the bi-level optimization framework in meta learning literature.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_35",
            "content": "In the MetaDistil framework, the student network \u03b8 S is the inner-learner and the teacher network \u03b8 T is the meta-learner. For each training step, we first copy the student model \u03b8 S to an \"experimental student\" \u03b8 S . Then given a batch of training examples x and the learning rate \u03bb, the experimental student is updated in the same way as conventional KD algorithms:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_36",
            "content": "\u03b8 S (\u03b8 T ) = \u03b8 S \u2212 \u03bb\u2207 \u03b8 S L S (x; \u03b8 S ; \u03b8 T ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_37",
            "content": "(2)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_38",
            "content": "To simplify notation, we will consider one gradient update for the rest of this section, but using multiple gradient updates is a straightforward extension. We observe that the updated experimental student parameter \u03b8 S , as well as the student quiz loss l q = L T (q, \u03b8 S (\u03b8 T )) on a batch of quiz samples q sampled from a held-out quiz set Q, is a function of the teacher parameter \u03b8 T . Therefore, we can optimize l q with respect to \u03b8 T by a learning rate \u00b5:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_39",
            "content": "\u03b8 T \u2190 \u03b8 T \u2212 \u00b5\u2207 \u03b8 T L T q, \u03b8 S (\u03b8 T )(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_40",
            "content": "We evaluate the performance of the experimental student on a separate quiz set to prevent overfitting the validation set, which is preserved for model selection. Note that the student is never trained on the quiz set and the teacher only performs meta-update on the quiz set instead of fitting it. We do not use a dynamic quiz set strategy because otherwise the student would have been trained on the quiz set and the loss would not be informative. After meta-updating the teacher model, we then update the \"real\" student model in the same way as described in Equation 2. Intuitively, optimizing the teacher network \u03b8 T with Equation 3 is maximizing the expected performance of the student network after being taught by the teacher with the KD objective in the inner-loop. This meta-objective allows the teacher model to adjust its parameters to better transfer its knowledge to the student model. We apply the pilot update strategy described in Section 3.2.1 to better align the learning of the teacher and student, as shown in Algorithm 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_41",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "21-ARR_v2_42",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "21-ARR_v2_43",
            "content": "We evaluate MetaDistil on two commonly used classification benchmarks for knowledge distillation in both Natural Language Processing and Computer Vision (see Appendix A).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_44",
            "content": "Settings For NLP, we evaluate our proposed approach on the GLUE benchmark . Specifically, we test on MRPC (Dolan & Brockett, 2005), QQP and STS-B (Conneau & Kiela, 2018) for Paraphrase Similarity Matching; SST-2 (Socher et al., 2013) for Sentiment Classification; MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016) and RTE for the Natural Language Inference; CoLA (Warstadt et al., 2019) for Linguistic Acceptability. Following previous studies (Sun et al., 2019;Jiao et al., 2019; we report the results on MNLI-m and MNLI-mm, respectively. For MRPC and QQP, we report both F1 and accuracy. For STS-B, we report Pearson and Spearman correlation. The metric for CoLA is Matthew's correlation. The other tasks use accuracy as the metric.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_45",
            "content": "Following previous works (Sun et al., 2019;Turc et al., 2019;, we evaluate MetaDistil in a task-specific setting where the teacher model is fine-tuned on a downstream task and the student model is trained on the task with the KD loss. We do not choose the pretraining distillation setting since it requires significant computational resources. We implement MetaDistil based on Hugging Face Transformers (Wolf et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_46",
            "content": "Baselines For comparison, we report the results of vanilla KD and patient knowledge distillation (Sun et al., 2019). We also include the results of progressive module replacing , a state-of-the-art task-specific compression method for BERT which also uses a larger teacher model to improve smaller ones like knowledge distillation. In addition, according to Turc et al. (2019), the reported performance of current taskspecific BERT compression methods is underestimated because the student model is not appropriately initialized. To ensure fair comparison, we re-run task-specific baselines with student models initialized by a pretrained 6-layer BERT model and report our results in addition to the official numbers in the original papers. We also compare against deep mutual learning (DML) (Zhang et al., 2018), teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020), route constraint optimization (RCO) , proximal knowledge teaching (ProKT) (Shi et al., 2021), and student-friendly teacher network (SFTN) (Park et al., 2021), where the teacher network is not fixed. For reference, we also present results of pretraining distilled models including DistilBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2019), MiniLM v1 and v2 (Wang et al., 2020b,a). Note that among these baselines, PKD (Sun et al., 2019) and Theseus exploit intermediate features while TinyBERT and the MiniLM family use both intermediate and Transformer-specific features. In contrast, MetaDistil uses none of these but the vanilla KD loss (Equation 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_47",
            "content": "Training Details For training hyperparameters, we fix the maximum sequence length to 128 and the temperature to 2 for all tasks. For our method and all baselines (except those with officially reported numbers), we perform grid search over the sets of the student learning rate \u03bb from {1e-5, 2e-5, 3e-5}, the teacher learning rate \u00b5 from {2e-6, 5e-6, 1e-5}, the batch size from {32, 64}, the weight of KD loss from {0.4, 0.5, 0.6}. We randomly split the original training set to a new training set and the quiz set by 9 : 1. For RCO, we select four unconverged teacher checkpoints as the intermediate training targets. For TAKD, we use KD to train a teacher assistant model with 10 Transformer layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_48",
            "content": "Experimental Results",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "21-ARR_v2_49",
            "content": "We report the experimental results on both the development set and test set of the eight GLUE tasks in Table 1. MetaDistil achieves state-of-the-art performance under the task-specific setting and outperforms all KD baselines. Notably, without using any intermediate or model-specific features in the loss function, MetaDistil outperforms methods with carefully designed features, e.g., PKD and TinyBERT (without data augmentation). Compared with other methods with a trainable teacher (Zhang et al., 2018;Mirzadeh et al., 2020;Shi et al., 2021), our method still demonstrates superior performance. As we analyze, with the help of meta learning, MetaDistil is able to directly optimize the teacher's teaching ability thus yielding a further improvement in terms of student accuracy. Also, we observe a performance drop by replacing pilot update with a normal update. This ablation study verifies the effectiveness of our proposed pilot update mechanism. Moreover, MetaDistil achieves very competitive results on image classification as well, as described in Section A.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_50",
            "content": "Analysis",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "21-ARR_v2_51",
            "content": "Why Does MetaDistil Work?",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "21-ARR_v2_52",
            "content": "We investigate the effect of meta-update for each iteration. We inspect (1) the validation loss of S after the teaching experiment and that of S after the real distillation update, and (2) the KD loss, which describes the discrepancy between student and teacher, before and after the teacher update.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_53",
            "content": "We find that for 87% of updates, the student model's validation loss after real update (Line 7 in Algorithm 1) is smaller than that after the teaching experiment (Line 4 in Algorithm 1), which would be the update to the student S in the variant without pilot update. This confirms the effectiveness of the pilot update mechanism on better matching the student and teacher model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_54",
            "content": "Moreover, we find that in 91% of the first half of the updates, the teacher becomes more similar (in terms of logits distributions) to the student after the meta-update, which indicates that the teacher is learning to adapt to a low-performance student (like an elementary school teacher). However, in the second half of MetaDistil, this percentage drops to 63%. We suspect this is because in the later training stages, the teacher needs to actively evolve itself beyond the student to guide the student towards further improvement (like a university professor).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_55",
            "content": "Finally, we try to apply a meta-learned teacher to a conventional static distillation and also to an unfamiliar student. We describe the results in details in Section A.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_56",
            "content": "Hyper-parameter Sensitivity",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "21-ARR_v2_57",
            "content": "A motivation of MetaDistil is to enable the teacher to dynamically adjust its knowledge transfer in an optimal way. Similar to Adam (Kingma & Ba, 2015) vs. SGD (Sinha & Griscik, 1971;Kiefer et al., 1952) for optimization, with the ability of dynamic adjusting, it is natural to expect MetaDistil to be more insensitive and robust to changes of the settings. Here, we evaluate the performance of MetaDistil with students of various capability, and a wide variety of hyperparameters, including loss weight and temperature.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_58",
            "content": "Student Capability To investigate the performance of MetaDistil under different student capacity, we experiment to distill BERT-Base into BERT-6L, Medium, Small, Mini and Tiny (Turc et al., 2019) with conventional KD and MetaDistil. We plot the performance with the student's parameter number in Figure 2. Additionally, we show results for different compression ratio in Appendix B.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_59",
            "content": "Loss Weight In KD, tuning the loss weight is nontrivial and often requires hyperparameter search.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_60",
            "content": "To test the robustness of MetaDistil under different loss weights, we run experiments with different \u03b1 (Equation 1). As shown in Figure 3, MetaDistil consistently outperforms conventional KD and is less sensitive to different \u03b1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_61",
            "content": "Temperature Temperature is a re-scaling trick introduced in Hinton et al. (2015b). We try different temperatures and illustrate the performance of KD and MetaDistil in Figure 4. MetaDistil shows better performance and robustness compared to KD.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_62",
            "content": "Limitation",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "21-ARR_v2_63",
            "content": "Like all meta learning algorithms, MetaDistil inevitably requires two rounds of updates involving both first and second order derivatives. Thus, MetaDistil requires additional computational time and memory than a normal KD method, which can be a limitation of our method. We compare the computational overheads of MetaDistil with other methods in Table 2. Although our approach takes more time to achieve its own peak performance, it can match up the performance of PKD (Sun et al., 2019) with a similar time cost. The memory use of our method is higher than PKD and ProKT (Shi et al., 2021). However, this one-off investment can lead to a better student model for inference, thus can be worthy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_64",
            "content": "Discussion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "21-ARR_v2_65",
            "content": "In this paper, we present MetaDistil, a knowledge distillation algorithm powered by meta learning that explicitly optimizes the teacher network to better transfer its knowledge to the student network.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_66",
            "content": "The extensive experiments verify the effectiveness and robustness of MetaDistil.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_67",
            "content": "MetaDistil focuses on improving the performance of knowledge distillation and does not introduce extra ethical concerns compared to vanilla KD methods. Nevertheless, we would like to point out that as suggested by Hooker et al. (2020), model compression may lead to biases. However, this is not an outstanding problem of our method but a common risk in model compression, which needs to be addressed in the future.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_68",
            "content": "In MetaDistil, the student is trained in a dynamic manner. To investigate the effect of such a dynamic distillation process, we attempt to use the teacher at the end of MetaDistil training to perform a static conventional KD, to verify the effectiveness of our dynamic distillation strategy. As shown in Table 4, on both experiments, dynamic MetaDistil outperforms conventional KD and static distillation with the teacher at the end of MetaDistil training. As mentioned in Section 3.2, a meta teacher is optimized to transfer its knowledge to a specific student network. To justify this motivation, we conduct experiments using a teacher optimized for the ResNet-32 student to statically distill to the ResNet-20 student, and also in reverse. As shown in Table 4, the cross-taught students underperform the static students taught by their own teachers by 0.27 and 0.12 for ResNet-32 and ResNet-20, respectively. This confirms our motivation that the meta teacher in MetaDistil can adjust itself according to its student.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_69",
            "content": "In this section, we present additional experimental results in settings with different compression ratios to further demonstrate the effectiveness of MetaDistil on bridging the gap between the student and teacher capacity. Specifically, we conduct experiments in the following two settings: (1) distilling BERT-base into a 4-layer BERT (110M\u219252M) and (2) distilling BERT-large into a 6-layer BERT (345M\u219266M). The results are shown in Table 4 and Table 5, respectively. We can see that MetaDistil consistently outperforms PKD and ProKT in both settings. This confirms the effectiveness of MetaDistil and also show its ability to adapt the teacher model to the student model, since the gap between teacher and student is even larger in these settings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_70",
            "content": "We also investigate why MetaDistil works by conducting experiments on the development sets of MNLI, SST, and MRPC, which are important tasks in GLUE that have a large, medium, and small training set, respectively. We illustrate the validation accuracy curves of the meta teacher and student models with training steps in Figure 5, and compare them to the student performance in conventional KD. We can see that the meta teacher maintains high accuracy in the first 5,000 steps and then begins to slowly degrade. Starting from step 8,000, the teacher model underperforms the student while the student's accuracy keeps increasing. This verifies our assumption that a model with the best accuracy is not necessarily the optimal teacher. Also, MetaDistil is not naively optimizing the teacher's accuracy but its \"teaching skills.\" This phenomenon suggests that beyond high accuracy, there could be more important properties of a good teacher that warrant further investigation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_71",
            "content": "While MetaDistil achieves improved student accuracy on the GLUE benchmark, it is still not very clear where the performance improvement comes from. There are two possibilities: (1) the student better mimics the teacher, and (2) the changes of teacher helps student perform better on hard examples that would be incorrectly classified by the student with vanilla KD. We conduct a series of analysis on the MRPC dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_72",
            "content": "For the first assumption, we compute the prediction loyalty (Xu et al., 2021a) of the student model distilled with PKD and MetaDistil, respectively. For MetaDistil, we measure the loyalty with respect to both the original teacher and the final teacher. We find that there is no significant difference between between PKD and MetaDistil. This suggests that the improvement does not come from student better mimicking the teacher.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_73",
            "content": "For the second assumption, we first identify the examples in the quiz set for which our model gives correct predictions while the student distilled by PKD makes a wrong prediction. We then compute",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "21-ARR_v2_74",
            "content": "Sungsoo Ahn, Shell Hu, Andreas Damianou, Neil Lawrence, Zhenwen Dai, Variational information distillation for knowledge transfer, 2019, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Sungsoo Ahn",
                    "Shell Hu",
                    "Andreas Damianou",
                    "Neil Lawrence",
                    "Zhenwen Dai"
                ],
                "title": "Variational information distillation for knowledge transfer",
                "pub_date": "2019",
                "pub_title": "CVPR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_75",
            "content": "Marcin Andrychowicz, Misha Denil, Sergio Colmenarejo, Matthew Hoffman, David Pfau, Tom Schaul, Nando De Freitas, Learning to learn by gradient descent by gradient descent, 2016, NeurIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Marcin Andrychowicz",
                    "Misha Denil",
                    "Sergio Colmenarejo",
                    "Matthew Hoffman",
                    "David Pfau",
                    "Tom Schaul",
                    "Nando De Freitas"
                ],
                "title": "Learning to learn by gradient descent by gradient descent",
                "pub_date": "2016",
                "pub_title": "NeurIPS",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_76",
            "content": "Robert Atilim Gunes Baydin, David Cornish, Mark Mart\u00ednez-Rubio, Frank Schmidt,  Wood, Online learning rate adaptation with hypergradient descent, 2018, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Robert Atilim Gunes Baydin",
                    "David Cornish",
                    "Mark Mart\u00ednez-Rubio",
                    "Frank Schmidt",
                    " Wood"
                ],
                "title": "Online learning rate adaptation with hypergradient descent",
                "pub_date": "2018",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_77",
            "content": "Alexis Conneau, Douwe Kiela, Senteval: An evaluation toolkit for universal sentence representations, 2018, LREC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Alexis Conneau",
                    "Douwe Kiela"
                ],
                "title": "Senteval: An evaluation toolkit for universal sentence representations",
                "pub_date": "2018",
                "pub_title": "LREC",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_78",
            "content": "Jeffrey Cornelius-White, Learner-centered teacherstudent relationships are effective: A meta-analysis, 2007, Review of educational research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Jeffrey Cornelius-White"
                ],
                "title": "Learner-centered teacherstudent relationships are effective: A meta-analysis",
                "pub_date": "2007",
                "pub_title": "Review of educational research",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_79",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: pre-training of deep bidirectional transformers for language understanding, 2019, NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_80",
            "content": "B William, Chris Dolan,  Brockett, Automatically constructing a corpus of sentential paraphrases, 2005, IWP@IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "B William",
                    "Chris Dolan",
                    " Brockett"
                ],
                "title": "Automatically constructing a corpus of sentential paraphrases",
                "pub_date": "2005",
                "pub_title": "IWP@IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_81",
            "content": "UNKNOWN, None, 2017, Model-agnostic meta-learning for fast adaptation of deep networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Model-agnostic meta-learning for fast adaptation of deep networks",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_82",
            "content": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep residual learning for image recognition, 2016, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Kaiming He",
                    "Xiangyu Zhang",
                    "Shaoqing Ren",
                    "Jian Sun"
                ],
                "title": "Deep residual learning for image recognition",
                "pub_date": "2016",
                "pub_title": "CVPR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_83",
            "content": "Byeongho Heo, Minsik Lee, Sangdoo Yun, Jin Choi, Knowledge transfer via distillation of activation boundaries formed by hidden neurons, 2019, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Byeongho Heo",
                    "Minsik Lee",
                    "Sangdoo Yun",
                    "Jin Choi"
                ],
                "title": "Knowledge transfer via distillation of activation boundaries formed by hidden neurons",
                "pub_date": "2019",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_84",
            "content": "UNKNOWN, None, 2015, Distilling the knowledge in a neural network, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Distilling the knowledge in a neural network",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_85",
            "content": "UNKNOWN, None, 2015, Distilling the knowledge in a neural network, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Distilling the knowledge in a neural network",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_86",
            "content": "UNKNOWN, None, 2010, Characterising bias in compressed models. CoRR, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2010",
                "pub_title": "Characterising bias in compressed models. CoRR, abs",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_87",
            "content": "UNKNOWN, None, 2019, Distilling bert for natural language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Distilling bert for natural language understanding",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_88",
            "content": "Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, Xiaolin Hu, Knowledge distillation via route constrained optimization, 2019, ICCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Xiao Jin",
                    "Baoyun Peng",
                    "Yichao Wu",
                    "Yu Liu",
                    "Jiaheng Liu",
                    "Ding Liang",
                    "Junjie Yan",
                    "Xiaolin Hu"
                ],
                "title": "Knowledge distillation via route constrained optimization",
                "pub_date": "2019",
                "pub_title": "ICCV",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_89",
            "content": "Jack Kiefer, Jacob Wolfowitz, Stochastic estimation of the maximum of a regression function, 1952, The Annals of Mathematical Statistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Jack Kiefer",
                    "Jacob Wolfowitz"
                ],
                "title": "Stochastic estimation of the maximum of a regression function",
                "pub_date": "1952",
                "pub_title": "The Annals of Mathematical Statistics",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_90",
            "content": "UNKNOWN, None, 2018, Paraphrasing complex network: Network compression via factor transfer, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Paraphrasing complex network: Network compression via factor transfer",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_91",
            "content": "Taehyeon Kim, Jaehoon Oh, Nakyil Kim, Sangwook Cho, Se-Young Yun, Comparing kullbackleibler divergence and mean squared error loss in knowledge distillation, 2021, IJCAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Taehyeon Kim",
                    "Jaehoon Oh",
                    "Nakyil Kim",
                    "Sangwook Cho",
                    "Se-Young Yun"
                ],
                "title": "Comparing kullbackleibler divergence and mean squared error loss in knowledge distillation",
                "pub_date": "2021",
                "pub_title": "IJCAI",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_92",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "P Diederik",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A method for stochastic optimization",
                "pub_date": "2015",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_93",
            "content": "UNKNOWN, None, 2009, Learning multiple layers of features from tiny images, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "Learning multiple layers of features from tiny images",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_94",
            "content": "Benlin Liu, Yongming Rao, Jiwen Lu, Jie Zhou, Cho-Jui Hsieh, Metadistiller: Network selfboosting via meta-learned top-down distillation, 2020, ECCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Benlin Liu",
                    "Yongming Rao",
                    "Jiwen Lu",
                    "Jie Zhou",
                    "Cho-Jui Hsieh"
                ],
                "title": "Metadistiller: Network selfboosting via meta-learned top-down distillation",
                "pub_date": "2020",
                "pub_title": "ECCV",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_95",
            "content": "Hanxiao Liu, Karen Simonyan, Yiming Yang, DARTS: differentiable architecture search, 2019, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Hanxiao Liu",
                    "Karen Simonyan",
                    "Yiming Yang"
                ],
                "title": "DARTS: differentiable architecture search",
                "pub_date": "2019",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_96",
            "content": "Michael Mccloskey, J Neal,  Cohen, Catastrophic interference in connectionist networks: The sequential learning problem, 1989, Psychology of learning and motivation, Elsevier.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Michael Mccloskey",
                    "J Neal",
                    " Cohen"
                ],
                "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
                "pub_date": "1989",
                "pub_title": "Psychology of learning and motivation",
                "pub": "Elsevier"
            }
        },
        {
            "ix": "21-ARR_v2_97",
            "content": "Mehrdad Seyed-Iman Mirzadeh, Ang Farajtabar, Nir Li, Akihiro Levine, Hassan Matsukawa,  Ghasemzadeh, Improved knowledge distillation via teacher assistant, 2020, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Mehrdad Seyed-Iman Mirzadeh",
                    "Ang Farajtabar",
                    "Nir Li",
                    "Akihiro Levine",
                    "Hassan Matsukawa",
                    " Ghasemzadeh"
                ],
                "title": "Improved knowledge distillation via teacher assistant",
                "pub_date": "2020",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_98",
            "content": "UNKNOWN, None, 2020, Meta-kd: A meta knowledge distillation framework for language model compression across domains, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Meta-kd: A meta knowledge distillation framework for language model compression across domains",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_99",
            "content": "UNKNOWN, None, 2021, Learning studentfriendly teacher networks for knowledge distillation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Learning studentfriendly teacher networks for knowledge distillation",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_100",
            "content": "Wonpyo Park, Dongju Kim, Yan Lu, Minsu Cho, Relational knowledge distillation, 2019, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Wonpyo Park",
                    "Dongju Kim",
                    "Yan Lu",
                    "Minsu Cho"
                ],
                "title": "Relational knowledge distillation",
                "pub_date": "2019",
                "pub_title": "CVPR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_101",
            "content": "Nikolaos Passalis, Anastasios Tefas, Learning deep representations with probabilistic knowledge transfer, 2018, ECCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Nikolaos Passalis",
                    "Anastasios Tefas"
                ],
                "title": "Learning deep representations with probabilistic knowledge transfer",
                "pub_date": "2018",
                "pub_title": "ECCV",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_102",
            "content": "Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao Wu, Jiaheng Liu, Zhaoning Zhang, Yu Liu, Correlation congruence for knowledge distillation, 2019, ICCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Baoyun Peng",
                    "Xiao Jin",
                    "Dongsheng Li",
                    "Shunfeng Zhou",
                    "Yichao Wu",
                    "Jiaheng Liu",
                    "Zhaoning Zhang",
                    "Yu Liu"
                ],
                "title": "Correlation congruence for knowledge distillation",
                "pub_date": "2019",
                "pub_title": "ICCV",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_103",
            "content": "UNKNOWN, None, 2020, Meta pseudo labels, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Meta pseudo labels",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_104",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_105",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Squad: 100, 000+ questions for machine comprehension of text, 2016, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Pranav Rajpurkar",
                    "Jian Zhang",
                    "Konstantin Lopyrev",
                    "Percy Liang"
                ],
                "title": "Squad: 100, 000+ questions for machine comprehension of text",
                "pub_date": "2016",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_106",
            "content": "UNKNOWN, None, 2015, Fitnets: Hints for thin deep nets, ICLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Fitnets: Hints for thin deep nets",
                "pub": "ICLR"
            }
        },
        {
            "ix": "21-ARR_v2_107",
            "content": "UNKNOWN, None, 2019, Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_108",
            "content": "UNKNOWN, None, 2021, Learning from deep model via exploring local targets, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Learning from deep model via exploring local targets",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_109",
            "content": "Karen Simonyan, Andrew Zisserman, Very deep convolutional networks for large-scale image recognition, 2015, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Karen Simonyan",
                    "Andrew Zisserman"
                ],
                "title": "Very deep convolutional networks for large-scale image recognition",
                "pub_date": "2015",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_110",
            "content": "K Naresh, Michael Sinha,  Griscik, A stochastic approximation method, 1971, IEEE Trans. Syst. Man Cybern, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "K Naresh",
                    "Michael Sinha",
                    " Griscik"
                ],
                "title": "A stochastic approximation method",
                "pub_date": "1971",
                "pub_title": "IEEE Trans. Syst. Man Cybern",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_111",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "Christopher Manning",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_112",
            "content": "Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu, Patient knowledge distillation for BERT model compression, 2019, EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Siqi Sun",
                    "Yu Cheng",
                    "Zhe Gan",
                    "Jingjing Liu"
                ],
                "title": "Patient knowledge distillation for BERT model compression",
                "pub_date": "2019",
                "pub_title": "EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_113",
            "content": "Yonglong Tian, Dilip Krishnan, Phillip Isola, Contrastive representation distillation, 2020, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Yonglong Tian",
                    "Dilip Krishnan",
                    "Phillip Isola"
                ],
                "title": "Contrastive representation distillation",
                "pub_date": "2020",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_114",
            "content": "Frederick Tung, Greg Mori, Similarity-preserving knowledge distillation, 2019, ICCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Frederick Tung",
                    "Greg Mori"
                ],
                "title": "Similarity-preserving knowledge distillation",
                "pub_date": "2019",
                "pub_title": "ICCV",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_115",
            "content": "UNKNOWN, None, 2019, Well-read students learn better: The impact of student initialization on knowledge distillation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Well-read students learn better: The impact of student initialization on knowledge distillation",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_116",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2019, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Alex Wang",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel Bowman"
                ],
                "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                "pub_date": "2019",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_117",
            "content": "UNKNOWN, None, 2020, Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_118",
            "content": "Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou, Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers, 2020, NeurIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Wenhui Wang",
                    "Furu Wei",
                    "Li Dong",
                    "Hangbo Bao",
                    "Nan Yang",
                    "Ming Zhou"
                ],
                "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
                "pub_date": "2020",
                "pub_title": "NeurIPS",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_119",
            "content": "UNKNOWN, None, 2019, Neural network acceptability judgments. TACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Neural network acceptability judgments. TACL",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_120",
            "content": "Adina Williams, Nikita Nangia, Samuel , Bowman. A broad-coverage challenge corpus for sentence understanding through inference, 2018, NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Adina Williams",
                    "Nikita Nangia",
                    "Samuel "
                ],
                "title": "Bowman. A broad-coverage challenge corpus for sentence understanding through inference",
                "pub_date": "2018",
                "pub_title": "NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_121",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, EMNLP (Demos), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Thomas Wolf",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Julien Chaumond",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Tim Rault",
                    "R\u00e9mi Louf",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer",
                    "Clara Patrick Von Platen",
                    "Yacine Ma",
                    "Julien Jernite",
                    "Canwen Plu",
                    "Teven Xu",
                    "Sylvain Scao",
                    "Mariama Gugger",
                    "Quentin Drame",
                    "Alexander Lhoest",
                    " Rush"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "EMNLP (Demos)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "21-ARR_v2_122",
            "content": "Gloria Brown, Wright , Student-centered learning in higher education, 2011, International Journal of Teaching and Learning in Higher Education, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Gloria Brown",
                    "Wright "
                ],
                "title": "Student-centered learning in higher education",
                "pub_date": "2011",
                "pub_title": "International Journal of Teaching and Learning in Higher Education",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_123",
            "content": "UNKNOWN, None, 2022, A survey on model compression for natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": null,
                "title": null,
                "pub_date": "2022",
                "pub_title": "A survey on model compression for natural language processing",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_124",
            "content": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou, Bert-of-theseus: Compressing BERT by progressive module replacing, 2020, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Canwen Xu",
                    "Wangchunshu Zhou",
                    "Tao Ge",
                    "Furu Wei",
                    "Ming Zhou"
                ],
                "title": "Bert-of-theseus: Compressing BERT by progressive module replacing",
                "pub_date": "2020",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_125",
            "content": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian Mcauley, Furu Wei, Beyond preserved accuracy: Evaluating loyalty and robustness of BERT compression, 2021, EMNLP, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Canwen Xu",
                    "Wangchunshu Zhou",
                    "Tao Ge",
                    "Ke Xu",
                    "Julian Mcauley",
                    "Furu Wei"
                ],
                "title": "Beyond preserved accuracy: Evaluating loyalty and robustness of BERT compression",
                "pub_date": "2021",
                "pub_title": "EMNLP",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "21-ARR_v2_126",
            "content": "UNKNOWN, None, 2021, A survey on green deep learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "A survey on green deep learning",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_127",
            "content": "Sergey Zagoruyko, Nikos Komodakis, Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer, 2017, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Sergey Zagoruyko",
                    "Nikos Komodakis"
                ],
                "title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer",
                "pub_date": "2017",
                "pub_title": "ICLR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_128",
            "content": "Ying Zhang, Tao Xiang, Timothy Hospedales, Huchuan Lu, Deep mutual learning, 2018, CVPR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Ying Zhang",
                    "Tao Xiang",
                    "Timothy Hospedales",
                    "Huchuan Lu"
                ],
                "title": "Deep mutual learning",
                "pub_date": "2018",
                "pub_title": "CVPR",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_129",
            "content": "UNKNOWN, None, 2020, BERT loses patience: Fast and robust inference with early exit, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "BERT loses patience: Fast and robust inference with early exit",
                "pub": null
            }
        },
        {
            "ix": "21-ARR_v2_130",
            "content": "UNKNOWN, None, 2021, Improving sequence-to-sequence pre-training via sequence span rewriting, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Improving sequence-to-sequence pre-training via sequence span rewriting",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "21-ARR_v2_0@0",
            "content": "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_0",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_2@0",
            "content": "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_2",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_2@1",
            "content": "We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_2",
            "start": 203,
            "end": 419,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_2@2",
            "content": "Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_2",
            "start": 421,
            "end": 604,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_2@3",
            "content": "Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_2",
            "start": 606,
            "end": 879,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_2@4",
            "content": "1 * Equal contribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_2",
            "start": 881,
            "end": 903,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_4@0",
            "content": "With the prevalence of large neural networks with millions or billions of parameters, model compression is gaining prominence for facilitating efficient, eco-friendly deployment for machine learning applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_4",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_4@1",
            "content": "Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015b) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015b;Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Ahn et al., 2019;Park et al., 2019;Passalis & Tefas, 2018;Heo et al., 2019;Kim et al., 2018;Shi et al., 2021;Sanh et al., 2019;Jiao et al., 2019;Wang et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_4",
            "start": 213,
            "end": 636,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_4@2",
            "content": "Previous works often train a large model as the \"teacher\"; then they fix the teacher and train a \"student\" model to mimic the behavior of the teacher, in order to transfer the knowledge from the teacher to the student.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_4",
            "start": 638,
            "end": 855,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_5@0",
            "content": "However, this paradigm has the following drawbacks: (1) The teacher is unaware of the student's capacity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_5",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_5@1",
            "content": "Recent studies in pedagogy suggest student-centered learning, which considers students' characteristics and learning capability, has shown effectiveness improving students' performance (Cornelius-White, 2007;Wright, 2011).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_5",
            "start": 106,
            "end": 327,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_5@2",
            "content": "However, in conventional knowledge distillation, the student passively accepts knowledge from the teacher, without regard for the student model's learning capability and performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_5",
            "start": 329,
            "end": 510,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_5@3",
            "content": "Recent works (Park et al., 2021;Shi et al., 2021) introduce student-aware distillation by jointly training the teacher and the student with task-specific objectives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_5",
            "start": 512,
            "end": 676,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_5@4",
            "content": "However, there is still space for improvement since: (2) The teacher is not optimized for distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_5",
            "start": 678,
            "end": 780,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_5@5",
            "content": "In previous works, the teacher is often trained to optimize its own inference performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_5",
            "start": 782,
            "end": 871,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_5@6",
            "content": "However, the teacher is not aware of the need to transfer its knowledge to a student and thus usually does so suboptimally.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_5",
            "start": 873,
            "end": 995,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_5@7",
            "content": "A real-world analogy is that a PhD student may have enough knowledge to solve problems themselves, but requires additional teaching training to qualify as a professor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_5",
            "start": 997,
            "end": 1163,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_6@0",
            "content": "To address these two drawbacks, we propose Knowledge Distillation with Meta Learning (MetaDistil), a new teacher-student distillation framework using meta learning (Finn et al., 2017) to exploit feedback about the student's learning progress to improve the teacher's knowledge transfer ability throughout the distillation process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_6",
            "start": 0,
            "end": 329,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_6@1",
            "content": "On the basis of previous formulations of bi-level optimization based meta learning (Finn et al., 2017), we propose a new mechanism called pilot update that aligns the learning of the bi-level learners (i.e., the teacher and the student).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_6",
            "start": 331,
            "end": 567,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_6@2",
            "content": "We illustrate the workflow of MetaDistil in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_6",
            "start": 569,
            "end": 621,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_6@3",
            "content": "The teacher in MetaDistil is trainable, which enables the teacher to adjust to its student network and also improves its L CE < l a t e x i t s h a 1 _ b a s e 6 4 = \" c W z Y M h 3 A e R J k O + v s c t A Q M H 7 i 1 8 8 = \" > A A A C A n i c b V B N S 8 N A E N 3 U r 1 q / o p 7 E S 7 A I n k p S B T 0 W i + D B Q w X b C m 0 I m + 2 2 X b r Z h N 2 J U E L w 4 l / x 4 k E R r / 4 K b / 4 b N 2 k O 2 v p g 4 e 1 7 M 8 z M 8 y P O F N j 2 t 1 F a W l 5 Z X S u v V z Y 2 t 7 Z 3 z N 2 9 j g p j S W i b h D y U 9 z 5 W l D N B 2 8 C A 0 / t I U h z 4 n H b 9 S T P z u w 9 U K h a K O 5 h G 1 A 3 w S L A h I x i 0 5 J k H / Q D D m G C e 3 K R e k n 8 Y J M 2 r N P X M q l 2 z c 1 Pilot update L 0 KD < l a t e x i t s h a 1 _ b a s e 6 4 = \" u z i w H V w 4 q q K f w s W y y 8 k 1 We call this process a \"teaching experiment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_6",
            "start": 623,
            "end": 1456,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_6@4",
            "content": "\" In this way, we can obtain an experimental student S that can be quizzed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_6",
            "start": 1457,
            "end": 1531,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_6@5",
            "content": "Then, we sample from the quiz set, and calculate the loss of S on these samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_6",
            "start": 1533,
            "end": 1612,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_6@6",
            "content": "We use this loss as a feedback signal to meta-update the teacher by calculating second derivatives and performing gradient descent (Finn et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_6",
            "start": 1614,
            "end": 1764,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_6@7",
            "content": "Finally, we discard the experimental subject S and use the updated teacher to distill into the student S on the same training batches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_6",
            "start": 1766,
            "end": 1899,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_6@8",
            "content": "The use of meta learning allows the teacher model to receive feedback from the student in a completely differentiable way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_6",
            "start": 1901,
            "end": 2022,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_6@9",
            "content": "We provide a simple and intuitive approach to explicitly optimize the teacher using the student's quiz performance as a proxy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_6",
            "start": 2024,
            "end": 2149,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_7@0",
            "content": "i L x C l I F R V o e e Z X f x C S O K A C C M d K 9 R w 7 A j f B E h j h N K 3 0 Y 0 U j T C Z 4 R H u a C h x Q 5 S b 5 C a l 1 r J W B N Q y l f g K s X P 3 d k e B A q W n g 6 8 p s R z X v Z e J / X i + G 4 Y W b M B H F Q A W Z D R r G 3 I L Q y v K w B k x S A n y q C S a S 6 V 0 t M s Y S E 9 C p V X Q I z v z J i 6 R T r z m n t f r t W b V x W c R R R o f o C J 0 g B 5 2 j B r p G L d R G B D 2 i Z / S K 3 o w n 4 8 V 4 N z 5 m p S W j 6 N l H f 2 B 8 / g A T 8 J f d < / l a t e x i t > T S",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_7",
            "start": 0,
            "end": 506,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_8@0",
            "content": "x 7 M u D t 4 = \" > A A A C A 3 i c b V D L S s N A F L 3 x W e s r 6 k 4 3 w S K 6 K k k V d F n Q h a C L C v Y B b Q i T 6 b Q d O p m E m Y l Q Q s C N v + L G h S J u / Q l 3 / o 2 T N A t t P T B w 5 p x 7 u f c e P 2 J U K t v + N h Y W l 5 Z X V k t r 5 f W N z a 1 t c 2 e 3 J c N Y Y N L E I Q t F x 0 e S M M p J U 1 H F S C c S B A U + I 2 1 / f J n 5 7 Q c i J A 3 5 v Z p E x A 3 Q k N M B x U h p y T P 3 e w F S I 4 x Y c n u c e k n + o y q 5 u U p T z 6 z Y V T u H N U + c g l S g Q M M z v 3 r 9 E M c B 4 Q o z J G X X s S P l J k g o i h l J y 7 1 Y k g j h M R q S r q Y c B U S 6 S X 5 D a h 1 p p W 8 N Q q E f V 1 a u / u 5 I U C D l J P B 1 Z b a j n P U y 8 T + v G 6 v B h Z t Q H s W K c D w d N I i Z p U I r C 8 T q U 0 G w Y h N N E B Z U 7 2 r h E R I I K x 1 b W Y f g z J 4 8 T 1 q 1 q n N a r d 2 d V e q 1 I o 4 S H M A h n I A D 5 1 C H a 2 h A E z A 8 w j O 8 w p v x Z L w Y 7 8 b H t H T B K H r 2 4 A + M z x + A Z 5 g F < / l a t e x i t > L 0 KD < l a t e x i t s h a 1 _ b a s e 6 4 = \" u z i w H V w 4 q q K f w s W y y 8 k 1 x 7 M u D t 4 = \" > A A A C A 3 i c b V D L S s N A F L 3 x W e s r 6 k 4 3 w S K 6 K k k V d F n Q h a C L C v Y B b Q i T 6 b Q d O p m E m Y l Q Q s C N v + L G h S J u / Q l 3 / o 2 T N A t t P T B w 5 p x 7 u f c e P 2 J U K t v + N h Y W l 5 Z X V k t r 5 f W N z a 1 t c 2 e 3 J c N Y Y N L E I Q t F x 0 e S M M p J U 1 H F S C c S B A U + I 2 1 / f J n 5 7 Q c i J A 3 5 v Z p E x A 3 Q k N M B x U h p y T P 3 e w F S I 4 x Y c n u c e k n + o y q 5 u U p T z 6 z Y V T u H N U + c g l S g Q M M z v 3 r 9 E M c B 4 Q o z J G X X s S P l J k g o i h l J y 7 1 Y k g j h M R q S r q Y c B U S 6 S X 5 D a h 1 p p W 8 N Q q E f V 1 a u / u 5 I U C D l J P B 1 Z b a j n P U y 8 T + v G 6 v B h Z t Q H s W K c D w d N I i Z p U I r C 8 T q U 0 G w Y h N N E B Z U 7 2 r h E R I I K x 1 b W Y f g z J 4 8 T 1 q 1 q n N a r d 2 d V e q 1 I o 4 S H M A h n I A D 5 1 C H a 2 h A E z A 8 w j O 8 w p v x Z L w Y 7 8 b H t H T B K H r 2 4 A + M z x + A Z 5 g F < / l a t e x i t > L KD < l a t e x i t s h a 1 _ b a s e 6 4 = \" O k / / 7 Q 4 U X i 0 + y v I X 1 p X 7 v f y q M Z s = \" > A A A C A n i c b V D L S s N A F J 3 U V 6 2 v q C t x E y y C q 5 J U Q Z c F X Q i 6 q G A f 0 I Y w m U 7 a o Z N J m L k R S g h u / B U 3 L h R x 6 1 e 4 8 2 + c t F l o 6 4 G B M + f c y 7 3 3 + D F n C m z 7 2 y g t L a + s r p X X K x u b W 9 s 7 5 u 5 e W 0 W J J L R F I h 7 J r o 8 V 5 U z Q F j D g t B t L i k O f 0 4 4 / v s z 9 z g O V i k X i H i Y x d U M 8 F C x g B I O W P P O g H 2 I Y E c z T 2 8 x L p x 8 G 6 c 1 V l n l m 1 a 7 Z U 1 i L x C l I F R V o e u Z X f x C R J K Q C C M d K 9 R w 7 B j f F E h j h N K v 0 E 0 V j T M Z 4 S H u a C h x S 5 a b T E z L r W C s D K 4 i k f g K s q f q 7 I 8 W h U p P Q 1 5 X 5 j m r e y 8 X / v F 4 C w Y W b M h E n Q A W Z D Q o S b k F k 5 X l Y A y Y p A T 7 R B B P J 9 K 4 W G W G J C e j U K j o E Z / 7 k R d K u 1 5 z T W v 3 u r N q o F 3 G U 0 S E 6 Q i f I Q e e o g a 5 R E 7 U Q Q Y / o G b 2 i N + P J e D H e j Y 9 Z a c k o e v b R H x i f P x n S l 9 Q = < / l a t",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_8",
            "start": 0,
            "end": 3116,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_9@0",
            "content": "To test the effectiveness of MetaDistil, we conduct extensive experiments on text and image classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_9",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_9@1",
            "content": "MetaDistil outperforms knowledge distillation by a large margin, verifying the effectiveness and versatility of our method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_9",
            "start": 114,
            "end": 236,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_9@2",
            "content": "Also, our method achieves state-of-the-art performance compressing BERT (Devlin et al., 2019) on the GLUE benchmark and shows competitive results compressing ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) on CIFAR-100 (Krizhevsky et al., 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_9",
            "start": 238,
            "end": 496,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_9@3",
            "content": "Additionally, we design experiments to analyze and explain the improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_9",
            "start": 498,
            "end": 572,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_9@4",
            "content": "Ablation studies show the effectiveness of our proposed pilot update and dynamic distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_9",
            "start": 574,
            "end": 667,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_9@5",
            "content": "Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably because of its ability to adjust the parameters of the teacher model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_9",
            "start": 669,
            "end": 867,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@0",
            "content": "Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., , 2021bXu & McAuley, 2022).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@1",
            "content": "Knowledge distillation is a prominent method for training compact networks to achieve comparable performance to a deep network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 138,
            "end": 264,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@2",
            "content": "Hinton et al. (2015b) first introduced the idea of knowledge distillation to exploit the \"dark knowledge\" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 266,
            "end": 493,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@3",
            "content": "Since its introduction, several works (Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Park et al., 2019;Sun et al., 2019;Jiao et al., 2019) have investigated methods that align different latent representations between the student and teacher models for better knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 495,
            "end": 792,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@4",
            "content": "In the context of knowledge distillation, MetaDistil shares some common ideas with the line of work that utilizes a sequence of intermediate teacher models to make the teacher network better adapt to the capacity of the student model throughout the training process, including teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020) and route constraint optimization (RCO) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 794,
            "end": 1183,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@5",
            "content": "However, the intermediate teachers are heuristically selected independently of the training process and the evolution of the teacher network is discrete.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 1185,
            "end": 1337,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@6",
            "content": "In contrast, MetaDistil employs meta learning to make the teacher model adapt to the current state of the student model and provide a continuously evolving meta-teacher that can better teach the student.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 1339,
            "end": 1541,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@7",
            "content": "Concurrently, Park et al. (2021) and Shi et al. (2021) propose to update the teacher model jointly with the student model with task specific objectives (e.g., cross-entropy loss) during the KD process and add constraints to keep student and teacher similar to each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 1543,
            "end": 1813,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@8",
            "content": "Their approaches makes the teacher model aware of the student model by constraining the teacher model's capacity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 1815,
            "end": 1927,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@9",
            "content": "However, the teacher models in their methods are still not optimized for knowledge transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 1929,
            "end": 2020,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@10",
            "content": "In addition, Zhang et al. (2018) introduced deep mutual learning where multiple models learn collaboratively and teach each other throughout the training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 2022,
            "end": 2183,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@11",
            "content": "While it is focused on a different setting where different models have approximately the same capacity and are learned from scratch, it also encourages the teacher model to behave similarly to the student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 2185,
            "end": 2395,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_11@12",
            "content": "Different from all aforementioned methods, MetaDistil employs meta learning to explicitly optimize the teacher model for better knowledge transfer ability, and leads to improved performance of the resulting student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_11",
            "start": 2397,
            "end": 2617,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@0",
            "content": "The core idea of meta learning is \"learning to learn,\" which means taking the optimization process of a learning algorithm into consideration when optimizing the learning algorithm itself.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@1",
            "content": "Meta learning typically involves a bi-level optimization process where the inner-learner provides feedback for optimization of the meta-learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 189,
            "end": 332,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@2",
            "content": "Successful applications of meta learning include learning better initialization (Finn et al., 2017), architecture search , learning to optimize the learning rate schedule (Baydin et al., 2018), and learning to optimize (Andrychowicz et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 334,
            "end": 580,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@3",
            "content": "These works typically aim to obtain an optimized meta-learner (i.e., the teacher model in MetaDistil), while the optimization of the inner-learner (i.e., the student model in MetaDistil), is mainly used to provide learning signal for the meta optimization process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 582,
            "end": 845,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@4",
            "content": "This is different from the objective of knowledge distillation where an optimized student model is the goal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 847,
            "end": 954,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@5",
            "content": "Recently, there have been a few works investigating using this bi-level optimization framework to obtain a better inner-learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 956,
            "end": 1083,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@6",
            "content": "For example, meta pseudo labels (Pham et al., 2020) use meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 1085,
            "end": 1350,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@7",
            "content": "These methods adapt the same bi-level optimization process as previous works where the goal is to obtain an optimized meta-learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 1352,
            "end": 1482,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@8",
            "content": "In these approaches, during each iteration, the meta-learner is optimized for the original inner-learner and then applied to the updated inner-learner in the next iteration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 1484,
            "end": 1656,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@9",
            "content": "This leads to a mismatch between the meta-learner and the inner-learner, and is therefore suboptimal for learning a good inner-learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 1658,
            "end": 1792,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_12@10",
            "content": "In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problems, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_12",
            "start": 1794,
            "end": 2026,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_13@0",
            "content": "Meta Knowledge Distillation Recently, some works on KD take a meta approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_13",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_13@1",
            "content": "Pan et al. (2020) proposed a framework to train a metateacher across domains that can better fit new domains with meta-learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_13",
            "start": 77,
            "end": 204,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_13@2",
            "content": "Then, traditional KD is performed to transfer the knowledge from the meta-teacher to the student.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_13",
            "start": 206,
            "end": 302,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_13@3",
            "content": "Liu et al. (2020) proposed a self-distillation network which utilizes meta-learning to train a label-generator as a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_13",
            "start": 304,
            "end": 517,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_13@4",
            "content": "Different from the above, MetaDistil is a general knowledge distillation method that exploits meta-learning to allow the teacher to learn to teach dynamically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_13",
            "start": 519,
            "end": 677,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_13@5",
            "content": "Instead of merely training a meta-teacher, our method uses meta-learning throughout the procedure of knowledge transfer, making the teacher model compatible for the student model for every training example during each training stage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_13",
            "start": 679,
            "end": 911,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_14@0",
            "content": "Knowledge Distillation with Meta Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_14",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_15@0",
            "content": "An overview of MetaDistil is presented in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_15",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_15@1",
            "content": "MetaDistil includes two major components.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_15",
            "start": 52,
            "end": 92,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_15@2",
            "content": "First, the meta update enables the teacher model to receive the student model's feedback on the distillation process, allowing the teacher model to \"learn to teach\" and provide distillation signals that are more suitable for the student model's current capacity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_15",
            "start": 94,
            "end": 355,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_15@3",
            "content": "The pilot update mechanism ensures a finergrained match between the student model and the meta-updated teacher model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_15",
            "start": 357,
            "end": 473,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_16@0",
            "content": "Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_16",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_17@0",
            "content": "Knowledge Distillation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_17",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_18@0",
            "content": "Knowledge distillation algorithms aim to exploit the hidden knowledge from a large teacher network, denoted as T , to guide the training of a shallow student network, denoted as S. To help transfer the knowledge from the teacher to the student, apart from the original task-specific objective (e.g., crossentropy loss), a knowledge distillation objective which aligns the behavior of the student and the teacher is included to train the student network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_18",
            "start": 0,
            "end": 452,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_18@1",
            "content": "Formally, given a labeled dataset D of N samples D = {(x 1 , y 1 ) , . . . , (x N , y N )}, we can write the loss function of the student network as follows,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_18",
            "start": 454,
            "end": 610,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_19@0",
            "content": "L S (D; \u03b8 S ; \u03b8 T ) = 1 N N i=1 [\u03b1L T (y i , S (x i ; \u03b8 S )) + (1 \u2212 \u03b1) L KD (T (x i ; \u03b8 T ) , S (x i ; \u03b8 S ))] (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_19",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_20@0",
            "content": "where \u03b1 is a hyper-parameter to control the relative importance of the two terms; \u03b8 T and \u03b8 S are the parameters of the teacher T and student S, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_20",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_20@1",
            "content": "L T refers to the task-specific loss and L KD refers to the knowledge distillation loss which measures the similarity of the student and the teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_20",
            "start": 159,
            "end": 307,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_20@2",
            "content": "Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error (MSE) between student and teacher logits, the similarity between the student and the teacher's attention distribution, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_20",
            "start": 309,
            "end": 562,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_20@3",
            "content": "We do not specify the detailed form of the loss function because MetaDistil is a general framework that can be easily applied to various kinds of KD objectives as long as the objective is differentiable with respect to the teacher parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_20",
            "start": 564,
            "end": 805,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_20@4",
            "content": "In the experiments of this paper, we use mean squared error between the hidden states of the teacher and the student for both our method and the KD baseline since recent study finds that it is more stable and slightly outperforms than KL divergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_20",
            "start": 807,
            "end": 1055,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_21@0",
            "content": "Meta Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_21",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_22@0",
            "content": "In meta learning algorithms that involve a bi-level optimization problem (Finn et al., 2017), there exists an inner-learner f i and a meta-learner f m .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_22",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_22@1",
            "content": "The inner-learner is trained to accomplish a task T or a distribution of tasks with help from the metalearner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_22",
            "start": 153,
            "end": 262,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_22@2",
            "content": "The training process of f i on T with the help of f m is typically called inner-loop, and we can denote f i (f m ) as the updated inner-learner after the inner-loop.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_22",
            "start": 264,
            "end": 428,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_22@3",
            "content": "We can express f i as a function of f m because learning f i depends on f m .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_22",
            "start": 430,
            "end": 506,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_22@4",
            "content": "In return, the meta-learner is optimized with a meta objective, which is generally the maximization of expected performance of the inner-learner after the innerloop, i.e., f i (f m ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_22",
            "start": 508,
            "end": 690,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_22@5",
            "content": "This learning process is called a meta-loop and is often accomplished by gradient descent with derivatives of L(f i (f m )), the loss of updated inner-leaner on some held-out support set (i.e., the quiz set in our paper).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_22",
            "start": 692,
            "end": 912,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_23@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_23",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_24@0",
            "content": "Pilot Update",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_24",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@0",
            "content": "In the original formulation of meta learning (Finn et al., 2017), the purpose is to learn a good metalearner f m that can generalize to different innerlearners f i for different tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@1",
            "content": "In their approach, the meta-learner is optimized for the \"original\" innerlearner at the beginning of each iteration and the current batch of training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 185,
            "end": 339,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@2",
            "content": "The updated metalearner is then applied to the updated inner-learner and a different batch of data in the next iteration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 341,
            "end": 461,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@3",
            "content": "This behavior is reasonable if the purpose is to optimize the meta-learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 463,
            "end": 537,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@4",
            "content": "However, in MetaDistil, we only care about the performance of the only innerlearner, i.e., the student.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 539,
            "end": 641,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@5",
            "content": "In this case, this behavior leads to a mismatch between the meta-learner and the inner-learner, and is therefore suboptimal for learning a good inner-learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 643,
            "end": 800,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@6",
            "content": "Therefore, we need a way to align and synchronize the learning of the meta-and inner-learner, in order to allow an update step of the meta-learner to have an instant effect on the inner-learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 802,
            "end": 995,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@7",
            "content": "This instant reflection prevents the meta-learner from catastrophic forgetting (McCloskey & Cohen, 1989).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 997,
            "end": 1101,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@8",
            "content": "To achieve this, we design a pilot update mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 1103,
            "end": 1154,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@9",
            "content": "For a batch of training data x, we first make a temporary copy of the inner-learner f i and update both the copy f i and the meta learner f m on x.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 1156,
            "end": 1302,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@10",
            "content": "Then, we discard f i and update f i again with the updated f m on the same data x.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 1304,
            "end": 1385,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@11",
            "content": "This mechanism can apply the impact of data x to both f m and f i at the same time, thus aligns the training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 1387,
            "end": 1503,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@12",
            "content": "Pilot update is a general technique that can potentially be applied to any meta learning application that optimizes the inner-learner performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 1505,
            "end": 1650,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_25@13",
            "content": "We will describe how we apply this mechanism to MetaDistil shortly and empirically verify the effectiveness of pilot update in Section 4.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_25",
            "start": 1652,
            "end": 1790,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_26@0",
            "content": "Learning to Teach",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_26",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_27@0",
            "content": "In MetaDistil, we would like to optimize the teacher model, which is fixed in traditional KD frameworks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_27",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_27@1",
            "content": "Different from previous deep mutual learning (Zhang et al., 2018) methods that switch the role between the student and teacher",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_27",
            "start": 105,
            "end": 230,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_28@0",
            "content": "Algorithm 1 Knowledge Distillation with Meta Learning (MetaDistil)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_28",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_29@0",
            "content": "Require: student \u03b8S, teacher \u03b8T , train set D, quiz set Q Require: \u03bb, \u00b5: learning rate for the student and the teacher 1: while not done do 2: Sample batch of training data x \u223c D 3:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_29",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_30@0",
            "content": "Copy student parameter \u03b8S to student \u03b8 S 4:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_30",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_31@0",
            "content": "Update \u03b8 S with x and \u03b8T : \u03b8 S \u2190 \u03b8 S \u2212 \u03bb\u2207 \u03b8 S LS(x; \u03b8S; \u03b8T ) 5:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_31",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_32@0",
            "content": "Sample a batch of quiz data q \u223c Q 6:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_32",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_33@0",
            "content": "Update \u03b8T with q and \u03b8 S : \u03b8T \u2190 \u03b8T \u2212 \u00b5\u2207 \u03b8 T LT (q, \u03b8 S (\u03b8T )) 7:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_33",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_34@0",
            "content": "Update original \u03b8S with x and the updated \u03b8T : \u03b8S \u2190 \u03b8S \u2212 \u03bb\u2207 \u03b8 S LS(x; \u03b8S; \u03b8T ) 8: end while network and train the original teacher model with soft labels generated by the student model, or recent works (Shi et al., 2021;Park et al., 2021) that update the teacher model with a task-specific loss during the KD process, MetaDistil explicitly optimizes the teacher model in a \"learning to teach\" fashion, so that it can better transfer its knowledge to the student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_34",
            "start": 0,
            "end": 467,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_34@1",
            "content": "Concretely, the optimization objective of the teacher model in the MetaDistil framework is the performance of the student model after distilling from the teacher model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_34",
            "start": 469,
            "end": 636,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_34@2",
            "content": "This \"learning to teach\" paradigm naturally fits the bi-level optimization framework in meta learning literature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_34",
            "start": 638,
            "end": 750,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_35@0",
            "content": "In the MetaDistil framework, the student network \u03b8 S is the inner-learner and the teacher network \u03b8 T is the meta-learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_35",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_35@1",
            "content": "For each training step, we first copy the student model \u03b8 S to an \"experimental student\" \u03b8 S .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_35",
            "start": 123,
            "end": 216,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_35@2",
            "content": "Then given a batch of training examples x and the learning rate \u03bb, the experimental student is updated in the same way as conventional KD algorithms:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_35",
            "start": 218,
            "end": 366,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_36@0",
            "content": "\u03b8 S (\u03b8 T ) = \u03b8 S \u2212 \u03bb\u2207 \u03b8 S L S (x; \u03b8 S ; \u03b8 T ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_36",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_37@0",
            "content": "(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_37",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_38@0",
            "content": "To simplify notation, we will consider one gradient update for the rest of this section, but using multiple gradient updates is a straightforward extension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_38",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_38@1",
            "content": "We observe that the updated experimental student parameter \u03b8 S , as well as the student quiz loss l q = L T (q, \u03b8 S (\u03b8 T )) on a batch of quiz samples q sampled from a held-out quiz set Q, is a function of the teacher parameter \u03b8 T .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_38",
            "start": 157,
            "end": 389,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_38@2",
            "content": "Therefore, we can optimize l q with respect to \u03b8 T by a learning rate \u00b5:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_38",
            "start": 391,
            "end": 462,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_39@0",
            "content": "\u03b8 T \u2190 \u03b8 T \u2212 \u00b5\u2207 \u03b8 T L T q, \u03b8 S (\u03b8 T )(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_39",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_40@0",
            "content": "We evaluate the performance of the experimental student on a separate quiz set to prevent overfitting the validation set, which is preserved for model selection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_40",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_40@1",
            "content": "Note that the student is never trained on the quiz set and the teacher only performs meta-update on the quiz set instead of fitting it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_40",
            "start": 162,
            "end": 296,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_40@2",
            "content": "We do not use a dynamic quiz set strategy because otherwise the student would have been trained on the quiz set and the loss would not be informative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_40",
            "start": 298,
            "end": 447,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_40@3",
            "content": "After meta-updating the teacher model, we then update the \"real\" student model in the same way as described in Equation 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_40",
            "start": 449,
            "end": 570,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_40@4",
            "content": "Intuitively, optimizing the teacher network \u03b8 T with Equation 3 is maximizing the expected performance of the student network after being taught by the teacher with the KD objective in the inner-loop.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_40",
            "start": 572,
            "end": 771,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_40@5",
            "content": "This meta-objective allows the teacher model to adjust its parameters to better transfer its knowledge to the student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_40",
            "start": 773,
            "end": 896,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_40@6",
            "content": "We apply the pilot update strategy described in Section 3.2.1 to better align the learning of the teacher and student, as shown in Algorithm 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_40",
            "start": 898,
            "end": 1040,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_41@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_41",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_42@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_42",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_43@0",
            "content": "We evaluate MetaDistil on two commonly used classification benchmarks for knowledge distillation in both Natural Language Processing and Computer Vision (see Appendix A).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_43",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_44@0",
            "content": "Settings For NLP, we evaluate our proposed approach on the GLUE benchmark .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_44",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_44@1",
            "content": "Specifically, we test on MRPC (Dolan & Brockett, 2005), QQP and STS-B (Conneau & Kiela, 2018) for Paraphrase Similarity Matching; SST-2 (Socher et al., 2013) for Sentiment Classification; MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016) and RTE for the Natural Language Inference; CoLA (Warstadt et al., 2019) for Linguistic Acceptability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_44",
            "start": 76,
            "end": 425,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_44@2",
            "content": "Following previous studies (Sun et al., 2019;Jiao et al., 2019; we report the results on MNLI-m and MNLI-mm, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_44",
            "start": 427,
            "end": 548,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_44@3",
            "content": "For MRPC and QQP, we report both F1 and accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_44",
            "start": 550,
            "end": 598,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_44@4",
            "content": "For STS-B, we report Pearson and Spearman correlation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_44",
            "start": 600,
            "end": 653,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_44@5",
            "content": "The metric for CoLA is Matthew's correlation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_44",
            "start": 655,
            "end": 699,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_44@6",
            "content": "The other tasks use accuracy as the metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_44",
            "start": 701,
            "end": 743,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_45@0",
            "content": "Following previous works (Sun et al., 2019;Turc et al., 2019;, we evaluate MetaDistil in a task-specific setting where the teacher model is fine-tuned on a downstream task and the student model is trained on the task with the KD loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_45",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_45@1",
            "content": "We do not choose the pretraining distillation setting since it requires significant computational resources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_45",
            "start": 235,
            "end": 342,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_45@2",
            "content": "We implement MetaDistil based on Hugging Face Transformers (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_45",
            "start": 344,
            "end": 422,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_46@0",
            "content": "Baselines For comparison, we report the results of vanilla KD and patient knowledge distillation (Sun et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_46",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_46@1",
            "content": "We also include the results of progressive module replacing , a state-of-the-art task-specific compression method for BERT which also uses a larger teacher model to improve smaller ones like knowledge distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_46",
            "start": 117,
            "end": 330,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_46@2",
            "content": "In addition, according to Turc et al. (2019), the reported performance of current taskspecific BERT compression methods is underestimated because the student model is not appropriately initialized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_46",
            "start": 332,
            "end": 528,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_46@3",
            "content": "To ensure fair comparison, we re-run task-specific baselines with student models initialized by a pretrained 6-layer BERT model and report our results in addition to the official numbers in the original papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_46",
            "start": 530,
            "end": 739,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_46@4",
            "content": "We also compare against deep mutual learning (DML) (Zhang et al., 2018), teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020), route constraint optimization (RCO) , proximal knowledge teaching (ProKT) (Shi et al., 2021), and student-friendly teacher network (SFTN) (Park et al., 2021), where the teacher network is not fixed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_46",
            "start": 741,
            "end": 1084,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_46@5",
            "content": "For reference, we also present results of pretraining distilled models including DistilBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2019), MiniLM v1 and v2 (Wang et al., 2020b,a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_46",
            "start": 1086,
            "end": 1268,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_46@6",
            "content": "Note that among these baselines, PKD (Sun et al., 2019) and Theseus exploit intermediate features while TinyBERT and the MiniLM family use both intermediate and Transformer-specific features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_46",
            "start": 1270,
            "end": 1460,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_46@7",
            "content": "In contrast, MetaDistil uses none of these but the vanilla KD loss (Equation 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_46",
            "start": 1462,
            "end": 1541,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_47@0",
            "content": "Training Details For training hyperparameters, we fix the maximum sequence length to 128 and the temperature to 2 for all tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_47",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_47@1",
            "content": "For our method and all baselines (except those with officially reported numbers), we perform grid search over the sets of the student learning rate \u03bb from {1e-5, 2e-5, 3e-5}, the teacher learning rate \u00b5 from {2e-6, 5e-6, 1e-5}, the batch size from {32, 64}, the weight of KD loss from {0.4, 0.5, 0.6}.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_47",
            "start": 129,
            "end": 429,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_47@2",
            "content": "We randomly split the original training set to a new training set and the quiz set by 9 : 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_47",
            "start": 431,
            "end": 522,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_47@3",
            "content": "For RCO, we select four unconverged teacher checkpoints as the intermediate training targets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_47",
            "start": 524,
            "end": 616,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_47@4",
            "content": "For TAKD, we use KD to train a teacher assistant model with 10 Transformer layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_47",
            "start": 618,
            "end": 699,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_48@0",
            "content": "Experimental Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_48",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_49@0",
            "content": "We report the experimental results on both the development set and test set of the eight GLUE tasks in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_49",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_49@1",
            "content": "MetaDistil achieves state-of-the-art performance under the task-specific setting and outperforms all KD baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_49",
            "start": 112,
            "end": 225,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_49@2",
            "content": "Notably, without using any intermediate or model-specific features in the loss function, MetaDistil outperforms methods with carefully designed features, e.g., PKD and TinyBERT (without data augmentation).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_49",
            "start": 227,
            "end": 431,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_49@3",
            "content": "Compared with other methods with a trainable teacher (Zhang et al., 2018;Mirzadeh et al., 2020;Shi et al., 2021), our method still demonstrates superior performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_49",
            "start": 433,
            "end": 597,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_49@4",
            "content": "As we analyze, with the help of meta learning, MetaDistil is able to directly optimize the teacher's teaching ability thus yielding a further improvement in terms of student accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_49",
            "start": 599,
            "end": 781,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_49@5",
            "content": "Also, we observe a performance drop by replacing pilot update with a normal update.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_49",
            "start": 783,
            "end": 865,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_49@6",
            "content": "This ablation study verifies the effectiveness of our proposed pilot update mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_49",
            "start": 867,
            "end": 952,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_49@7",
            "content": "Moreover, MetaDistil achieves very competitive results on image classification as well, as described in Section A.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_49",
            "start": 954,
            "end": 1069,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_50@0",
            "content": "Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_50",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_51@0",
            "content": "Why Does MetaDistil Work?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_51",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_52@0",
            "content": "We investigate the effect of meta-update for each iteration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_52",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_52@1",
            "content": "We inspect (1) the validation loss of S after the teaching experiment and that of S after the real distillation update, and (2) the KD loss, which describes the discrepancy between student and teacher, before and after the teacher update.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_52",
            "start": 61,
            "end": 298,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_53@0",
            "content": "We find that for 87% of updates, the student model's validation loss after real update (Line 7 in Algorithm 1) is smaller than that after the teaching experiment (Line 4 in Algorithm 1), which would be the update to the student S in the variant without pilot update.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_53",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_53@1",
            "content": "This confirms the effectiveness of the pilot update mechanism on better matching the student and teacher model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_53",
            "start": 267,
            "end": 377,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_54@0",
            "content": "Moreover, we find that in 91% of the first half of the updates, the teacher becomes more similar (in terms of logits distributions) to the student after the meta-update, which indicates that the teacher is learning to adapt to a low-performance student (like an elementary school teacher).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_54",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_54@1",
            "content": "However, in the second half of MetaDistil, this percentage drops to 63%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_54",
            "start": 290,
            "end": 361,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_54@2",
            "content": "We suspect this is because in the later training stages, the teacher needs to actively evolve itself beyond the student to guide the student towards further improvement (like a university professor).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_54",
            "start": 363,
            "end": 561,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_55@0",
            "content": "Finally, we try to apply a meta-learned teacher to a conventional static distillation and also to an unfamiliar student.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_55",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_55@1",
            "content": "We describe the results in details in Section A.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_55",
            "start": 121,
            "end": 170,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_56@0",
            "content": "Hyper-parameter Sensitivity",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_56",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_57@0",
            "content": "A motivation of MetaDistil is to enable the teacher to dynamically adjust its knowledge transfer in an optimal way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_57",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_57@1",
            "content": "Similar to Adam (Kingma & Ba, 2015) vs. SGD (Sinha & Griscik, 1971;Kiefer et al., 1952) for optimization, with the ability of dynamic adjusting, it is natural to expect MetaDistil to be more insensitive and robust to changes of the settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_57",
            "start": 116,
            "end": 356,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_57@2",
            "content": "Here, we evaluate the performance of MetaDistil with students of various capability, and a wide variety of hyperparameters, including loss weight and temperature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_57",
            "start": 358,
            "end": 519,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_58@0",
            "content": "Student Capability To investigate the performance of MetaDistil under different student capacity, we experiment to distill BERT-Base into BERT-6L, Medium, Small, Mini and Tiny (Turc et al., 2019) with conventional KD and MetaDistil.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_58",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_58@1",
            "content": "We plot the performance with the student's parameter number in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_58",
            "start": 233,
            "end": 304,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_58@2",
            "content": "Additionally, we show results for different compression ratio in Appendix B.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_58",
            "start": 306,
            "end": 381,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_59@0",
            "content": "Loss Weight In KD, tuning the loss weight is nontrivial and often requires hyperparameter search.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_59",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_60@0",
            "content": "To test the robustness of MetaDistil under different loss weights, we run experiments with different \u03b1 (Equation 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_60",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_60@1",
            "content": "As shown in Figure 3, MetaDistil consistently outperforms conventional KD and is less sensitive to different \u03b1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_60",
            "start": 117,
            "end": 227,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_61@0",
            "content": "Temperature Temperature is a re-scaling trick introduced in Hinton et al. (2015b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_61",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_61@1",
            "content": "We try different temperatures and illustrate the performance of KD and MetaDistil in Figure 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_61",
            "start": 83,
            "end": 176,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_61@2",
            "content": "MetaDistil shows better performance and robustness compared to KD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_61",
            "start": 178,
            "end": 243,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_62@0",
            "content": "Limitation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_62",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_63@0",
            "content": "Like all meta learning algorithms, MetaDistil inevitably requires two rounds of updates involving both first and second order derivatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_63",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_63@1",
            "content": "Thus, MetaDistil requires additional computational time and memory than a normal KD method, which can be a limitation of our method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_63",
            "start": 139,
            "end": 270,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_63@2",
            "content": "We compare the computational overheads of MetaDistil with other methods in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_63",
            "start": 272,
            "end": 354,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_63@3",
            "content": "Although our approach takes more time to achieve its own peak performance, it can match up the performance of PKD (Sun et al., 2019) with a similar time cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_63",
            "start": 356,
            "end": 513,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_63@4",
            "content": "The memory use of our method is higher than PKD and ProKT (Shi et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_63",
            "start": 515,
            "end": 591,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_63@5",
            "content": "However, this one-off investment can lead to a better student model for inference, thus can be worthy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_63",
            "start": 593,
            "end": 694,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_64@0",
            "content": "Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_64",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_65@0",
            "content": "In this paper, we present MetaDistil, a knowledge distillation algorithm powered by meta learning that explicitly optimizes the teacher network to better transfer its knowledge to the student network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_65",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_66@0",
            "content": "The extensive experiments verify the effectiveness and robustness of MetaDistil.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_66",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_67@0",
            "content": "MetaDistil focuses on improving the performance of knowledge distillation and does not introduce extra ethical concerns compared to vanilla KD methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_67",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_67@1",
            "content": "Nevertheless, we would like to point out that as suggested by Hooker et al. (2020), model compression may lead to biases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_67",
            "start": 152,
            "end": 272,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_67@2",
            "content": "However, this is not an outstanding problem of our method but a common risk in model compression, which needs to be addressed in the future.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_67",
            "start": 274,
            "end": 413,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_68@0",
            "content": "In MetaDistil, the student is trained in a dynamic manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_68",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_68@1",
            "content": "To investigate the effect of such a dynamic distillation process, we attempt to use the teacher at the end of MetaDistil training to perform a static conventional KD, to verify the effectiveness of our dynamic distillation strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_68",
            "start": 59,
            "end": 290,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_68@2",
            "content": "As shown in Table 4, on both experiments, dynamic MetaDistil outperforms conventional KD and static distillation with the teacher at the end of MetaDistil training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_68",
            "start": 292,
            "end": 455,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_68@3",
            "content": "As mentioned in Section 3.2, a meta teacher is optimized to transfer its knowledge to a specific student network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_68",
            "start": 457,
            "end": 569,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_68@4",
            "content": "To justify this motivation, we conduct experiments using a teacher optimized for the ResNet-32 student to statically distill to the ResNet-20 student, and also in reverse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_68",
            "start": 571,
            "end": 741,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_68@5",
            "content": "As shown in Table 4, the cross-taught students underperform the static students taught by their own teachers by 0.27 and 0.12 for ResNet-32 and ResNet-20, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_68",
            "start": 743,
            "end": 910,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_68@6",
            "content": "This confirms our motivation that the meta teacher in MetaDistil can adjust itself according to its student.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_68",
            "start": 912,
            "end": 1019,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_69@0",
            "content": "In this section, we present additional experimental results in settings with different compression ratios to further demonstrate the effectiveness of MetaDistil on bridging the gap between the student and teacher capacity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_69",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_69@1",
            "content": "Specifically, we conduct experiments in the following two settings: (1) distilling BERT-base into a 4-layer BERT (110M\u219252M) and (2) distilling BERT-large into a 6-layer BERT (345M\u219266M).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_69",
            "start": 223,
            "end": 407,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_69@2",
            "content": "The results are shown in Table 4 and Table 5, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_69",
            "start": 409,
            "end": 467,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_69@3",
            "content": "We can see that MetaDistil consistently outperforms PKD and ProKT in both settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_69",
            "start": 469,
            "end": 551,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_69@4",
            "content": "This confirms the effectiveness of MetaDistil and also show its ability to adapt the teacher model to the student model, since the gap between teacher and student is even larger in these settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_69",
            "start": 553,
            "end": 748,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_70@0",
            "content": "We also investigate why MetaDistil works by conducting experiments on the development sets of MNLI, SST, and MRPC, which are important tasks in GLUE that have a large, medium, and small training set, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_70",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_70@1",
            "content": "We illustrate the validation accuracy curves of the meta teacher and student models with training steps in Figure 5, and compare them to the student performance in conventional KD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_70",
            "start": 214,
            "end": 393,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_70@2",
            "content": "We can see that the meta teacher maintains high accuracy in the first 5,000 steps and then begins to slowly degrade.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_70",
            "start": 395,
            "end": 510,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_70@3",
            "content": "Starting from step 8,000, the teacher model underperforms the student while the student's accuracy keeps increasing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_70",
            "start": 512,
            "end": 627,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_70@4",
            "content": "This verifies our assumption that a model with the best accuracy is not necessarily the optimal teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_70",
            "start": 629,
            "end": 732,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_70@5",
            "content": "Also, MetaDistil is not naively optimizing the teacher's accuracy but its \"teaching skills.\" This phenomenon suggests that beyond high accuracy, there could be more important properties of a good teacher that warrant further investigation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_70",
            "start": 734,
            "end": 972,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_71@0",
            "content": "While MetaDistil achieves improved student accuracy on the GLUE benchmark, it is still not very clear where the performance improvement comes from.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_71",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_71@1",
            "content": "There are two possibilities: (1) the student better mimics the teacher, and (2) the changes of teacher helps student perform better on hard examples that would be incorrectly classified by the student with vanilla KD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_71",
            "start": 148,
            "end": 364,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_71@2",
            "content": "We conduct a series of analysis on the MRPC dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_71",
            "start": 366,
            "end": 417,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_72@0",
            "content": "For the first assumption, we compute the prediction loyalty (Xu et al., 2021a) of the student model distilled with PKD and MetaDistil, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_72",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_72@1",
            "content": "For MetaDistil, we measure the loyalty with respect to both the original teacher and the final teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_72",
            "start": 149,
            "end": 251,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_72@2",
            "content": "We find that there is no significant difference between between PKD and MetaDistil.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_72",
            "start": 253,
            "end": 335,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_72@3",
            "content": "This suggests that the improvement does not come from student better mimicking the teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_72",
            "start": 337,
            "end": 427,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_73@0",
            "content": "For the second assumption, we first identify the examples in the quiz set for which our model gives correct predictions while the student distilled by PKD makes a wrong prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_73",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_73@1",
            "content": "We then compute",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_73",
            "start": 181,
            "end": 195,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_74@0",
            "content": "Sungsoo Ahn, Shell Hu, Andreas Damianou, Neil Lawrence, Zhenwen Dai, Variational information distillation for knowledge transfer, 2019, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_74",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_75@0",
            "content": "Marcin Andrychowicz, Misha Denil, Sergio Colmenarejo, Matthew Hoffman, David Pfau, Tom Schaul, Nando De Freitas, Learning to learn by gradient descent by gradient descent, 2016, NeurIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_75",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_76@0",
            "content": "Robert Atilim Gunes Baydin, David Cornish, Mark Mart\u00ednez-Rubio, Frank Schmidt,  Wood, Online learning rate adaptation with hypergradient descent, 2018, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_76",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_77@0",
            "content": "Alexis Conneau, Douwe Kiela, Senteval: An evaluation toolkit for universal sentence representations, 2018, LREC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_77",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_78@0",
            "content": "Jeffrey Cornelius-White, Learner-centered teacherstudent relationships are effective: A meta-analysis, 2007, Review of educational research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_78",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_79@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: pre-training of deep bidirectional transformers for language understanding, 2019, NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_79",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_80@0",
            "content": "B William, Chris Dolan,  Brockett, Automatically constructing a corpus of sentential paraphrases, 2005, IWP@IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_80",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_81@0",
            "content": "UNKNOWN, None, 2017, Model-agnostic meta-learning for fast adaptation of deep networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_81",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_82@0",
            "content": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep residual learning for image recognition, 2016, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_82",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_83@0",
            "content": "Byeongho Heo, Minsik Lee, Sangdoo Yun, Jin Choi, Knowledge transfer via distillation of activation boundaries formed by hidden neurons, 2019, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_83",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_84@0",
            "content": "UNKNOWN, None, 2015, Distilling the knowledge in a neural network, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_84",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_85@0",
            "content": "UNKNOWN, None, 2015, Distilling the knowledge in a neural network, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_85",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_86@0",
            "content": "UNKNOWN, None, 2010, Characterising bias in compressed models. CoRR, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_86",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_87@0",
            "content": "UNKNOWN, None, 2019, Distilling bert for natural language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_87",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_88@0",
            "content": "Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, Xiaolin Hu, Knowledge distillation via route constrained optimization, 2019, ICCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_88",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_89@0",
            "content": "Jack Kiefer, Jacob Wolfowitz, Stochastic estimation of the maximum of a regression function, 1952, The Annals of Mathematical Statistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_89",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_90@0",
            "content": "UNKNOWN, None, 2018, Paraphrasing complex network: Network compression via factor transfer, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_90",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_91@0",
            "content": "Taehyeon Kim, Jaehoon Oh, Nakyil Kim, Sangwook Cho, Se-Young Yun, Comparing kullbackleibler divergence and mean squared error loss in knowledge distillation, 2021, IJCAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_91",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_92@0",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_92",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_93@0",
            "content": "UNKNOWN, None, 2009, Learning multiple layers of features from tiny images, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_93",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_94@0",
            "content": "Benlin Liu, Yongming Rao, Jiwen Lu, Jie Zhou, Cho-Jui Hsieh, Metadistiller: Network selfboosting via meta-learned top-down distillation, 2020, ECCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_94",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_95@0",
            "content": "Hanxiao Liu, Karen Simonyan, Yiming Yang, DARTS: differentiable architecture search, 2019, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_95",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_96@0",
            "content": "Michael Mccloskey, J Neal,  Cohen, Catastrophic interference in connectionist networks: The sequential learning problem, 1989, Psychology of learning and motivation, Elsevier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_96",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_97@0",
            "content": "Mehrdad Seyed-Iman Mirzadeh, Ang Farajtabar, Nir Li, Akihiro Levine, Hassan Matsukawa,  Ghasemzadeh, Improved knowledge distillation via teacher assistant, 2020, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_97",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_98@0",
            "content": "UNKNOWN, None, 2020, Meta-kd: A meta knowledge distillation framework for language model compression across domains, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_98",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_99@0",
            "content": "UNKNOWN, None, 2021, Learning studentfriendly teacher networks for knowledge distillation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_99",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_100@0",
            "content": "Wonpyo Park, Dongju Kim, Yan Lu, Minsu Cho, Relational knowledge distillation, 2019, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_100",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_101@0",
            "content": "Nikolaos Passalis, Anastasios Tefas, Learning deep representations with probabilistic knowledge transfer, 2018, ECCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_101",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_102@0",
            "content": "Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao Wu, Jiaheng Liu, Zhaoning Zhang, Yu Liu, Correlation congruence for knowledge distillation, 2019, ICCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_102",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_103@0",
            "content": "UNKNOWN, None, 2020, Meta pseudo labels, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_103",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_104@0",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_104",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_105@0",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Squad: 100, 000+ questions for machine comprehension of text, 2016, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_105",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_106@0",
            "content": "UNKNOWN, None, 2015, Fitnets: Hints for thin deep nets, ICLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_106",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_107@0",
            "content": "UNKNOWN, None, 2019, Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_107",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_108@0",
            "content": "UNKNOWN, None, 2021, Learning from deep model via exploring local targets, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_108",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_109@0",
            "content": "Karen Simonyan, Andrew Zisserman, Very deep convolutional networks for large-scale image recognition, 2015, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_109",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_110@0",
            "content": "K Naresh, Michael Sinha,  Griscik, A stochastic approximation method, 1971, IEEE Trans. Syst. Man Cybern, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_110",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_111@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_111",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_112@0",
            "content": "Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu, Patient knowledge distillation for BERT model compression, 2019, EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_112",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_113@0",
            "content": "Yonglong Tian, Dilip Krishnan, Phillip Isola, Contrastive representation distillation, 2020, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_113",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_114@0",
            "content": "Frederick Tung, Greg Mori, Similarity-preserving knowledge distillation, 2019, ICCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_114",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_115@0",
            "content": "UNKNOWN, None, 2019, Well-read students learn better: The impact of student initialization on knowledge distillation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_115",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_116@0",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, GLUE: A multi-task benchmark and analysis platform for natural language understanding, 2019, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_116",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_117@0",
            "content": "UNKNOWN, None, 2020, Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_117",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_118@0",
            "content": "Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou, Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers, 2020, NeurIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_118",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_119@0",
            "content": "UNKNOWN, None, 2019, Neural network acceptability judgments. TACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_119",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_120@0",
            "content": "Adina Williams, Nikita Nangia, Samuel , Bowman. A broad-coverage challenge corpus for sentence understanding through inference, 2018, NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_120",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_121@0",
            "content": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, EMNLP (Demos), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_121",
            "start": 0,
            "end": 440,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_122@0",
            "content": "Gloria Brown, Wright , Student-centered learning in higher education, 2011, International Journal of Teaching and Learning in Higher Education, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_122",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_123@0",
            "content": "UNKNOWN, None, 2022, A survey on model compression for natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_123",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_124@0",
            "content": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou, Bert-of-theseus: Compressing BERT by progressive module replacing, 2020, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_124",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_125@0",
            "content": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian Mcauley, Furu Wei, Beyond preserved accuracy: Evaluating loyalty and robustness of BERT compression, 2021, EMNLP, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_125",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_126@0",
            "content": "UNKNOWN, None, 2021, A survey on green deep learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_126",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_127@0",
            "content": "Sergey Zagoruyko, Nikos Komodakis, Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer, 2017, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_127",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_128@0",
            "content": "Ying Zhang, Tao Xiang, Timothy Hospedales, Huchuan Lu, Deep mutual learning, 2018, CVPR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_128",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_129@0",
            "content": "UNKNOWN, None, 2020, BERT loses patience: Fast and robust inference with early exit, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_129",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "21-ARR_v2_130@0",
            "content": "UNKNOWN, None, 2021, Improving sequence-to-sequence pre-training via sequence span rewriting, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "21-ARR_v2_130",
            "start": 0,
            "end": 94,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "21-ARR_v2_0",
            "tgt_ix": "21-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_0",
            "tgt_ix": "21-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_1",
            "tgt_ix": "21-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_1",
            "tgt_ix": "21-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_0",
            "tgt_ix": "21-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_2",
            "tgt_ix": "21-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_4",
            "tgt_ix": "21-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_5",
            "tgt_ix": "21-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_7",
            "tgt_ix": "21-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_8",
            "tgt_ix": "21-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_3",
            "tgt_ix": "21-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_3",
            "tgt_ix": "21-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_3",
            "tgt_ix": "21-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_3",
            "tgt_ix": "21-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_3",
            "tgt_ix": "21-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_3",
            "tgt_ix": "21-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_3",
            "tgt_ix": "21-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_0",
            "tgt_ix": "21-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_9",
            "tgt_ix": "21-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_10",
            "tgt_ix": "21-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_10",
            "tgt_ix": "21-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_10",
            "tgt_ix": "21-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_10",
            "tgt_ix": "21-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_0",
            "tgt_ix": "21-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_13",
            "tgt_ix": "21-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_14",
            "tgt_ix": "21-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_14",
            "tgt_ix": "21-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_14",
            "tgt_ix": "21-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_15",
            "tgt_ix": "21-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_14",
            "tgt_ix": "21-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_16",
            "tgt_ix": "21-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_18",
            "tgt_ix": "21-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_19",
            "tgt_ix": "21-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_17",
            "tgt_ix": "21-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_17",
            "tgt_ix": "21-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_17",
            "tgt_ix": "21-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_17",
            "tgt_ix": "21-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_14",
            "tgt_ix": "21-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_20",
            "tgt_ix": "21-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_21",
            "tgt_ix": "21-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_21",
            "tgt_ix": "21-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_14",
            "tgt_ix": "21-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_22",
            "tgt_ix": "21-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_14",
            "tgt_ix": "21-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_23",
            "tgt_ix": "21-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_24",
            "tgt_ix": "21-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_24",
            "tgt_ix": "21-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_14",
            "tgt_ix": "21-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_27",
            "tgt_ix": "21-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_28",
            "tgt_ix": "21-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_29",
            "tgt_ix": "21-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_30",
            "tgt_ix": "21-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_31",
            "tgt_ix": "21-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_32",
            "tgt_ix": "21-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_33",
            "tgt_ix": "21-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_34",
            "tgt_ix": "21-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_35",
            "tgt_ix": "21-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_36",
            "tgt_ix": "21-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_37",
            "tgt_ix": "21-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_38",
            "tgt_ix": "21-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_39",
            "tgt_ix": "21-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_0",
            "tgt_ix": "21-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_40",
            "tgt_ix": "21-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_41",
            "tgt_ix": "21-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_41",
            "tgt_ix": "21-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_43",
            "tgt_ix": "21-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_44",
            "tgt_ix": "21-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_45",
            "tgt_ix": "21-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_46",
            "tgt_ix": "21-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_42",
            "tgt_ix": "21-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_42",
            "tgt_ix": "21-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_42",
            "tgt_ix": "21-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_42",
            "tgt_ix": "21-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_42",
            "tgt_ix": "21-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_42",
            "tgt_ix": "21-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_41",
            "tgt_ix": "21-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_47",
            "tgt_ix": "21-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_48",
            "tgt_ix": "21-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_48",
            "tgt_ix": "21-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_0",
            "tgt_ix": "21-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_49",
            "tgt_ix": "21-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_50",
            "tgt_ix": "21-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_50",
            "tgt_ix": "21-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_52",
            "tgt_ix": "21-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_53",
            "tgt_ix": "21-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_54",
            "tgt_ix": "21-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_51",
            "tgt_ix": "21-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_51",
            "tgt_ix": "21-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_51",
            "tgt_ix": "21-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_51",
            "tgt_ix": "21-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_51",
            "tgt_ix": "21-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_50",
            "tgt_ix": "21-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_55",
            "tgt_ix": "21-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_57",
            "tgt_ix": "21-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_58",
            "tgt_ix": "21-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_59",
            "tgt_ix": "21-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_60",
            "tgt_ix": "21-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_56",
            "tgt_ix": "21-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_56",
            "tgt_ix": "21-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_56",
            "tgt_ix": "21-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_56",
            "tgt_ix": "21-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_56",
            "tgt_ix": "21-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_56",
            "tgt_ix": "21-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_50",
            "tgt_ix": "21-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_61",
            "tgt_ix": "21-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_62",
            "tgt_ix": "21-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_62",
            "tgt_ix": "21-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_0",
            "tgt_ix": "21-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_63",
            "tgt_ix": "21-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_65",
            "tgt_ix": "21-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_66",
            "tgt_ix": "21-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_67",
            "tgt_ix": "21-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_68",
            "tgt_ix": "21-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_69",
            "tgt_ix": "21-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_71",
            "tgt_ix": "21-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_72",
            "tgt_ix": "21-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_70",
            "tgt_ix": "21-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "21-ARR_v2_0",
            "tgt_ix": "21-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_1",
            "tgt_ix": "21-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_2",
            "tgt_ix": "21-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_2",
            "tgt_ix": "21-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_2",
            "tgt_ix": "21-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_2",
            "tgt_ix": "21-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_2",
            "tgt_ix": "21-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_3",
            "tgt_ix": "21-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_4",
            "tgt_ix": "21-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_4",
            "tgt_ix": "21-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_4",
            "tgt_ix": "21-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_5",
            "tgt_ix": "21-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_5",
            "tgt_ix": "21-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_5",
            "tgt_ix": "21-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_5",
            "tgt_ix": "21-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_5",
            "tgt_ix": "21-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_5",
            "tgt_ix": "21-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_5",
            "tgt_ix": "21-ARR_v2_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_5",
            "tgt_ix": "21-ARR_v2_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_6@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_6",
            "tgt_ix": "21-ARR_v2_6@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_7",
            "tgt_ix": "21-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_8",
            "tgt_ix": "21-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_9",
            "tgt_ix": "21-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_9",
            "tgt_ix": "21-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_9",
            "tgt_ix": "21-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_9",
            "tgt_ix": "21-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_9",
            "tgt_ix": "21-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_9",
            "tgt_ix": "21-ARR_v2_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_10",
            "tgt_ix": "21-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_11",
            "tgt_ix": "21-ARR_v2_11@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_12",
            "tgt_ix": "21-ARR_v2_12@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_13",
            "tgt_ix": "21-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_13",
            "tgt_ix": "21-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_13",
            "tgt_ix": "21-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_13",
            "tgt_ix": "21-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_13",
            "tgt_ix": "21-ARR_v2_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_13",
            "tgt_ix": "21-ARR_v2_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_14",
            "tgt_ix": "21-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_15",
            "tgt_ix": "21-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_15",
            "tgt_ix": "21-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_15",
            "tgt_ix": "21-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_15",
            "tgt_ix": "21-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_16",
            "tgt_ix": "21-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_17",
            "tgt_ix": "21-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_18",
            "tgt_ix": "21-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_18",
            "tgt_ix": "21-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_19",
            "tgt_ix": "21-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_20",
            "tgt_ix": "21-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_20",
            "tgt_ix": "21-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_20",
            "tgt_ix": "21-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_20",
            "tgt_ix": "21-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_20",
            "tgt_ix": "21-ARR_v2_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_21",
            "tgt_ix": "21-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_22",
            "tgt_ix": "21-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_22",
            "tgt_ix": "21-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_22",
            "tgt_ix": "21-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_22",
            "tgt_ix": "21-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_22",
            "tgt_ix": "21-ARR_v2_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_22",
            "tgt_ix": "21-ARR_v2_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_23",
            "tgt_ix": "21-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_24",
            "tgt_ix": "21-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_25",
            "tgt_ix": "21-ARR_v2_25@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_26",
            "tgt_ix": "21-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_27",
            "tgt_ix": "21-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_27",
            "tgt_ix": "21-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_28",
            "tgt_ix": "21-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_29",
            "tgt_ix": "21-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_30",
            "tgt_ix": "21-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_31",
            "tgt_ix": "21-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_32",
            "tgt_ix": "21-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_33",
            "tgt_ix": "21-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_34",
            "tgt_ix": "21-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_34",
            "tgt_ix": "21-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_34",
            "tgt_ix": "21-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_35",
            "tgt_ix": "21-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_35",
            "tgt_ix": "21-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_35",
            "tgt_ix": "21-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_36",
            "tgt_ix": "21-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_37",
            "tgt_ix": "21-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_38",
            "tgt_ix": "21-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_38",
            "tgt_ix": "21-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_38",
            "tgt_ix": "21-ARR_v2_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_39",
            "tgt_ix": "21-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_40",
            "tgt_ix": "21-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_40",
            "tgt_ix": "21-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_40",
            "tgt_ix": "21-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_40",
            "tgt_ix": "21-ARR_v2_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_40",
            "tgt_ix": "21-ARR_v2_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_40",
            "tgt_ix": "21-ARR_v2_40@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_40",
            "tgt_ix": "21-ARR_v2_40@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_41",
            "tgt_ix": "21-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_42",
            "tgt_ix": "21-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_43",
            "tgt_ix": "21-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_44",
            "tgt_ix": "21-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_44",
            "tgt_ix": "21-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_44",
            "tgt_ix": "21-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_44",
            "tgt_ix": "21-ARR_v2_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_44",
            "tgt_ix": "21-ARR_v2_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_44",
            "tgt_ix": "21-ARR_v2_44@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_44",
            "tgt_ix": "21-ARR_v2_44@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_45",
            "tgt_ix": "21-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_45",
            "tgt_ix": "21-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_45",
            "tgt_ix": "21-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_46",
            "tgt_ix": "21-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_46",
            "tgt_ix": "21-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_46",
            "tgt_ix": "21-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_46",
            "tgt_ix": "21-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_46",
            "tgt_ix": "21-ARR_v2_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_46",
            "tgt_ix": "21-ARR_v2_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_46",
            "tgt_ix": "21-ARR_v2_46@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_46",
            "tgt_ix": "21-ARR_v2_46@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_47",
            "tgt_ix": "21-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_47",
            "tgt_ix": "21-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_47",
            "tgt_ix": "21-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_47",
            "tgt_ix": "21-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_47",
            "tgt_ix": "21-ARR_v2_47@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_48",
            "tgt_ix": "21-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_49",
            "tgt_ix": "21-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_49",
            "tgt_ix": "21-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_49",
            "tgt_ix": "21-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_49",
            "tgt_ix": "21-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_49",
            "tgt_ix": "21-ARR_v2_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_49",
            "tgt_ix": "21-ARR_v2_49@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_49",
            "tgt_ix": "21-ARR_v2_49@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_49",
            "tgt_ix": "21-ARR_v2_49@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_50",
            "tgt_ix": "21-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_51",
            "tgt_ix": "21-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_52",
            "tgt_ix": "21-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_52",
            "tgt_ix": "21-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_53",
            "tgt_ix": "21-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_53",
            "tgt_ix": "21-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_54",
            "tgt_ix": "21-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_54",
            "tgt_ix": "21-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_54",
            "tgt_ix": "21-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_55",
            "tgt_ix": "21-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_55",
            "tgt_ix": "21-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_56",
            "tgt_ix": "21-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_57",
            "tgt_ix": "21-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_57",
            "tgt_ix": "21-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_57",
            "tgt_ix": "21-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_58",
            "tgt_ix": "21-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_58",
            "tgt_ix": "21-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_58",
            "tgt_ix": "21-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_59",
            "tgt_ix": "21-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_60",
            "tgt_ix": "21-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_60",
            "tgt_ix": "21-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_61",
            "tgt_ix": "21-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_61",
            "tgt_ix": "21-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_61",
            "tgt_ix": "21-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_62",
            "tgt_ix": "21-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_63",
            "tgt_ix": "21-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_63",
            "tgt_ix": "21-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_63",
            "tgt_ix": "21-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_63",
            "tgt_ix": "21-ARR_v2_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_63",
            "tgt_ix": "21-ARR_v2_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_63",
            "tgt_ix": "21-ARR_v2_63@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_64",
            "tgt_ix": "21-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_65",
            "tgt_ix": "21-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_66",
            "tgt_ix": "21-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_67",
            "tgt_ix": "21-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_67",
            "tgt_ix": "21-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_67",
            "tgt_ix": "21-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_68",
            "tgt_ix": "21-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_68",
            "tgt_ix": "21-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_68",
            "tgt_ix": "21-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_68",
            "tgt_ix": "21-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_68",
            "tgt_ix": "21-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_68",
            "tgt_ix": "21-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_68",
            "tgt_ix": "21-ARR_v2_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_69",
            "tgt_ix": "21-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_69",
            "tgt_ix": "21-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_69",
            "tgt_ix": "21-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_69",
            "tgt_ix": "21-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_69",
            "tgt_ix": "21-ARR_v2_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_70",
            "tgt_ix": "21-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_70",
            "tgt_ix": "21-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_70",
            "tgt_ix": "21-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_70",
            "tgt_ix": "21-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_70",
            "tgt_ix": "21-ARR_v2_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_70",
            "tgt_ix": "21-ARR_v2_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_71",
            "tgt_ix": "21-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_71",
            "tgt_ix": "21-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_71",
            "tgt_ix": "21-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_72",
            "tgt_ix": "21-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_72",
            "tgt_ix": "21-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_72",
            "tgt_ix": "21-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_72",
            "tgt_ix": "21-ARR_v2_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_73",
            "tgt_ix": "21-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_73",
            "tgt_ix": "21-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_74",
            "tgt_ix": "21-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_75",
            "tgt_ix": "21-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_76",
            "tgt_ix": "21-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_77",
            "tgt_ix": "21-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_78",
            "tgt_ix": "21-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_79",
            "tgt_ix": "21-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_80",
            "tgt_ix": "21-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_81",
            "tgt_ix": "21-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_82",
            "tgt_ix": "21-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_83",
            "tgt_ix": "21-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_84",
            "tgt_ix": "21-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_85",
            "tgt_ix": "21-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_86",
            "tgt_ix": "21-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_87",
            "tgt_ix": "21-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_88",
            "tgt_ix": "21-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_89",
            "tgt_ix": "21-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_90",
            "tgt_ix": "21-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_91",
            "tgt_ix": "21-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_92",
            "tgt_ix": "21-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_93",
            "tgt_ix": "21-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_94",
            "tgt_ix": "21-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_95",
            "tgt_ix": "21-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_96",
            "tgt_ix": "21-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_97",
            "tgt_ix": "21-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_98",
            "tgt_ix": "21-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_99",
            "tgt_ix": "21-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_100",
            "tgt_ix": "21-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_101",
            "tgt_ix": "21-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_102",
            "tgt_ix": "21-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_103",
            "tgt_ix": "21-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_104",
            "tgt_ix": "21-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_105",
            "tgt_ix": "21-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_106",
            "tgt_ix": "21-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_107",
            "tgt_ix": "21-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_108",
            "tgt_ix": "21-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_109",
            "tgt_ix": "21-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_110",
            "tgt_ix": "21-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_111",
            "tgt_ix": "21-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_112",
            "tgt_ix": "21-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_113",
            "tgt_ix": "21-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_114",
            "tgt_ix": "21-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_115",
            "tgt_ix": "21-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_116",
            "tgt_ix": "21-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_117",
            "tgt_ix": "21-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_118",
            "tgt_ix": "21-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_119",
            "tgt_ix": "21-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_120",
            "tgt_ix": "21-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_121",
            "tgt_ix": "21-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_122",
            "tgt_ix": "21-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_123",
            "tgt_ix": "21-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_124",
            "tgt_ix": "21-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_125",
            "tgt_ix": "21-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_126",
            "tgt_ix": "21-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_127",
            "tgt_ix": "21-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_128",
            "tgt_ix": "21-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_129",
            "tgt_ix": "21-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "21-ARR_v2_130",
            "tgt_ix": "21-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 893,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "21-ARR",
        "version": 2
    }
}