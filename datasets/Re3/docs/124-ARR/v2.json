{
    "nodes": [
        {
            "ix": "124-ARR_v2_0",
            "content": "MReD: A Meta-Review Dataset for Structure-Controllable Text Generation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_2",
            "content": "When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited. A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences. A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with a deep understanding of the domain knowledge. Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc. We present experimental results on start-of-the-art summarization models, and propose methods for structurecontrolled generation with both extractive and abstractive models using our annotated data. By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain. 1 * * Equally Contributed. \u2020 Chenhui, Liying, and Ran are under the Joint PhD Program between Alibaba and their corresponding universities.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_3",
            "content": "\u2021 \u2021 Corresponding author. 1 Our code and data are released at https://github. com/Shen-Chenhui/MReD.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_4",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "124-ARR_v2_5",
            "content": "Text generation entered a new era because of the development of neural network based generation techniques. Along the dimension of the mapping relation between the input information and the output text, we can roughly group the recent tasks meta-review: [This paper studies n-step returns in off-policy RL and introduces a novel algorithm which adapts the return's horizon n in function of a notion of policy's age.]\u2190ABSTRACT [Over-all, the reviewers found that the paper presents interesting observations and promising experimental results.]\u2190STRENGTH [However, they also raised concerns in their initial reviews, regarding the clarity of the paper, its theoretical foundations and its positioning (notably regarding the bias/variance tradeoff of uncorrected n-step returns) and parts of the experimental results. ]\u2190WEAKNESS [In the absence of rebuttal or revised manuscript from the authors, not much discussion was triggered.]\u2190REBUTTAL PROCESS [Based on the initial reviews, the AC cannot recommend accepting this paper, but the authors are encouraged to pursue this interesting research direction.]\u2190DECISION Table 1: An example of annotated meta-review. CATE-GORY indicates the category of each sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_6",
            "content": "into three clusters: more-to-less, less-to-more, and neck-to-neck. The more-to-less text generation tasks output a concise piece of text from some more abundant input, such as text summarization (Tan et al., 2017;Kry\u015bci\u0144ski et al., 2018). The lessto-more generation tasks generate a more abundant output from some obviously simpler input, such as prompt-based story generation (Fan et al., 2018b). The neck-to-neck generation aims at generating an output text which conveys the same quantity of knowledge as the input but in natural language, such as typical RDF triples to text tasks (Gardent et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_7",
            "content": "To some extent, the existing task settings are not so adequate because they do not have a deep understanding of the domains they are working on, i.e., domain knowledge. Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail (Nallapati et al., 2016) is composed of the training pairs of news content and human-written summary bullets. However, it does not tell why a particular piece of news content should have that corresponding summary, for example for the same earnings report, why one media emphasizes its new business success in the summary, but another emphasizes its net income. Obviously, there is not a standard answer regarding right or wrong. For such cases, if we can specify a control signal, e.g., \"emphasizing new business\", the generated text would make more sense to users using the text generator.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_8",
            "content": "To allow controlling not only the intent of a single generated sentence but also the whole structure of a generated passage, we prepare a new dataset MReD (short for Meta-Review Dataset) with in-depth understanding of the structure of meta-reviews in a peer-reviewing system, namely the open review system of ICLR. MReD for the first time allows a generator to be trained by simultaneously taking the text (i.e. reviews) and the structure control signal as input to generate a meta-review which is not only derivable from the reviews but also complies with the control intent. Thus from the same input text, the trained generator can generate varied outputs according to the given control signals. For example, if the area chair is inclined to accept a borderline paper, he or she may invoke our generator with a structure of \"abstract | strength | decision\" to generate a meta-review, or may use a structure of \"abstract | weakness | suggestion\" otherwise. Note that for ease of preparation and explanation, we ground our dataset in the peer review domain. However, the data preparation methodology and proposed models are transferable to other domains, which is indeed what we hope to motivate with this effort. Specifically, we collect 7,089 meta-reviews of ICLR in recent years (2018 -2021) and fully annotate the dataset. Each sentence in a meta-review is classified into one of the 9 pre-defined intent categories: abstract, strength, weakness, rating summary, area chair (AC) disagreement, rebuttal process, suggestion, decision, and miscellaneous (misc). Table 1 shows an annotated example, where each sentence is classified into a single category that best describes the intent of this sentence. Our MReD is obviously different from the previous text generation/summarization datasets because, given the rich annotations of individual meta-review sentences, a model is allowed to learn more sophisticated generation behaviors to control the structure of the generated passage. Our proposed task is also noticeably different from the existing controllable text generation tasks (e.g., text style transfer on sentiment polarity (Shen et al., 2017;Liao et al., 2018) and formality (Shang et al., 2019)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_9",
            "content": "To summarize, our contributions are as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_10",
            "content": "(1) We introduce a fully-annotated meta-review dataset to make better use of the domain knowledge for text generation. With thorough data analysis, we derive useful insights into the domain characteristics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_11",
            "content": "(2) We propose a new task of controllable generation focusing on controlling the passage macro structures. It offers stronger generation flexibility and applicability for practical use cases.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_12",
            "content": "(3) We design simple yet effective control methods that are independent of the model architecture. We show the effectiveness of enforcing different generation structures with a detailed model analysis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_13",
            "content": "MReD: Meta-Review Dataset",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "124-ARR_v2_14",
            "content": "In this paper, we explore a new task, named the structure-controllable text generation, in a new domain, namely the meta-reviews in the peerreviewing system. Unlike the previous datasets that mainly focus on domains like news, the domain for meta-reviews is worth-studying because it contains essential and high-density opinions. Specifically, during the peer review process of scientific papers, a senior reviewer or area chair will recommend a decision and manually write a meta-review to summarize the opinions from different reviews written by the reviewers. We first introduce the data collection process and then describe the annotation details, followed by dataset analysis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_15",
            "content": "Data Collection",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "124-ARR_v2_16",
            "content": "We collect the meta-review related data of ICLR from an online peer-reviewing platform, i.e., Open-Review 2 from 2018 to 2021. Note that the submissions from earlier years are not collected because their meta-reviews are not released. To prepare our dataset for controllable text generation, for each submission, we collect all of its corresponding official reviews with reviewer ratings and confidence scores, the final meta-review decision, and the meta-review passage. Table 2 shows the statistics of data collected from each year. Initially, 7,894 submissions are collected. After filtering, 7,089 meta-reviews are retained with their corresponding 23,675 reviews. Note that even without any further annotation, the dataset can already naturally serve the purpose of multi-document summarization (MDS). Compared with those conventional datasets for MDS, such as TAC (Owczarzak and Dang, 2011) and DUC (Over and Yen, 2004), which contain in total a few hundred input articles (equivalent to reviews in MReD), our dataset is more than 10 times larger.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_17",
            "content": "Data Annotation",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "124-ARR_v2_18",
            "content": "As aforementioned, the structure-controllable text generation aims at controlling the structure of the generated passage. Therefore, we need to comprehensively understand the structures of metareviews so as to enable a model to learn how to generate outputs complying with certain structures. Specifically, based on the nature of meta-reviews, we pre-define 9 intent categories: abstract, strength, weakness, suggestion, rebuttal process, rating summary, area chair (AC) disagreement, decision, and miscellaneous (misc). Table 3 shows the definition for each category (see example sentences in Appendix A.1). The identification of category for some sentences is fairly straightforward, while some sentences are relatively ambiguous. Therefore, besides following the definition of each category, the annotators are also required to follow the additional rules as elaborated in Appendix A.2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_19",
            "content": "For conducting the annotation work, 14 professional data annotators from a data company are initially trained, and 12 of them are selected for the task according to their annotation quality during a trial round. These 12 annotators are fully paid for their work. Each meta-review sentence is independently labeled by 2 different annotators, and a third expert annotator resolves any disagreement between the first two annotators. We label 45,929 sentences from 7,089 meta-reviews in total, and the Cohen's kappa is 0.778 between the first two annotators, showing that the annotation is of quite high quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_20",
            "content": "Data Analysis",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "124-ARR_v2_21",
            "content": "To better understand the MReD dataset, we conduct the following analysis along different dimensions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_22",
            "content": "Sentence distribution across categories. The number of sentences in different categories are shown in Figure 1, breakdown by the decision (i.e., accept or reject). Among 7,089 submissions, there are 2,368 accepted and 4,721 rejected. Among all submissions and the rejected submissions, \"weakness\" accounts for the largest proportion, while across the accepted ones, \"abstract\" and \"strength\" take up a great proportion. To some extent, these three categories which dominate in meta-reviews could be easily summarized from the reviewers' comments. However, some minor or subjective categories (e.g., \"ac disagreement\") are hard to generate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_23",
            "content": "Breakdown analysis by meta-review lengths and average rating scores. We present the percentage of meta-reviews of different lengths in each score range, as shown in Figure 2. For example, among the meta-reviews that receive the reviewers' average score below 2 (i.e., the first column in the figure), 28% are less than or equal to 50 words, and 38% fall in the length range of 51 to 100 words. We can observe that the meta-reviews tend to be longer for those submissions receiving scores in the middle range, while shorter for those with lower scores or higher scores. This coincides with our commonsense that for high-score and low-score submissions, the decision tends to be a clear accept or reject so that meta-reviews can be relatively shorter, while for those borderline submissions, area chairs have to carefully weigh the pros and cons to make the final decision (see Appendix B.1 for borderline submission analysis). As shown in Figure 3, the meta-reviews with more than 150 words generally have a larger proportion of sentences describing \"weakness\" and \"suggestion\" for authors to improve the submissions. Additional analysis on the category breakdown for accepted and rejected papers across the score ranges is shown in Appendix B.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_24",
            "content": "Meta-review patterns. To study the common structures of meta-reviews, we present the transition matrix of different category segments in Figure 4, where the sum of each row is 1. Note that each segment represents the longest consecutive sentences with the same category. We add \"<start>\" and \"<end>\" tokens before and after each metareview accordingly to investigate which categories tend to be at the start/end of the meta-reviews. It is clear to see that \"abstract\" usually positions at the beginning of the meta-review, while \"suggestion\" and \"decision\" usually appear at the end. There are also some clear patterns appearing in the metareviews, such as \"abstract | strength | weakness\", \"rating summary | weakness | rebuttal process\", and \"abstract | weakness | decision\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_25",
            "content": "3 Structure-Controllable Text Generation",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_26",
            "content": "Task Definition",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "124-ARR_v2_27",
            "content": "As aforementioned, in uncontrolled generation, users cannot instruct the model to emphasize on desired aspects. However, in a domain such as meta-reviews, given the same review inputs, one AC may emphasize more on the \"strength\" of the paper following a structure of \"abstract | strength | .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 decision\", whereas another AC may prefer a different structure with more focus on reviewers' opinions and suggestions (i.e., \"rating summary\" and \"suggestion\"). To achieve such flexibility, the task of structure-controllable text generation is defined as: given the text input (i.e., reviews) and a control sequence of the output structure, a model should generate a meta-review that is derivable from the reviews and presents the required structure.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_28",
            "content": "Explored Methods",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "124-ARR_v2_29",
            "content": "As the recent generation works (Vaswani et al., 2017;Liu and Lapata, 2019;Xing et al., 2020) basically adopt an encoder-decoder based architecture and achieve state-of-the-art performance on many tasks and datasets, we primarily investigate the performance of such a framework on our task. Thus in this subsection, we mainly present how to re-organize the input reviews and the control structure as an input sequence of the encoder. We also explore other baselines in the experiments later.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_30",
            "content": "In order to summarize multiple reviews into a meta-review showing a required structure, we explicitly specify the control label sequence that a model should comply with during generation. Specifically, we intuitively add the control sequence in front of the input text. By directly combining both the control and textual information as a single input, our control method is independent of any specially designed encoder and decoder structures. Moreover, by placing the short control sequence in front, an encoder can immediately observe the control signal at the very beginning, thus avoids the possible interference by the subsequent sequence. Moreover, the control sequence in front will never be truncated when the encoder truncates the input to a certain length limit.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_31",
            "content": "Given the multiple review inputs, we need to linearize them into a single input. One simple method, concat, is to concatenate all inputs one after another (Fabbri et al., 2019). Besides the text inputs, the review rating, which cannot be found in the review passages but exists in the field of rating score, is also crucial information for writing meta-reviews. Therefore, we create a rating sentence that consists of the extracted ratings given by the corresponding reviewers and prepend it to our concatenated review texts to obtain the final input. We name this method rate-concat (see Table 4, upper). We also explore an alternative method, merge, as follows: From all review inputs, we use the longest one as a backbone. We segment all reviews' content on a paragraph level, and encode them using Sen-tenceTransformers (Reimers and Gurevych, 2019). Then, for each paragraph embedding in the nonbackbone reviews, we calculate a cosine similarity score with each backbone paragraph embedding. We then insert each non-backbone paragraph after the backbone paragraph with which it has the highest similarity score. We repeat the process for all paragraphs in non-backbone reviews to obtain a single passage. We further add rating sentences in front of the results of merge to obtain rate-merge. Additionally, we provide a longest-review baseline, which does not combine reviews but only uses the longest review as the input.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_32",
            "content": "As aforementioned, we place the control sequence in front of the re-organized review information. Specifically, we explore two different control methods, namely, sent-ctrl and seg-ctrl. Sent-ctrl uses one control label per target sentence and controls generation on the sentence-level. Note that this method can allow implicit control on the length (i.e., number of sentences) of the generation. Segctrl treats consecutive sentences of the same label as one segment and only uses one label for a single segment. Example inputs of different control settings are shown in Table 4 (lower). For instance, sent-ctrl repeats \"abstract\" in its control sequence whereas seg-ctrl does not. This is because seg-ctrl treats the 1 st and 2 nd target sentences of \"abstract\" as the same segment and only uses a single label to indicate it in the sequence. Additionally, we provide a vanilla setting for uncontrolled generation, unctrl, where no control sequence is used.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_33",
            "content": "Using the above input sequence as the source and the corresponding meta-review as the target, we can train an encoder-decoder model for controllable generation. Many transformer-based models have achieved state-of-the-art performance. Common abstractive summarization models include BART (Lewis et al., 2020), T5 (Raffel et al., 2020) and PEGASUS (Zhang et al., 2020). In this paper we focus on the bart-large-cnn model, one variant of the BART model (results on other pretrained models can be found in Appendix C.1, which show similar trend). More specifically, we use the Py-Torch implementation in the open-source library Hugging Face Transformers (Wolf et al., 2020) and its hosted pretrained models 3 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_34",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "124-ARR_v2_35",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "124-ARR_v2_36",
            "content": "Extractive Baselines. We employ three common extractive summarization baselines each of which basically provides a mechanism to rank the input sentences. LexRank (Erkan and Radev, 2004) represents sentences in a graph and uses eigenvector centrality to calculate sentence importance scores. TextRank (Mihalcea and Tarau, 2004) is another graph-based sentence ranking method that obtains vertex scores by running a \"random-surfer model\" until convergence. MMR (Carbonell and Goldstein, 1998) calculates sentence scores by balancing the redundancy score with the information relevance score. After ranking with each of the above models, we select sentences as output with different strategies according to the controlled and uncontrolled settings. For the uncontrolled setting, we simply select the top k sentences as the generated output, where k is a hyperparameter deciding the size of the generated output. For the controlled setting, we select only the top sentences with the right category labels according to the control sequence. To do so, we employ an LSTM-CRF (Lample et al., 2016) tagger trained on the labeled meta-reviews to predict the labels of each input review sentence. Refer to Appendix C.2 for more details of the tagger.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_37",
            "content": "Generic Sentence Baselines. Considering the nature of meta-reviews, we could imagine some categories may have common phrases inflating the Rouge scores, such as \"This paper proposes ...\" for abstract, and \"I recommend acceptance.\" for decision, etc. To examine such impact, we select sentences that are generic in each category and combine these sentences to generate outputs according to the control sequences. For instance, if the control sequence is \"abstract | strength | decision\", we take the most generic sentences from the categories of \"abstract\", \"strength\" and \"decision\" respectively to form the output. Specifically, we create two generic sentence baselines by obtaining generic sentences from the training data from either the meta-review references (i.e., target) or the input reviews (i.e., source), namely \"Target Generic\" and \"Source Generic\". Moreover, we also study such impact on the high-score and low-score submissions respectively, since an AC may write more succinct meta-reviews for clear-cut papers, as suggested by Figure 2. See Appendix C.3 for more details and results on generic sentence baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_38",
            "content": "Experimental Setting",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "124-ARR_v2_39",
            "content": "To conduct text generation experiments, we preprocess our MReD dataset by filtering to ensure the selected meta-reviews have 20 to 400 words, as certain meta-review passages are extremely short or long. After preprocessing, we obtain 6,693 sourcetarget pairs, for which we randomly split into train, validation, and test sets by a ratio of 8:1:1. We evaluate our generated outputs against the reference meta-reviews using the F 1 scores of ROUGE ROUGE 2 , and ROUGE L (Lin, 2004) 4 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_40",
            "content": "For the extractive and generic baselines, a key hyperparameter is the sentence number k. Recall that under the sent-ctrl setting, the control sequence length is the same as the sentence number of the target meta-review. Therefore, to conduct a fair comparison, we set the hyperparameter k equal to the number of labels in the control sequence for both controlled and uncontrolled extractive baselines, and sent-ctrl is used for all controlled extractive baselines. We also adopt the same k for the generic baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_41",
            "content": "For bart-large-cnn, we first load the pretrained model and then fine-tune it on MReD. All experiments are conducted on single V100 GPUs, using a batch size of 1 in order to fit the large pretrained model on a single GPU. During fine-tuning, we set the hyperparameters of \"minimum_target_length\" to 20, and \"maximum_target_length\" to 400, according to our filter range on the meta-review lengths. Due to long inputs (see Table 17), we experiment with different source truncation lengths of 1024, 2048, and 3072 tokens. We cannot explore truncation length of more than 3072 tokens due to the limitation of GPU space. Our learning rate is 5e-5, and we use Adam optimizer with momentum \u03b2 1 = 0.9, \u03b2 2 = 0.999 without any warm-up steps or weight decay. We set the seed to be 0, and train the model for 3 epochs with gradient accumulation step of 1. For decoding, we use a beam size of 4 and length penalty of 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_42",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "124-ARR_v2_43",
            "content": "We show results in Table 5. Only the best settings of rate-concat ( Section 4.4) and input truncation of 2048 tokens (Appendix C.4) for bart-large-cnn are included. Amongst the extractive baselines, TextRank performs the best in both unctrl and sentctrl settings. Nevertheless, all controlled methods outperform their unctrl settings (same for the Transformers). This validates our intuition that structure-controlled generation is more suitable for user-subjective writings such as meta-reviews, because the model can better satisfy different structure requirements when supplied with the corresponding control sequences. On the other hand, for bart-large-cnn, sent-ctrl is the best, followed by seg-ctrl. This is most likely due to the former's more fine-grained sentence-level control that provides a clearer structure outline, as compared to the coarser segment-level control.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_44",
            "content": "Moreover, bart-large-cnn far outperforms the extractive baselines, showing that the extractionbased methods are insufficient for MReD. This also suggests that meta-review writings are different from the input reviews, therefore copying full review sentences to form meta-reviews doesn't work well. This is also validated by the \"Target Generic\" baseline's consistent improvement over the \"Source Generic\" baseline, which shows that generic sentences from meta-reviews can suit generation better than those in reviews. Nevertheless, all Transformers results are still much better than the \"Target Generic\" sentence baseline, showing that despite generic phrases in some categories contributing to Rouge, the Transformers model is capable of capturing content-specific information for each input.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_45",
            "content": "Review Combination Results",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "124-ARR_v2_46",
            "content": "We also show uncontrolled generation results for different review combination methods in Table 6, with source truncation of 2048. The longest-review setting has the worst performance, thus validating that the review combination methods are necessary in order not to omit important information. Rateconcat has the best overall performance, which is the setting we used for the main results. Never- theless, it is not significantly better than merge. It is also interesting to see that for merge, providing additional rating information (rate-merge) slightly worsens the performance. We will leave the investigation of better review combination methods for future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_47",
            "content": "R 1 R 2 R L longest-",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_48",
            "content": "Case Study",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "124-ARR_v2_49",
            "content": "We study some cases for a better understanding of the structure-controllable generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_50",
            "content": "Identify the control label for each sentence. We first evaluate whether the model is able to attend to the correct control label during generation. For each generation step, we obtain the cross attention weights from the decoder's output token towards the control labels, and plot them in Figure 5. The given control sequence is \"abstract | weakness | decision\". When generating each sentence, we can see that the attention weights of the corresponding control token are the highest, which demonstrates that our model can effectively pay attention to the correct control label and thus generate the content complying with the intent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_51",
            "content": "To understand what information the model attends to when generating each sentence, we aggregate the cross attention weights to obtain the attention scores from each generated sentence towards all input sentences (Appendix C.5). Then, we select the top 3 input sentences with the highest attention scores for each generated sentence, and visualize the normalized attention weights on all tokens in",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_52",
            "content": "Sent 1 (abstract):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_53",
            "content": "This paper proposes a selfsupervised contrastive learning method for few-shot learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_54",
            "content": "Sent 2 (weakness):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_55",
            "content": "The reviewers agree that the idea is interesting, but have concerns about the clarity of the paper and the lack of comparison to the baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_56",
            "content": "The paper is not suitable for publication at ICLR in its current form.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_57",
            "content": "Table 8: Attention analysis for each output sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_58",
            "content": "the selected sentences and the control sequence in Table 8. As shown, the model can correctly extract relevant information from the source sentences. For example, it identifies important phrases such as \"interesting\", \"clarity\" and \"lack of comparison to baselines\" when generating \"Sent 2\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_59",
            "content": "Generate varied outputs given different control sequences. To further investigate the effectiveness of the control sequence, we change the control sequence of the above example and re-generate the meta-reviews given the same input reviews. In Table 7, we first show the gold meta-review and the model output using the original control sequence in Row 0 and Row 1, and then show the model outputs with alternative control sequences in Row 2 and Row 3. From the outputs, we can see that indeed each generated sentence corresponds to its control label well. In Row 2, we add an additional control label in the sequence and by repeating the \"abstract\" label, the generator can further elaborate more details of the studied method. This is one key advantage of our sent-ctrl compared to the seg-ctrl, which allows the control of length and the level of the generation details. In Row 3, a very comprehensive control sequence is specified. We can see that the output meta-review is quite fluent and polite to reject the borderline paper. See Appendix C.6 for more examples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_60",
            "content": "Human Evaluation",
            "ntype": "title",
            "meta": {
                "section": "4.6"
            }
        },
        {
            "ix": "124-ARR_v2_61",
            "content": "In addition to the Rouge evaluation, we ask 3 human judges to manually assess the generation quality of the bart-large-cnn model trained under different control methods from Table 5 on 100 random test instances. For each test instance, we provide the judges with the input reviews and randomly ordered generations from different models, and ask them to individually evaluate the generations based on the following criteria: (1) Fluency: is the generation fluent, grammatical, and without unnecessary repetitions? (2) Content Relevance: does the generation reflect the review content well, or does it produce general but trivial sentences? (3) Structure Similarity: how close does the generation structure resemble the gold structure (i.e., the control sequence)? ( 4) Decision Correctness: does the generation correctly predicts the gold human decision?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_62",
            "content": "We grade fluency and content relevance on a scale of 1 to 5, whereas structure similarity and decision correctness are calculated from 0 to 1 (Appendix C.7). For structure similarity, because sent-ctrl and seg-ctrl have different control sequences, we evaluate the two models on sentence-level (sent) and segment-level (seg) structures respectively, and provide both evaluations for unctrl.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_63",
            "content": "As shown in Table 9, both sent-ctrl and seg-ctrl models show significant improvements on the generation structure over the uncontrolled baseline, which affirms the effectiveness of our proposed methods for structure-controllable generation. Sentctrl also has better fluency and decision correctness, suggesting that having a better output structure can benefit readability and decision generation. For the content relevance, the scores of all methods are reasonably good, and significance tests cannot prove any best model (p > 0.08). Nevertheless, it is possible that the looser control a method applies, the better relevance score it achieves. It is because a tighter control narrows the content that a model can use from the reviews.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_64",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "124-ARR_v2_65",
            "content": "To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long documents include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016). In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely, structurecontrollable text generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_66",
            "content": "Researchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. Our work is the first fully-annotated dataset in this domain for the structure-controllable generation task. There are also some datasets and annotation schemes on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain and cannot be easily transferred to our task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_67",
            "content": "A wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality , domains (Takeno et al., 2017) and persona ) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)). Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output. Our task also differs from content planning (Reiter and Dale, 1997;Shao et al., 2019;, which involves explicitly selecting and arranging the input content. Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_68",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "124-ARR_v2_69",
            "content": "This paper introduces a fully-annotated text generation dataset MReD in a new domain, i.e., the meta-reviews in the peer review system, and provides thorough data analysis to better understand the data characteristics. With such rich annotations, we propose simple yet effective methods for structure-controllable text generation. Extensive experimental results are presented as baselines for future study and thorough result analysis is conducted to shed light on the control mechanisms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_70",
            "content": "Ethical Concerns",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "124-ARR_v2_71",
            "content": "We have obtained approval from ICLR organizers to use the data collected from ICLR 2018-2021 on OpenReview. having \"rebuttal process\" is larger for submissions with lower scores. This suggests that the rebuttal process plays an important role in the peer review process, especially in helping the borderline papers to be accepted. On the other hand, for rejected papers, the percentage of meta-reviews having \"strength\" increases as the average score increases. This coincides with our common sense that the submissions receiving higher scores tend to have more strengths. One interesting finding here is that the percentage of \"weakness\" and \"suggestion\" also increases as the average rating score increases. This may be due to two main reasons. First, to reject a submission with higher scores, the area chair has to explain the weakness with more details and provide more suggestions for authors to further improve their submissions. Second, compared to the percentage of \"strength\", \"weakness\" definitely has a larger percentage within any range of rating scores. The difference in the percentage of \"strength\" and \"weakness\" is intuitively different between the accepted papers and the rejected papers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_72",
            "content": "We provide baselines of uncontrolled generation and controlled generation on MReD using other common Transformer pretrained models in Table 13. Note that due to limited GPU space, we cannot fit 2048 input tokens for T5. Thus, for fair comparison, all results shown are from source truncation of 1024.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_73",
            "content": "To obtain labels on source input, we train a tagger based on the human-annotated meta-reviews, then use it to predict labels on the input sentences. Specifically, we define the task as a sequence labeling problem and apply the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks with a conditional random field (CRF) (Lafferty et al., 2001) (i.e., LSTM-CRF (Lample et al., 2016)) model on the annotated MReD dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_74",
            "content": "The same data split as the meta-review generation task is used. We adopt the standard IOBES tagging scheme (Ramshaw, 1995;Ratinov and Roth, 2009), and fine-tune BERT (Devlin et al., 2019) and RoBERTa models in Hugging Face. All models are trained for 30 epochs with an early stop of 20, and each epoch takes about 30 minutes. We select the best model parameters based on the best micro F 1 score on the development set and apply it to the test set for evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_75",
            "content": "All models are run with single V100 GPUs. We use Adam (Kingma and Ba, 2014) with an initial learning rate of 2e-5. We report the F 1 scores for each category as well as the overall micro F 1 and macro F 1 scores in Table 14. Micro F1 is the overall accuracy regardless of the categories, whereas macro F1 is an average of per category accuracy evaluation. Since some of the category labels (eg. \"ac disagreement\") are very rare, their classification accuracy is low. Overall, micro F1 is a more important metric since it suggests general performance. The results stand proof that the majority of the categories have their own characteristics that can be identified from other categories. RoBERTabase is the best performing model, therefore we use this model to predict review sentence labels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_76",
            "content": "Besides the baselines of \"Source Generic\" and \"Target Generic\", we explore subsets of papers with high scores (average reviewers' rating \u2a7e 7) or low scores (average reviewers' rating \u2a7d 3) to obtain 4 generic baselines: \"Source High Score\", \"Source Low Score\", \"Target High Score\", \"Target Low Score\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_77",
            "content": "We use \"Target High Score\" as an example to explain how we obtain the generic sentences: From the training subset of high score papers, We first separate all meta-review sentences into the corresponding label categories, obtaining a total of 9 groups of sentences. Then, we re-arrange the sentences in each group using TextRank (our best extractive model). Since TextRank ranks the input sentences based on each sentence's content connection with others, sentences with higher rankings are also more general in the sense that they have more shared content with others.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_78",
            "content": "After obtaining the generic sentence sets, we can create baseline generations using the sent-ctrl sequence on the corresponding high score paper test data. We avoid using the same sentence twice inside the same generation, so if the same label appears multiple times in a control sequence, we will use the same number of generic sentences for that category down the ranking order.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_79",
            "content": "All generic sentence baselines can be obtained in a similarly procedure as outlined above, and we show results in Table 15. Both \"Target High Score\" and \"Target Low Score\" perform much better than the \"Target Genric\" baseline, suggesting that papers with very high or low scores tend to have more typical patterns in their meta-reviews. Nevertheless, the pattern is less evident in the source (reviews) baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_80",
            "content": "By default, the Transformers truncate the source to 1024 tokens. We further investigate the performance of different source truncation lengths under the setting of rate-concat. As shown in Table 18, truncating the source to 2048 tokens consistently achieves the best performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_81",
            "content": "During generation, we can obtain the attention weights of each output token towards all input tokens. Specifically, we average all decoder layers' cross attention weights for the same output token generated at each decoding step. We then calculate an attention value for that output token on each input sentence, by aggregating the token's attention weights on the list of input tokens that belong to the same sentence by max pooling. Finally, we can calculate an output-sentence-to-input-sentence attention score, by adding up these attention values for the output tokens that belong to the same sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_82",
            "content": "Common attention aggregation methods include summation, average-pooling, and max-pooling. We use max-pooling to aggregate attention for same-sentence input tokens, because summation unfairly gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out. With max-pooling, we can correctly identify sentences with spiked attention at important phrases, regardless of sentence lengths. For attention aggregation on the samesentence output tokens, summation is used and can be viewed as allowing each output token to vote an attention score on all input sentences, so that the input sentence receiving the highest total score is the most relevant. We conduct trial runs of all aggregation methods on input tokens with summation for output-token aggregation for multiple generation examples, and indeed max-pooling outperforms the other two by identifying more relevant input sentences with the generated sentence. Once we have the attention scores, we can attribute the generation of each output sentence to a few topmost relevant input sentences. Then, we can draw a color map of the input tokens in the selected sentences based on their relative attention weights.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_83",
            "content": "We show examples of the generation results using alternative control sequences on another submission in Table 16. We can see the effectiveness of controlling the output structure using our proposed method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_84",
            "content": "For structure similarity, we instruct the judges to label each generated sentence with the closest category. We then calculate the normalized token-level edit distance between the judge-annotated label sequence and the given control sequence, where each label is considered as a single token, and finally deduct this value from 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_85",
            "content": "For decision correctness, we evaluate it on a binary scale where 1 indicates complete correctness and 0 otherwise. More specifically, we give 0 if the generation produces either contradictory decisions or a wrong decision, or if the generation does not show enough hints for rejection or acceptance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v2_86",
            "content": "Chaitanya Bhatia, Tribikram Pradhan, Metagen: An academic meta-review generation system, 2020, Proceedings of ACM-SIGIR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Chaitanya Bhatia",
                    "Tribikram Pradhan"
                ],
                "title": "Metagen: An academic meta-review generation system",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACM-SIGIR",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_87",
            "content": "Jaime Carbonell, Jade Goldstein, The use of mmr, diversity-based reranking for reordering documents and producing summaries, 1998, Proceedings of ACM-SIGIR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Jaime Carbonell",
                    "Jade Goldstein"
                ],
                "title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
                "pub_date": "1998",
                "pub_title": "Proceedings of ACM-SIGIR",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_88",
            "content": "Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, Luo Si, Argument pair extraction from peer review and rebuttal via multi-task learning, 2020, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Liying Cheng",
                    "Lidong Bing",
                    "Qian Yu",
                    "Wei Lu",
                    "Luo Si"
                ],
                "title": "Argument pair extraction from peer review and rebuttal via multi-task learning",
                "pub_date": "2020",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_89",
            "content": "Arman Cohan, Franck Dernoncourt, Soon Doo, Trung Kim, Seokhwan Bui, Walter Kim, Nazli Chang,  Goharian, A discourse-aware attention model for abstractive summarization of long documents, 2018, Proceedings of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Arman Cohan",
                    "Franck Dernoncourt",
                    "Soon Doo",
                    "Trung Kim",
                    "Seokhwan Bui",
                    "Walter Kim",
                    "Nazli Chang",
                    " Goharian"
                ],
                "title": "A discourse-aware attention model for abstractive summarization of long documents",
                "pub_date": "2018",
                "pub_title": "Proceedings of NAACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_90",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_91",
            "content": "Yuguang Duan, Canwen Xu, Jiaxin Pei, Jialong Han, Chenliang Li, Pre-train and plug-in: Flexible conditional text generation with variational autoencoders, 2020, Proceedings of the ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Yuguang Duan",
                    "Canwen Xu",
                    "Jiaxin Pei",
                    "Jialong Han",
                    "Chenliang Li"
                ],
                "title": "Pre-train and plug-in: Flexible conditional text generation with variational autoencoders",
                "pub_date": "2020",
                "pub_title": "Proceedings of the ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_92",
            "content": "G\u00fcnes Erkan,  Dragomir R Radev, Lexrank: Graph-based lexical centrality as salience in text summarization, 2004, Artificial Intelligence Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "G\u00fcnes Erkan",
                    " Dragomir R Radev"
                ],
                "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
                "pub_date": "2004",
                "pub_title": "Artificial Intelligence Research",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_93",
            "content": "Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir Radev, Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model, 2019, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Alexander Richard Fabbri",
                    "Irene Li",
                    "Tianwei She",
                    "Suyi Li",
                    "Dragomir Radev"
                ],
                "title": "Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_94",
            "content": "Angela Fan, David Grangier, Michael Auli, Controllable abstractive summarization, 2018, Proceedings of WNGT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Angela Fan",
                    "David Grangier",
                    "Michael Auli"
                ],
                "title": "Controllable abstractive summarization",
                "pub_date": "2018",
                "pub_title": "Proceedings of WNGT",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_95",
            "content": "Angela Fan, Mike Lewis, Yann Dauphin, Hierarchical neural story generation, 2018, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Angela Fan",
                    "Mike Lewis",
                    "Yann Dauphin"
                ],
                "title": "Hierarchical neural story generation",
                "pub_date": "2018",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_96",
            "content": "Beatriz Fisas, Francesco Ronzano, Horacio Saggion, A multi-layered annotated corpus of scientific papers, 2016, Proceedings of LREC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Beatriz Fisas",
                    "Francesco Ronzano",
                    "Horacio Saggion"
                ],
                "title": "A multi-layered annotated corpus of scientific papers",
                "pub_date": "2016",
                "pub_title": "Proceedings of LREC",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_97",
            "content": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, The webnlg challenge: Generating text from rdf data, 2017, Proceedings of INLG, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Claire Gardent",
                    "Anastasia Shimorina",
                    "Shashi Narayan",
                    "Laura Perez-Beltrachini"
                ],
                "title": "The webnlg challenge: Generating text from rdf data",
                "pub_date": "2017",
                "pub_title": "Proceedings of INLG",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_98",
            "content": "Max Grusky, Mor Naaman, Yoav Artzi, Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies, 2018, Proceedings of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Max Grusky",
                    "Mor Naaman",
                    "Yoav Artzi"
                ],
                "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
                "pub_date": "2018",
                "pub_title": "Proceedings of NAACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_99",
            "content": "Karl Moritz Hermann, Tom\u00e1s Kocisk\u1ef3, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend, 2015, Proceedings of NIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Karl Moritz Hermann",
                    "Tom\u00e1s Kocisk\u1ef3",
                    "Edward Grefenstette",
                    "Lasse Espeholt",
                    "Will Kay",
                    "Mustafa Suleyman",
                    "Phil Blunsom"
                ],
                "title": "Teaching machines to read and comprehend",
                "pub_date": "2015",
                "pub_title": "Proceedings of NIPS",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_100",
            "content": "Sepp Hochreiter, J\u00fcrgen Schmidhuber, Long short-term memory, 1997, Neural Computation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Sepp Hochreiter",
                    "J\u00fcrgen Schmidhuber"
                ],
                "title": "Long short-term memory",
                "pub_date": "1997",
                "pub_title": "Neural Computation",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_101",
            "content": "Xinyu Hua, Mitko Nikolov, Nikhil Badugu, Lu Wang, Argument mining for understanding peer reviews, 2019, Proceedings of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Xinyu Hua",
                    "Mitko Nikolov",
                    "Nikhil Badugu",
                    "Lu Wang"
                ],
                "title": "Argument mining for understanding peer reviews",
                "pub_date": "2019",
                "pub_title": "Proceedings of NAACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_102",
            "content": "Xinyu Hua, Lu Wang, Sentence-level content planning and style specification for neural text generation, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Xinyu Hua",
                    "Lu Wang"
                ],
                "title": "Sentence-level content planning and style specification for neural text generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_103",
            "content": "Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard Hovy, Roy Schwartz, A dataset of peer reviews (peerread): Collection, insights and nlp applications, 2018, Proceedings of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Dongyeop Kang",
                    "Waleed Ammar",
                    "Bhavana Dalvi",
                    "Madeleine Van Zuylen",
                    "Sebastian Kohlmeier",
                    "Eduard Hovy",
                    "Roy Schwartz"
                ],
                "title": "A dataset of peer reviews (peerread): Collection, insights and nlp applications",
                "pub_date": "2018",
                "pub_title": "Proceedings of NAACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_104",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Adam: A method for stochastic optimization",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_105",
            "content": "Wojciech Kry\u015bci\u0144ski, Romain Paulus, Caiming Xiong, Richard Socher, Improving abstraction in text summarization, 2018, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Wojciech Kry\u015bci\u0144ski",
                    "Romain Paulus",
                    "Caiming Xiong",
                    "Richard Socher"
                ],
                "title": "Improving abstraction in text summarization",
                "pub_date": "2018",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_106",
            "content": "John Lafferty, Andrew Mccallum, Fernando Cn Pereira, Conditional random fields: Probabilistic models for segmenting and labeling sequence data, 2001, Proceedings of ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "John Lafferty",
                    "Andrew Mccallum",
                    "Fernando Cn Pereira"
                ],
                "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
                "pub_date": "2001",
                "pub_title": "Proceedings of ICML",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_107",
            "content": "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer, Neural architectures for named entity recognition, 2016, Proceedings of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Guillaume Lample",
                    "Miguel Ballesteros",
                    "Sandeep Subramanian",
                    "Kazuya Kawakami",
                    "Chris Dyer"
                ],
                "title": "Neural architectures for named entity recognition",
                "pub_date": "2016",
                "pub_title": "Proceedings of NAACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_108",
            "content": "Anne Lauscher, Goran Glava\u0161, Kai Eckert, Arguminsci: A tool for analyzing argumentation and rhetorical aspects in scientific writing, 2018, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Anne Lauscher",
                    "Goran Glava\u0161",
                    "Kai Eckert"
                ],
                "title": "Arguminsci: A tool for analyzing argumentation and rhetorical aspects in scientific writing",
                "pub_date": "2018",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_109",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal",
                    "Marjan Ghazvininejad",
                    "Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_110",
            "content": "Maria Liakata, Simone Teufel, Advaith Siddharthan, Colin Batchelor, Corpora for the conceptualisation and zoning of scientific papers, 2010, Proceedings of LREC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Maria Liakata",
                    "Simone Teufel",
                    "Advaith Siddharthan",
                    "Colin Batchelor"
                ],
                "title": "Corpora for the conceptualisation and zoning of scientific papers",
                "pub_date": "2010",
                "pub_title": "Proceedings of LREC",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_111",
            "content": "Yi Liao, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, Tong Zhang, QuaSE: Sequence editing under quantifiable guidance, 2018, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Yi Liao",
                    "Lidong Bing",
                    "Piji Li",
                    "Shuming Shi",
                    "Wai Lam",
                    "Tong Zhang"
                ],
                "title": "QuaSE: Sequence editing under quantifiable guidance",
                "pub_date": "2018",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_112",
            "content": "Chin-Yew Lin, Rouge: A package for automatic evaluation of summaries, 2004, Text summarization branches out, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Chin-Yew Lin"
                ],
                "title": "Rouge: A package for automatic evaluation of summaries",
                "pub_date": "2004",
                "pub_title": "Text summarization branches out",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_113",
            "content": "Yang Liu, Mirella Lapata, Hierarchical transformers for multi-document summarization, 2019, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Yang Liu",
                    "Mirella Lapata"
                ],
                "title": "Hierarchical transformers for multi-document summarization",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_114",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_115",
            "content": "Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnab\u00e1s P\u00f3czos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan Black, Shrimai Prabhumoye, Politeness transfer: A tag and generate approach, 2020, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Aman Madaan",
                    "Amrith Setlur",
                    "Tanmay Parekh",
                    "Barnab\u00e1s P\u00f3czos",
                    "Graham Neubig",
                    "Yiming Yang",
                    "Ruslan Salakhutdinov",
                    "Alan Black",
                    "Shrimai Prabhumoye"
                ],
                "title": "Politeness transfer: A tag and generate approach",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_116",
            "content": "Rada Mihalcea, Paul Tarau, Textrank: Bringing order into text, 2004, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Rada Mihalcea",
                    "Paul Tarau"
                ],
                "title": "Textrank: Bringing order into text",
                "pub_date": "2004",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_117",
            "content": "Ramesh Nallapati, Bowen Zhou, \u00c7aglar Cicero Dos Santos, Bing Gul\u00e7ehre,  Xiang, Abstractive text summarization using sequence-to-sequence rnns and beyond, 2016, Proceedings of SIGNLL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Ramesh Nallapati",
                    "Bowen Zhou",
                    "\u00c7aglar Cicero Dos Santos",
                    "Bing Gul\u00e7ehre",
                    " Xiang"
                ],
                "title": "Abstractive text summarization using sequence-to-sequence rnns and beyond",
                "pub_date": "2016",
                "pub_title": "Proceedings of SIGNLL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_118",
            "content": "Courtney Napoles, Benjamin Matthew R Gormley,  Van Durme, Annotated gigaword, 2012, Proceedings of AKBC-WEKEX, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Courtney Napoles",
                    "Benjamin Matthew R Gormley",
                    " Van Durme"
                ],
                "title": "Annotated gigaword",
                "pub_date": "2012",
                "pub_title": "Proceedings of AKBC-WEKEX",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_119",
            "content": "Shashi Narayan, B Shay, Mirella Cohen,  Lapata, Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization, 2018, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Shashi Narayan",
                    "B Shay",
                    "Mirella Cohen",
                    " Lapata"
                ],
                "title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
                "pub_date": "2018",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_120",
            "content": "Paul Over, James Yen, An introduction to duc-2004, 2004, Proceedings of DUC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Paul Over",
                    "James Yen"
                ],
                "title": "An introduction to duc-2004",
                "pub_date": "2004",
                "pub_title": "Proceedings of DUC",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_121",
            "content": "Karolina Owczarzak, Hoa Dang, Overview of the tac 2011 summarization track: Guided task and aesop task, 2011, Proceedings of TAC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Karolina Owczarzak",
                    "Hoa Dang"
                ],
                "title": "Overview of the tac 2011 summarization track: Guided task and aesop task",
                "pub_date": "2011",
                "pub_title": "Proceedings of TAC",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_122",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter J Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_123",
            "content": "La Ramshaw, Text chunking using transformation-based learning, 1995, Proceedings of Third Workshop on Very Large Corpora, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    " La Ramshaw"
                ],
                "title": "Text chunking using transformation-based learning",
                "pub_date": "1995",
                "pub_title": "Proceedings of Third Workshop on Very Large Corpora",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_124",
            "content": "Lev Ratinov, Dan Roth, Design challenges and misconceptions in named entity recognition, 2009, Proceedings of CoNLL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Lev Ratinov",
                    "Dan Roth"
                ],
                "title": "Design challenges and misconceptions in named entity recognition",
                "pub_date": "2009",
                "pub_title": "Proceedings of CoNLL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_125",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, 2019, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Nils Reimers",
                    "Iryna Gurevych"
                ],
                "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_126",
            "content": "Ehud Reiter, Robert Dale, Building applied natural language generation systems, 1997, Natural Language Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Ehud Reiter",
                    "Robert Dale"
                ],
                "title": "Building applied natural language generation systems",
                "pub_date": "1997",
                "pub_title": "Natural Language Engineering",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_127",
            "content": "UNKNOWN, None, 2008, The new york times annotated corpus. Linguistic Data Consortium, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2008",
                "pub_title": "The new york times annotated corpus. Linguistic Data Consortium",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_128",
            "content": "Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan Zhao, Shuming Shi, Rui Yan, Semi-supervised text style transfer: Cross projection in latent space, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Mingyue Shang",
                    "Piji Li",
                    "Zhenxin Fu",
                    "Lidong Bing",
                    "Dongyan Zhao",
                    "Shuming Shi",
                    "Rui Yan"
                ],
                "title": "Semi-supervised text style transfer: Cross projection in latent space",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_129",
            "content": "Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu, Long and diverse text generation with planning-based hierarchical variational model, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Zhihong Shao",
                    "Minlie Huang",
                    "Jiangtao Wen",
                    "Wenfei Xu",
                    "Xiaoyan Zhu"
                ],
                "title": "Long and diverse text generation with planning-based hierarchical variational model",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_130",
            "content": "Eva Sharma, Chen Li, Lu Wang, Bigpatent: A large-scale dataset for abstractive and coherent summarization, 2019, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Eva Sharma",
                    "Chen Li",
                    "Lu Wang"
                ],
                "title": "Bigpatent: A large-scale dataset for abstractive and coherent summarization",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_131",
            "content": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola, Style transfer from non-parallel text by cross-alignment, 2017, Proceedings of NIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Tianxiao Shen",
                    "Tao Lei",
                    "Regina Barzilay",
                    "Tommi Jaakkola"
                ],
                "title": "Style transfer from non-parallel text by cross-alignment",
                "pub_date": "2017",
                "pub_title": "Proceedings of NIPS",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_132",
            "content": "Shunsuke Takeno, Masaaki Nagata, Kazuhide Yamamoto, Controlling target features in neural machine translation via prefix constraints, 2017, Proceedings of WAT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Shunsuke Takeno",
                    "Masaaki Nagata",
                    "Kazuhide Yamamoto"
                ],
                "title": "Controlling target features in neural machine translation via prefix constraints",
                "pub_date": "2017",
                "pub_title": "Proceedings of WAT",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_133",
            "content": "Jiwei Tan, Xiaojun Wan, Jianguo Xiao, Abstractive document summarization with a graphbased attentional neural model, 2017, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Jiwei Tan",
                    "Xiaojun Wan",
                    "Jianguo Xiao"
                ],
                "title": "Abstractive document summarization with a graphbased attentional neural model",
                "pub_date": "2017",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_134",
            "content": "Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric Xing, Zhiting Hu, Targetguided open-domain conversation, 2019, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Jianheng Tang",
                    "Tiancheng Zhao",
                    "Chenyan Xiong",
                    "Xiaodan Liang",
                    "Eric Xing",
                    "Zhiting Hu"
                ],
                "title": "Targetguided open-domain conversation",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_135",
            "content": "Simone Teufel, Jean Carletta, Marc Moens, An annotation scheme for discourse-level argumentation in research articles, 1999, Proceedings of EACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Simone Teufel",
                    "Jean Carletta",
                    "Marc Moens"
                ],
                "title": "An annotation scheme for discourse-level argumentation in research articles",
                "pub_date": "1999",
                "pub_title": "Proceedings of EACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_136",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Proceedings of NIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Proceedings of NIPS",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_137",
            "content": "Yunli Wang, Yu Wu, Lili Mou, Zhoujun Li, Wenhan Chao, Harnessing pre-trained neural networks with rules for formality style transfer, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Yunli Wang",
                    "Yu Wu",
                    "Lili Mou",
                    "Zhoujun Li",
                    "Wenhan Chao"
                ],
                "title": "Harnessing pre-trained neural networks with rules for formality style transfer",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_138",
            "content": "Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, Transformers: State-of-theart natural language processing, 2020, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Thomas Wolf",
                    "Julien Chaumond",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer"
                ],
                "title": "Transformers: State-of-theart natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_139",
            "content": "Xinyu Xing, Xiaosheng Fan, Xiaojun Wan, Automatic generation of citation texts in scholarly papers: A pilot study, 2020, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Xinyu Xing",
                    "Xiaosheng Fan",
                    "Xiaojun Wan"
                ],
                "title": "Automatic generation of citation texts in scholarly papers: A pilot study",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_140",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, Pegasus: Pre-training with extracted gap-sentences for abstractive summarization, 2020, Proceedings of ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Jingqing Zhang",
                    "Yao Zhao",
                    "Mohammad Saleh",
                    "Peter Liu"
                ],
                "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
                "pub_date": "2020",
                "pub_title": "Proceedings of ICML",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v2_141",
            "content": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston, Personalizing dialogue agents: I have a dog, do you have pets too?, 2018, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Saizheng Zhang",
                    "Emily Dinan",
                    "Jack Urbanek",
                    "Arthur Szlam",
                    "Douwe Kiela",
                    "Jason Weston"
                ],
                "title": "Personalizing dialogue agents: I have a dog, do you have pets too?",
                "pub_date": "2018",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "124-ARR_v2_0@0",
            "content": "MReD: A Meta-Review Dataset for Structure-Controllable Text Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_0",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_2@0",
            "content": "When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_2",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_2@1",
            "content": "A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_2",
            "start": 203,
            "end": 362,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_2@2",
            "content": "A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with a deep understanding of the domain knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_2",
            "start": 364,
            "end": 550,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_2@3",
            "content": "Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_2",
            "start": 552,
            "end": 841,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_2@4",
            "content": "We present experimental results on start-of-the-art summarization models, and propose methods for structurecontrolled generation with both extractive and abstractive models using our annotated data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_2",
            "start": 843,
            "end": 1040,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_2@5",
            "content": "By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_2",
            "start": 1042,
            "end": 1315,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_2@6",
            "content": "1 * * Equally Contributed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_2",
            "start": 1317,
            "end": 1342,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_2@7",
            "content": "\u2020 Chenhui, Liying, and Ran are under the Joint PhD Program between Alibaba and their corresponding universities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_2",
            "start": 1344,
            "end": 1455,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_3@0",
            "content": "\u2021 \u2021 Corresponding author.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_3",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_3@1",
            "content": "1 Our code and data are released at https://github.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_3",
            "start": 26,
            "end": 76,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_3@2",
            "content": "com/Shen-Chenhui/MReD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_3",
            "start": 78,
            "end": 99,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_4@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_4",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_5@0",
            "content": "Text generation entered a new era because of the development of neural network based generation techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_5",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_5@1",
            "content": "Along the dimension of the mapping relation between the input information and the output text, we can roughly group the recent tasks meta-review: [This paper studies n-step returns in off-policy RL and introduces a novel algorithm which adapts the return's horizon n in function of a notion of policy's age.]\u2190ABSTRACT [Over-all, the reviewers found that the paper presents interesting observations and promising experimental results.]\u2190STRENGTH [However, they also raised concerns in their initial reviews, regarding the clarity of the paper, its theoretical foundations and its positioning (notably regarding the bias/variance tradeoff of uncorrected n-step returns) and parts of the experimental results. ]\u2190WEAKNESS [In the absence of rebuttal or revised manuscript from the authors, not much discussion was triggered.]\u2190REBUTTAL PROCESS [Based on the initial reviews, the AC cannot recommend accepting this paper, but the authors are encouraged to pursue this interesting research direction.]\u2190DECISION Table 1: An example of annotated meta-review.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_5",
            "start": 108,
            "end": 1155,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_5@2",
            "content": "CATE-GORY indicates the category of each sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_5",
            "start": 1157,
            "end": 1206,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_6@0",
            "content": "into three clusters: more-to-less, less-to-more, and neck-to-neck.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_6",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_6@1",
            "content": "The more-to-less text generation tasks output a concise piece of text from some more abundant input, such as text summarization (Tan et al., 2017;Kry\u015bci\u0144ski et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_6",
            "start": 67,
            "end": 237,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_6@2",
            "content": "The lessto-more generation tasks generate a more abundant output from some obviously simpler input, such as prompt-based story generation (Fan et al., 2018b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_6",
            "start": 239,
            "end": 396,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_6@3",
            "content": "The neck-to-neck generation aims at generating an output text which conveys the same quantity of knowledge as the input but in natural language, such as typical RDF triples to text tasks (Gardent et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_6",
            "start": 398,
            "end": 607,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_7@0",
            "content": "To some extent, the existing task settings are not so adequate because they do not have a deep understanding of the domains they are working on, i.e., domain knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_7",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_7@1",
            "content": "Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail (Nallapati et al., 2016) is composed of the training pairs of news content and human-written summary bullets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_7",
            "start": 169,
            "end": 368,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_7@2",
            "content": "However, it does not tell why a particular piece of news content should have that corresponding summary, for example for the same earnings report, why one media emphasizes its new business success in the summary, but another emphasizes its net income.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_7",
            "start": 370,
            "end": 620,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_7@3",
            "content": "Obviously, there is not a standard answer regarding right or wrong.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_7",
            "start": 622,
            "end": 688,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_7@4",
            "content": "For such cases, if we can specify a control signal, e.g., \"emphasizing new business\", the generated text would make more sense to users using the text generator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_7",
            "start": 690,
            "end": 850,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@0",
            "content": "To allow controlling not only the intent of a single generated sentence but also the whole structure of a generated passage, we prepare a new dataset MReD (short for Meta-Review Dataset) with in-depth understanding of the structure of meta-reviews in a peer-reviewing system, namely the open review system of ICLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 0,
            "end": 313,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@1",
            "content": "MReD for the first time allows a generator to be trained by simultaneously taking the text (i.e. reviews) and the structure control signal as input to generate a meta-review which is not only derivable from the reviews but also complies with the control intent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 315,
            "end": 575,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@2",
            "content": "Thus from the same input text, the trained generator can generate varied outputs according to the given control signals.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 577,
            "end": 696,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@3",
            "content": "For example, if the area chair is inclined to accept a borderline paper, he or she may invoke our generator with a structure of \"abstract | strength | decision\" to generate a meta-review, or may use a structure of \"abstract | weakness | suggestion\" otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 698,
            "end": 956,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@4",
            "content": "Note that for ease of preparation and explanation, we ground our dataset in the peer review domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 958,
            "end": 1056,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@5",
            "content": "However, the data preparation methodology and proposed models are transferable to other domains, which is indeed what we hope to motivate with this effort.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 1058,
            "end": 1212,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@6",
            "content": "Specifically, we collect 7,089 meta-reviews of ICLR in recent years (2018 -2021) and fully annotate the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 1214,
            "end": 1325,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@7",
            "content": "Each sentence in a meta-review is classified into one of the 9 pre-defined intent categories: abstract, strength, weakness, rating summary, area chair (AC) disagreement, rebuttal process, suggestion, decision, and miscellaneous (misc).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 1327,
            "end": 1561,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@8",
            "content": "Table 1 shows an annotated example, where each sentence is classified into a single category that best describes the intent of this sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 1563,
            "end": 1703,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@9",
            "content": "Our MReD is obviously different from the previous text generation/summarization datasets because, given the rich annotations of individual meta-review sentences, a model is allowed to learn more sophisticated generation behaviors to control the structure of the generated passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 1705,
            "end": 1984,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_8@10",
            "content": "Our proposed task is also noticeably different from the existing controllable text generation tasks (e.g., text style transfer on sentiment polarity (Shen et al., 2017;Liao et al., 2018) and formality (Shang et al., 2019)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_8",
            "start": 1986,
            "end": 2307,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_9@0",
            "content": "To summarize, our contributions are as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_9",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_10@0",
            "content": "(1) We introduce a fully-annotated meta-review dataset to make better use of the domain knowledge for text generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_10",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_10@1",
            "content": "With thorough data analysis, we derive useful insights into the domain characteristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_10",
            "start": 119,
            "end": 205,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_11@0",
            "content": "(2) We propose a new task of controllable generation focusing on controlling the passage macro structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_11",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_11@1",
            "content": "It offers stronger generation flexibility and applicability for practical use cases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_11",
            "start": 107,
            "end": 190,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_12@0",
            "content": "(3) We design simple yet effective control methods that are independent of the model architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_12",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_12@1",
            "content": "We show the effectiveness of enforcing different generation structures with a detailed model analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_12",
            "start": 99,
            "end": 200,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_13@0",
            "content": "MReD: Meta-Review Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_13",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_14@0",
            "content": "In this paper, we explore a new task, named the structure-controllable text generation, in a new domain, namely the meta-reviews in the peerreviewing system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_14",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_14@1",
            "content": "Unlike the previous datasets that mainly focus on domains like news, the domain for meta-reviews is worth-studying because it contains essential and high-density opinions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_14",
            "start": 158,
            "end": 328,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_14@2",
            "content": "Specifically, during the peer review process of scientific papers, a senior reviewer or area chair will recommend a decision and manually write a meta-review to summarize the opinions from different reviews written by the reviewers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_14",
            "start": 330,
            "end": 561,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_14@3",
            "content": "We first introduce the data collection process and then describe the annotation details, followed by dataset analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_14",
            "start": 563,
            "end": 680,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_15@0",
            "content": "Data Collection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_15",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_16@0",
            "content": "We collect the meta-review related data of ICLR from an online peer-reviewing platform, i.e., Open-Review 2 from 2018 to 2021.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_16",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_16@1",
            "content": "Note that the submissions from earlier years are not collected because their meta-reviews are not released.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_16",
            "start": 127,
            "end": 233,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_16@2",
            "content": "To prepare our dataset for controllable text generation, for each submission, we collect all of its corresponding official reviews with reviewer ratings and confidence scores, the final meta-review decision, and the meta-review passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_16",
            "start": 235,
            "end": 470,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_16@3",
            "content": "Table 2 shows the statistics of data collected from each year.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_16",
            "start": 472,
            "end": 533,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_16@4",
            "content": "Initially, 7,894 submissions are collected.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_16",
            "start": 535,
            "end": 577,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_16@5",
            "content": "After filtering, 7,089 meta-reviews are retained with their corresponding 23,675 reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_16",
            "start": 579,
            "end": 667,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_16@6",
            "content": "Note that even without any further annotation, the dataset can already naturally serve the purpose of multi-document summarization (MDS).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_16",
            "start": 669,
            "end": 805,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_16@7",
            "content": "Compared with those conventional datasets for MDS, such as TAC (Owczarzak and Dang, 2011) and DUC (Over and Yen, 2004), which contain in total a few hundred input articles (equivalent to reviews in MReD), our dataset is more than 10 times larger.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_16",
            "start": 807,
            "end": 1052,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_17@0",
            "content": "Data Annotation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_17",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_18@0",
            "content": "As aforementioned, the structure-controllable text generation aims at controlling the structure of the generated passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_18",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_18@1",
            "content": "Therefore, we need to comprehensively understand the structures of metareviews so as to enable a model to learn how to generate outputs complying with certain structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_18",
            "start": 122,
            "end": 291,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_18@2",
            "content": "Specifically, based on the nature of meta-reviews, we pre-define 9 intent categories: abstract, strength, weakness, suggestion, rebuttal process, rating summary, area chair (AC) disagreement, decision, and miscellaneous (misc).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_18",
            "start": 293,
            "end": 519,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_18@3",
            "content": "Table 3 shows the definition for each category (see example sentences in Appendix A.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_18",
            "start": 521,
            "end": 607,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_18@4",
            "content": "The identification of category for some sentences is fairly straightforward, while some sentences are relatively ambiguous.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_18",
            "start": 609,
            "end": 731,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_18@5",
            "content": "Therefore, besides following the definition of each category, the annotators are also required to follow the additional rules as elaborated in Appendix A.2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_18",
            "start": 733,
            "end": 887,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_19@0",
            "content": "For conducting the annotation work, 14 professional data annotators from a data company are initially trained, and 12 of them are selected for the task according to their annotation quality during a trial round.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_19",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_19@1",
            "content": "These 12 annotators are fully paid for their work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_19",
            "start": 212,
            "end": 261,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_19@2",
            "content": "Each meta-review sentence is independently labeled by 2 different annotators, and a third expert annotator resolves any disagreement between the first two annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_19",
            "start": 263,
            "end": 428,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_19@3",
            "content": "We label 45,929 sentences from 7,089 meta-reviews in total, and the Cohen's kappa is 0.778 between the first two annotators, showing that the annotation is of quite high quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_19",
            "start": 430,
            "end": 607,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_20@0",
            "content": "Data Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_20",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_21@0",
            "content": "To better understand the MReD dataset, we conduct the following analysis along different dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_21",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_22@0",
            "content": "Sentence distribution across categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_22",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_22@1",
            "content": "The number of sentences in different categories are shown in Figure 1, breakdown by the decision (i.e., accept or reject).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_22",
            "start": 41,
            "end": 162,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_22@2",
            "content": "Among 7,089 submissions, there are 2,368 accepted and 4,721 rejected.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_22",
            "start": 164,
            "end": 232,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_22@3",
            "content": "Among all submissions and the rejected submissions, \"weakness\" accounts for the largest proportion, while across the accepted ones, \"abstract\" and \"strength\" take up a great proportion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_22",
            "start": 234,
            "end": 418,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_22@4",
            "content": "To some extent, these three categories which dominate in meta-reviews could be easily summarized from the reviewers' comments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_22",
            "start": 420,
            "end": 545,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_22@5",
            "content": "However, some minor or subjective categories (e.g., \"ac disagreement\") are hard to generate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_22",
            "start": 547,
            "end": 638,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_23@0",
            "content": "Breakdown analysis by meta-review lengths and average rating scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_23",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_23@1",
            "content": "We present the percentage of meta-reviews of different lengths in each score range, as shown in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_23",
            "start": 69,
            "end": 173,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_23@2",
            "content": "For example, among the meta-reviews that receive the reviewers' average score below 2 (i.e., the first column in the figure), 28% are less than or equal to 50 words, and 38% fall in the length range of 51 to 100 words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_23",
            "start": 175,
            "end": 392,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_23@3",
            "content": "We can observe that the meta-reviews tend to be longer for those submissions receiving scores in the middle range, while shorter for those with lower scores or higher scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_23",
            "start": 394,
            "end": 567,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_23@4",
            "content": "This coincides with our commonsense that for high-score and low-score submissions, the decision tends to be a clear accept or reject so that meta-reviews can be relatively shorter, while for those borderline submissions, area chairs have to carefully weigh the pros and cons to make the final decision (see Appendix B.1 for borderline submission analysis).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_23",
            "start": 569,
            "end": 924,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_23@5",
            "content": "As shown in Figure 3, the meta-reviews with more than 150 words generally have a larger proportion of sentences describing \"weakness\" and \"suggestion\" for authors to improve the submissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_23",
            "start": 926,
            "end": 1115,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_23@6",
            "content": "Additional analysis on the category breakdown for accepted and rejected papers across the score ranges is shown in Appendix B.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_23",
            "start": 1117,
            "end": 1244,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_24@0",
            "content": "Meta-review patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_24",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_24@1",
            "content": "To study the common structures of meta-reviews, we present the transition matrix of different category segments in Figure 4, where the sum of each row is 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_24",
            "start": 22,
            "end": 177,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_24@2",
            "content": "Note that each segment represents the longest consecutive sentences with the same category.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_24",
            "start": 179,
            "end": 269,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_24@3",
            "content": "We add \"<start>\" and \"<end>\" tokens before and after each metareview accordingly to investigate which categories tend to be at the start/end of the meta-reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_24",
            "start": 271,
            "end": 431,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_24@4",
            "content": "It is clear to see that \"abstract\" usually positions at the beginning of the meta-review, while \"suggestion\" and \"decision\" usually appear at the end.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_24",
            "start": 433,
            "end": 582,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_24@5",
            "content": "There are also some clear patterns appearing in the metareviews, such as \"abstract | strength | weakness\", \"rating summary | weakness | rebuttal process\", and \"abstract | weakness | decision\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_24",
            "start": 584,
            "end": 775,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_25@0",
            "content": "3 Structure-Controllable Text Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_25",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_26@0",
            "content": "Task Definition",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_26",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_27@0",
            "content": "As aforementioned, in uncontrolled generation, users cannot instruct the model to emphasize on desired aspects.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_27",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_27@1",
            "content": "However, in a domain such as meta-reviews, given the same review inputs, one AC may emphasize more on the \"strength\" of the paper following a structure of \"abstract | strength | .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 decision\", whereas another AC may prefer a different structure with more focus on reviewers' opinions and suggestions (i.e., \"rating summary\" and \"suggestion\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_27",
            "start": 112,
            "end": 493,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_27@2",
            "content": "To achieve such flexibility, the task of structure-controllable text generation is defined as: given the text input (i.e., reviews) and a control sequence of the output structure, a model should generate a meta-review that is derivable from the reviews and presents the required structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_27",
            "start": 495,
            "end": 783,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_28@0",
            "content": "Explored Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_28",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_29@0",
            "content": "As the recent generation works (Vaswani et al., 2017;Liu and Lapata, 2019;Xing et al., 2020) basically adopt an encoder-decoder based architecture and achieve state-of-the-art performance on many tasks and datasets, we primarily investigate the performance of such a framework on our task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_29",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_29@1",
            "content": "Thus in this subsection, we mainly present how to re-organize the input reviews and the control structure as an input sequence of the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_29",
            "start": 290,
            "end": 431,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_29@2",
            "content": "We also explore other baselines in the experiments later.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_29",
            "start": 433,
            "end": 489,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_30@0",
            "content": "In order to summarize multiple reviews into a meta-review showing a required structure, we explicitly specify the control label sequence that a model should comply with during generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_30",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_30@1",
            "content": "Specifically, we intuitively add the control sequence in front of the input text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_30",
            "start": 188,
            "end": 268,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_30@2",
            "content": "By directly combining both the control and textual information as a single input, our control method is independent of any specially designed encoder and decoder structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_30",
            "start": 270,
            "end": 442,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_30@3",
            "content": "Moreover, by placing the short control sequence in front, an encoder can immediately observe the control signal at the very beginning, thus avoids the possible interference by the subsequent sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_30",
            "start": 444,
            "end": 643,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_30@4",
            "content": "Moreover, the control sequence in front will never be truncated when the encoder truncates the input to a certain length limit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_30",
            "start": 645,
            "end": 771,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@0",
            "content": "Given the multiple review inputs, we need to linearize them into a single input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@1",
            "content": "One simple method, concat, is to concatenate all inputs one after another (Fabbri et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 81,
            "end": 176,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@2",
            "content": "Besides the text inputs, the review rating, which cannot be found in the review passages but exists in the field of rating score, is also crucial information for writing meta-reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 178,
            "end": 360,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@3",
            "content": "Therefore, we create a rating sentence that consists of the extracted ratings given by the corresponding reviewers and prepend it to our concatenated review texts to obtain the final input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 362,
            "end": 550,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@4",
            "content": "We name this method rate-concat (see Table 4, upper).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 552,
            "end": 604,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@5",
            "content": "We also explore an alternative method, merge, as follows: From all review inputs, we use the longest one as a backbone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 606,
            "end": 724,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@6",
            "content": "We segment all reviews' content on a paragraph level, and encode them using Sen-tenceTransformers (Reimers and Gurevych, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 726,
            "end": 852,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@7",
            "content": "Then, for each paragraph embedding in the nonbackbone reviews, we calculate a cosine similarity score with each backbone paragraph embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 854,
            "end": 994,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@8",
            "content": "We then insert each non-backbone paragraph after the backbone paragraph with which it has the highest similarity score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 996,
            "end": 1114,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@9",
            "content": "We repeat the process for all paragraphs in non-backbone reviews to obtain a single passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 1116,
            "end": 1207,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@10",
            "content": "We further add rating sentences in front of the results of merge to obtain rate-merge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 1209,
            "end": 1294,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_31@11",
            "content": "Additionally, we provide a longest-review baseline, which does not combine reviews but only uses the longest review as the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_31",
            "start": 1296,
            "end": 1424,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_32@0",
            "content": "As aforementioned, we place the control sequence in front of the re-organized review information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_32",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_32@1",
            "content": "Specifically, we explore two different control methods, namely, sent-ctrl and seg-ctrl.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_32",
            "start": 98,
            "end": 184,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_32@2",
            "content": "Sent-ctrl uses one control label per target sentence and controls generation on the sentence-level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_32",
            "start": 186,
            "end": 284,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_32@3",
            "content": "Note that this method can allow implicit control on the length (i.e., number of sentences) of the generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_32",
            "start": 286,
            "end": 394,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_32@4",
            "content": "Segctrl treats consecutive sentences of the same label as one segment and only uses one label for a single segment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_32",
            "start": 396,
            "end": 510,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_32@5",
            "content": "Example inputs of different control settings are shown in Table 4 (lower).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_32",
            "start": 512,
            "end": 585,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_32@6",
            "content": "For instance, sent-ctrl repeats \"abstract\" in its control sequence whereas seg-ctrl does not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_32",
            "start": 587,
            "end": 679,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_32@7",
            "content": "This is because seg-ctrl treats the 1 st and 2 nd target sentences of \"abstract\" as the same segment and only uses a single label to indicate it in the sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_32",
            "start": 681,
            "end": 841,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_32@8",
            "content": "Additionally, we provide a vanilla setting for uncontrolled generation, unctrl, where no control sequence is used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_32",
            "start": 843,
            "end": 956,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_33@0",
            "content": "Using the above input sequence as the source and the corresponding meta-review as the target, we can train an encoder-decoder model for controllable generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_33",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_33@1",
            "content": "Many transformer-based models have achieved state-of-the-art performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_33",
            "start": 161,
            "end": 233,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_33@2",
            "content": "Common abstractive summarization models include BART (Lewis et al., 2020), T5 (Raffel et al., 2020) and PEGASUS (Zhang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_33",
            "start": 235,
            "end": 367,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_33@3",
            "content": "In this paper we focus on the bart-large-cnn model, one variant of the BART model (results on other pretrained models can be found in Appendix C.1, which show similar trend).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_33",
            "start": 369,
            "end": 542,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_33@4",
            "content": "More specifically, we use the Py-Torch implementation in the open-source library Hugging Face Transformers (Wolf et al., 2020) and its hosted pretrained models 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_33",
            "start": 544,
            "end": 706,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_34@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_34",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_35@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_35",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_36@0",
            "content": "Extractive Baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_36",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_36@1",
            "content": "We employ three common extractive summarization baselines each of which basically provides a mechanism to rank the input sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_36",
            "start": 22,
            "end": 152,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_36@2",
            "content": "LexRank (Erkan and Radev, 2004) represents sentences in a graph and uses eigenvector centrality to calculate sentence importance scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_36",
            "start": 154,
            "end": 289,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_36@3",
            "content": "TextRank (Mihalcea and Tarau, 2004) is another graph-based sentence ranking method that obtains vertex scores by running a \"random-surfer model\" until convergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_36",
            "start": 291,
            "end": 453,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_36@4",
            "content": "MMR (Carbonell and Goldstein, 1998) calculates sentence scores by balancing the redundancy score with the information relevance score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_36",
            "start": 455,
            "end": 588,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_36@5",
            "content": "After ranking with each of the above models, we select sentences as output with different strategies according to the controlled and uncontrolled settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_36",
            "start": 590,
            "end": 744,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_36@6",
            "content": "For the uncontrolled setting, we simply select the top k sentences as the generated output, where k is a hyperparameter deciding the size of the generated output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_36",
            "start": 746,
            "end": 907,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_36@7",
            "content": "For the controlled setting, we select only the top sentences with the right category labels according to the control sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_36",
            "start": 909,
            "end": 1034,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_36@8",
            "content": "To do so, we employ an LSTM-CRF (Lample et al., 2016) tagger trained on the labeled meta-reviews to predict the labels of each input review sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_36",
            "start": 1036,
            "end": 1184,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_36@9",
            "content": "Refer to Appendix C.2 for more details of the tagger.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_36",
            "start": 1186,
            "end": 1238,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_37@0",
            "content": "Generic Sentence Baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_37",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_37@1",
            "content": "Considering the nature of meta-reviews, we could imagine some categories may have common phrases inflating the Rouge scores, such as \"This paper proposes ...\" for abstract, and \"I recommend acceptance.\" for decision, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_37",
            "start": 28,
            "end": 248,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_37@2",
            "content": "To examine such impact, we select sentences that are generic in each category and combine these sentences to generate outputs according to the control sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_37",
            "start": 250,
            "end": 410,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_37@3",
            "content": "For instance, if the control sequence is \"abstract | strength | decision\", we take the most generic sentences from the categories of \"abstract\", \"strength\" and \"decision\" respectively to form the output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_37",
            "start": 412,
            "end": 614,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_37@4",
            "content": "Specifically, we create two generic sentence baselines by obtaining generic sentences from the training data from either the meta-review references (i.e., target) or the input reviews (i.e., source), namely \"Target Generic\" and \"Source Generic\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_37",
            "start": 616,
            "end": 860,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_37@5",
            "content": "Moreover, we also study such impact on the high-score and low-score submissions respectively, since an AC may write more succinct meta-reviews for clear-cut papers, as suggested by Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_37",
            "start": 862,
            "end": 1051,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_37@6",
            "content": "See Appendix C.3 for more details and results on generic sentence baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_37",
            "start": 1053,
            "end": 1128,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_38@0",
            "content": "Experimental Setting",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_38",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_39@0",
            "content": "To conduct text generation experiments, we preprocess our MReD dataset by filtering to ensure the selected meta-reviews have 20 to 400 words, as certain meta-review passages are extremely short or long.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_39",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_39@1",
            "content": "After preprocessing, we obtain 6,693 sourcetarget pairs, for which we randomly split into train, validation, and test sets by a ratio of 8:1:1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_39",
            "start": 203,
            "end": 345,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_39@2",
            "content": "We evaluate our generated outputs against the reference meta-reviews using the F 1 scores of ROUGE ROUGE 2 , and ROUGE L (Lin, 2004) 4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_39",
            "start": 347,
            "end": 482,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_40@0",
            "content": "For the extractive and generic baselines, a key hyperparameter is the sentence number k. Recall that under the sent-ctrl setting, the control sequence length is the same as the sentence number of the target meta-review.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_40",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_40@1",
            "content": "Therefore, to conduct a fair comparison, we set the hyperparameter k equal to the number of labels in the control sequence for both controlled and uncontrolled extractive baselines, and sent-ctrl is used for all controlled extractive baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_40",
            "start": 220,
            "end": 463,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_40@2",
            "content": "We also adopt the same k for the generic baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_40",
            "start": 465,
            "end": 515,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_41@0",
            "content": "For bart-large-cnn, we first load the pretrained model and then fine-tune it on MReD. All experiments are conducted on single V100 GPUs, using a batch size of 1 in order to fit the large pretrained model on a single GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_41",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_41@1",
            "content": "During fine-tuning, we set the hyperparameters of \"minimum_target_length\" to 20, and \"maximum_target_length\" to 400, according to our filter range on the meta-review lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_41",
            "start": 221,
            "end": 394,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_41@2",
            "content": "Due to long inputs (see Table 17), we experiment with different source truncation lengths of 1024, 2048, and 3072 tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_41",
            "start": 396,
            "end": 516,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_41@3",
            "content": "We cannot explore truncation length of more than 3072 tokens due to the limitation of GPU space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_41",
            "start": 518,
            "end": 613,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_41@4",
            "content": "Our learning rate is 5e-5, and we use Adam optimizer with momentum \u03b2 1 = 0.9, \u03b2 2 = 0.999 without any warm-up steps or weight decay.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_41",
            "start": 615,
            "end": 746,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_41@5",
            "content": "We set the seed to be 0, and train the model for 3 epochs with gradient accumulation step of 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_41",
            "start": 748,
            "end": 842,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_41@6",
            "content": "For decoding, we use a beam size of 4 and length penalty of 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_41",
            "start": 844,
            "end": 905,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_42@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_42",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_43@0",
            "content": "We show results in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_43",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_43@1",
            "content": "Only the best settings of rate-concat ( Section 4.4) and input truncation of 2048 tokens (Appendix C.4) for bart-large-cnn are included.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_43",
            "start": 28,
            "end": 163,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_43@2",
            "content": "Amongst the extractive baselines, TextRank performs the best in both unctrl and sentctrl settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_43",
            "start": 165,
            "end": 262,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_43@3",
            "content": "Nevertheless, all controlled methods outperform their unctrl settings (same for the Transformers).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_43",
            "start": 264,
            "end": 361,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_43@4",
            "content": "This validates our intuition that structure-controlled generation is more suitable for user-subjective writings such as meta-reviews, because the model can better satisfy different structure requirements when supplied with the corresponding control sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_43",
            "start": 363,
            "end": 621,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_43@5",
            "content": "On the other hand, for bart-large-cnn, sent-ctrl is the best, followed by seg-ctrl.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_43",
            "start": 623,
            "end": 705,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_43@6",
            "content": "This is most likely due to the former's more fine-grained sentence-level control that provides a clearer structure outline, as compared to the coarser segment-level control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_43",
            "start": 707,
            "end": 879,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_44@0",
            "content": "Moreover, bart-large-cnn far outperforms the extractive baselines, showing that the extractionbased methods are insufficient for MReD. This also suggests that meta-review writings are different from the input reviews, therefore copying full review sentences to form meta-reviews doesn't work well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_44",
            "start": 0,
            "end": 296,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_44@1",
            "content": "This is also validated by the \"Target Generic\" baseline's consistent improvement over the \"Source Generic\" baseline, which shows that generic sentences from meta-reviews can suit generation better than those in reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_44",
            "start": 298,
            "end": 516,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_44@2",
            "content": "Nevertheless, all Transformers results are still much better than the \"Target Generic\" sentence baseline, showing that despite generic phrases in some categories contributing to Rouge, the Transformers model is capable of capturing content-specific information for each input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_44",
            "start": 518,
            "end": 793,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_45@0",
            "content": "Review Combination Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_45",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_46@0",
            "content": "We also show uncontrolled generation results for different review combination methods in Table 6, with source truncation of 2048.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_46",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_46@1",
            "content": "The longest-review setting has the worst performance, thus validating that the review combination methods are necessary in order not to omit important information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_46",
            "start": 130,
            "end": 292,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_46@2",
            "content": "Rateconcat has the best overall performance, which is the setting we used for the main results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_46",
            "start": 294,
            "end": 388,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_46@3",
            "content": "Never- theless, it is not significantly better than merge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_46",
            "start": 390,
            "end": 447,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_46@4",
            "content": "It is also interesting to see that for merge, providing additional rating information (rate-merge) slightly worsens the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_46",
            "start": 449,
            "end": 580,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_46@5",
            "content": "We will leave the investigation of better review combination methods for future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_46",
            "start": 582,
            "end": 666,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_47@0",
            "content": "R 1 R 2 R L longest-",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_47",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_48@0",
            "content": "Case Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_48",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_49@0",
            "content": "We study some cases for a better understanding of the structure-controllable generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_49",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_50@0",
            "content": "Identify the control label for each sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_50",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_50@1",
            "content": "We first evaluate whether the model is able to attend to the correct control label during generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_50",
            "start": 46,
            "end": 146,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_50@2",
            "content": "For each generation step, we obtain the cross attention weights from the decoder's output token towards the control labels, and plot them in Figure 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_50",
            "start": 148,
            "end": 297,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_50@3",
            "content": "The given control sequence is \"abstract | weakness | decision\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_50",
            "start": 299,
            "end": 361,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_50@4",
            "content": "When generating each sentence, we can see that the attention weights of the corresponding control token are the highest, which demonstrates that our model can effectively pay attention to the correct control label and thus generate the content complying with the intent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_50",
            "start": 363,
            "end": 632,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_51@0",
            "content": "To understand what information the model attends to when generating each sentence, we aggregate the cross attention weights to obtain the attention scores from each generated sentence towards all input sentences (Appendix C.5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_51",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_51@1",
            "content": "Then, we select the top 3 input sentences with the highest attention scores for each generated sentence, and visualize the normalized attention weights on all tokens in",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_51",
            "start": 228,
            "end": 395,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_52@0",
            "content": "Sent 1 (abstract):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_52",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_53@0",
            "content": "This paper proposes a selfsupervised contrastive learning method for few-shot learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_53",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_54@0",
            "content": "Sent 2 (weakness):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_54",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_55@0",
            "content": "The reviewers agree that the idea is interesting, but have concerns about the clarity of the paper and the lack of comparison to the baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_55",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_56@0",
            "content": "The paper is not suitable for publication at ICLR in its current form.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_56",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_57@0",
            "content": "Table 8: Attention analysis for each output sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_57",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_58@0",
            "content": "the selected sentences and the control sequence in Table 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_58",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_58@1",
            "content": "As shown, the model can correctly extract relevant information from the source sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_58",
            "start": 60,
            "end": 148,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_58@2",
            "content": "For example, it identifies important phrases such as \"interesting\", \"clarity\" and \"lack of comparison to baselines\" when generating \"Sent 2\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_58",
            "start": 150,
            "end": 290,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_59@0",
            "content": "Generate varied outputs given different control sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_59",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_59@1",
            "content": "To further investigate the effectiveness of the control sequence, we change the control sequence of the above example and re-generate the meta-reviews given the same input reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_59",
            "start": 59,
            "end": 238,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_59@2",
            "content": "In Table 7, we first show the gold meta-review and the model output using the original control sequence in Row 0 and Row 1, and then show the model outputs with alternative control sequences in Row 2 and Row 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_59",
            "start": 240,
            "end": 449,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_59@3",
            "content": "From the outputs, we can see that indeed each generated sentence corresponds to its control label well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_59",
            "start": 451,
            "end": 553,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_59@4",
            "content": "In Row 2, we add an additional control label in the sequence and by repeating the \"abstract\" label, the generator can further elaborate more details of the studied method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_59",
            "start": 555,
            "end": 725,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_59@5",
            "content": "This is one key advantage of our sent-ctrl compared to the seg-ctrl, which allows the control of length and the level of the generation details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_59",
            "start": 727,
            "end": 870,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_59@6",
            "content": "In Row 3, a very comprehensive control sequence is specified.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_59",
            "start": 872,
            "end": 932,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_59@7",
            "content": "We can see that the output meta-review is quite fluent and polite to reject the borderline paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_59",
            "start": 934,
            "end": 1030,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_59@8",
            "content": "See Appendix C.6 for more examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_59",
            "start": 1032,
            "end": 1066,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_60@0",
            "content": "Human Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_60",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_61@0",
            "content": "In addition to the Rouge evaluation, we ask 3 human judges to manually assess the generation quality of the bart-large-cnn model trained under different control methods from Table 5 on 100 random test instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_61",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_61@1",
            "content": "For each test instance, we provide the judges with the input reviews and randomly ordered generations from different models, and ask them to individually evaluate the generations based on the following criteria: (1) Fluency: is the generation fluent, grammatical, and without unnecessary repetitions? (2) Content Relevance: does the generation reflect the review content well, or does it produce general but trivial sentences? (3) Structure Similarity: how close does the generation structure resemble the gold structure (i.e., the control sequence)? ( 4) Decision Correctness: does the generation correctly predicts the gold human decision?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_61",
            "start": 212,
            "end": 852,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_62@0",
            "content": "We grade fluency and content relevance on a scale of 1 to 5, whereas structure similarity and decision correctness are calculated from 0 to 1 (Appendix C.7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_62",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_62@1",
            "content": "For structure similarity, because sent-ctrl and seg-ctrl have different control sequences, we evaluate the two models on sentence-level (sent) and segment-level (seg) structures respectively, and provide both evaluations for unctrl.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_62",
            "start": 158,
            "end": 389,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_63@0",
            "content": "As shown in Table 9, both sent-ctrl and seg-ctrl models show significant improvements on the generation structure over the uncontrolled baseline, which affirms the effectiveness of our proposed methods for structure-controllable generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_63",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_63@1",
            "content": "Sentctrl also has better fluency and decision correctness, suggesting that having a better output structure can benefit readability and decision generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_63",
            "start": 241,
            "end": 396,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_63@2",
            "content": "For the content relevance, the scores of all methods are reasonably good, and significance tests cannot prove any best model (p > 0.08).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_63",
            "start": 398,
            "end": 533,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_63@3",
            "content": "Nevertheless, it is possible that the looser control a method applies, the better relevance score it achieves.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_63",
            "start": 535,
            "end": 644,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_63@4",
            "content": "It is because a tighter control narrows the content that a model can use from the reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_63",
            "start": 646,
            "end": 735,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_64@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_64",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_65@0",
            "content": "To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_65",
            "start": 0,
            "end": 296,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_65@1",
            "content": "Datasets for long documents include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_65",
            "start": 298,
            "end": 399,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_65@2",
            "content": "In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely, structurecontrollable text generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_65",
            "start": 401,
            "end": 696,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_66@0",
            "content": "Researchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_66",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_66@1",
            "content": "Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_66",
            "start": 281,
            "end": 377,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_66@2",
            "content": "Our work is the first fully-annotated dataset in this domain for the structure-controllable generation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_66",
            "start": 379,
            "end": 486,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_66@3",
            "content": "There are also some datasets and annotation schemes on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain and cannot be easily transferred to our task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_66",
            "start": 488,
            "end": 722,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_67@0",
            "content": "A wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality , domains (Takeno et al., 2017) and persona ) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_67",
            "start": 0,
            "end": 354,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_67@1",
            "content": "Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_67",
            "start": 356,
            "end": 571,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_67@2",
            "content": "Our task also differs from content planning (Reiter and Dale, 1997;Shao et al., 2019;, which involves explicitly selecting and arranging the input content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_67",
            "start": 573,
            "end": 727,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_67@3",
            "content": "Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_67",
            "start": 729,
            "end": 863,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_68@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_68",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_69@0",
            "content": "This paper introduces a fully-annotated text generation dataset MReD in a new domain, i.e., the meta-reviews in the peer review system, and provides thorough data analysis to better understand the data characteristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_69",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_69@1",
            "content": "With such rich annotations, we propose simple yet effective methods for structure-controllable text generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_69",
            "start": 219,
            "end": 329,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_69@2",
            "content": "Extensive experimental results are presented as baselines for future study and thorough result analysis is conducted to shed light on the control mechanisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_69",
            "start": 331,
            "end": 487,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_70@0",
            "content": "Ethical Concerns",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_70",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_71@0",
            "content": "We have obtained approval from ICLR organizers to use the data collected from ICLR 2018-2021 on OpenReview.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_71",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_71@1",
            "content": "having \"rebuttal process\" is larger for submissions with lower scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_71",
            "start": 108,
            "end": 177,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_71@2",
            "content": "This suggests that the rebuttal process plays an important role in the peer review process, especially in helping the borderline papers to be accepted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_71",
            "start": 179,
            "end": 329,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_71@3",
            "content": "On the other hand, for rejected papers, the percentage of meta-reviews having \"strength\" increases as the average score increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_71",
            "start": 331,
            "end": 460,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_71@4",
            "content": "This coincides with our common sense that the submissions receiving higher scores tend to have more strengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_71",
            "start": 462,
            "end": 571,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_71@5",
            "content": "One interesting finding here is that the percentage of \"weakness\" and \"suggestion\" also increases as the average rating score increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_71",
            "start": 573,
            "end": 708,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_71@6",
            "content": "This may be due to two main reasons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_71",
            "start": 710,
            "end": 745,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_71@7",
            "content": "First, to reject a submission with higher scores, the area chair has to explain the weakness with more details and provide more suggestions for authors to further improve their submissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_71",
            "start": 747,
            "end": 935,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_71@8",
            "content": "Second, compared to the percentage of \"strength\", \"weakness\" definitely has a larger percentage within any range of rating scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_71",
            "start": 937,
            "end": 1066,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_71@9",
            "content": "The difference in the percentage of \"strength\" and \"weakness\" is intuitively different between the accepted papers and the rejected papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_71",
            "start": 1068,
            "end": 1206,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_72@0",
            "content": "We provide baselines of uncontrolled generation and controlled generation on MReD using other common Transformer pretrained models in Table 13.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_72",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_72@1",
            "content": "Note that due to limited GPU space, we cannot fit 2048 input tokens for T5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_72",
            "start": 144,
            "end": 218,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_72@2",
            "content": "Thus, for fair comparison, all results shown are from source truncation of 1024.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_72",
            "start": 220,
            "end": 299,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_73@0",
            "content": "To obtain labels on source input, we train a tagger based on the human-annotated meta-reviews, then use it to predict labels on the input sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_73",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_73@1",
            "content": "Specifically, we define the task as a sequence labeling problem and apply the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks with a conditional random field (CRF) (Lafferty et al., 2001) (i.e., LSTM-CRF (Lample et al., 2016)) model on the annotated MReD dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_73",
            "start": 149,
            "end": 437,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_74@0",
            "content": "The same data split as the meta-review generation task is used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_74",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_74@1",
            "content": "We adopt the standard IOBES tagging scheme (Ramshaw, 1995;Ratinov and Roth, 2009), and fine-tune BERT (Devlin et al., 2019) and RoBERTa models in Hugging Face.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_74",
            "start": 64,
            "end": 222,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_74@2",
            "content": "All models are trained for 30 epochs with an early stop of 20, and each epoch takes about 30 minutes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_74",
            "start": 224,
            "end": 324,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_74@3",
            "content": "We select the best model parameters based on the best micro F 1 score on the development set and apply it to the test set for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_74",
            "start": 326,
            "end": 462,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_75@0",
            "content": "All models are run with single V100 GPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_75",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_75@1",
            "content": "We use Adam (Kingma and Ba, 2014) with an initial learning rate of 2e-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_75",
            "start": 42,
            "end": 113,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_75@2",
            "content": "We report the F 1 scores for each category as well as the overall micro F 1 and macro F 1 scores in Table 14.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_75",
            "start": 115,
            "end": 223,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_75@3",
            "content": "Micro F1 is the overall accuracy regardless of the categories, whereas macro F1 is an average of per category accuracy evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_75",
            "start": 225,
            "end": 354,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_75@4",
            "content": "Since some of the category labels (eg. \"ac disagreement\") are very rare, their classification accuracy is low.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_75",
            "start": 356,
            "end": 465,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_75@5",
            "content": "Overall, micro F1 is a more important metric since it suggests general performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_75",
            "start": 467,
            "end": 549,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_75@6",
            "content": "The results stand proof that the majority of the categories have their own characteristics that can be identified from other categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_75",
            "start": 551,
            "end": 686,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_75@7",
            "content": "RoBERTabase is the best performing model, therefore we use this model to predict review sentence labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_75",
            "start": 688,
            "end": 791,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_76@0",
            "content": "Besides the baselines of \"Source Generic\" and \"Target Generic\", we explore subsets of papers with high scores (average reviewers' rating \u2a7e 7) or low scores (average reviewers' rating \u2a7d 3) to obtain 4 generic baselines: \"Source High Score\", \"Source Low Score\", \"Target High Score\", \"Target Low Score\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_76",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_77@0",
            "content": "We use \"Target High Score\" as an example to explain how we obtain the generic sentences: From the training subset of high score papers, We first separate all meta-review sentences into the corresponding label categories, obtaining a total of 9 groups of sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_77",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_77@1",
            "content": "Then, we re-arrange the sentences in each group using TextRank (our best extractive model).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_77",
            "start": 265,
            "end": 355,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_77@2",
            "content": "Since TextRank ranks the input sentences based on each sentence's content connection with others, sentences with higher rankings are also more general in the sense that they have more shared content with others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_77",
            "start": 357,
            "end": 567,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_78@0",
            "content": "After obtaining the generic sentence sets, we can create baseline generations using the sent-ctrl sequence on the corresponding high score paper test data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_78",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_78@1",
            "content": "We avoid using the same sentence twice inside the same generation, so if the same label appears multiple times in a control sequence, we will use the same number of generic sentences for that category down the ranking order.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_78",
            "start": 156,
            "end": 379,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_79@0",
            "content": "All generic sentence baselines can be obtained in a similarly procedure as outlined above, and we show results in Table 15.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_79",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_79@1",
            "content": "Both \"Target High Score\" and \"Target Low Score\" perform much better than the \"Target Genric\" baseline, suggesting that papers with very high or low scores tend to have more typical patterns in their meta-reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_79",
            "start": 124,
            "end": 335,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_79@2",
            "content": "Nevertheless, the pattern is less evident in the source (reviews) baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_79",
            "start": 337,
            "end": 412,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_80@0",
            "content": "By default, the Transformers truncate the source to 1024 tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_80",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_80@1",
            "content": "We further investigate the performance of different source truncation lengths under the setting of rate-concat.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_80",
            "start": 65,
            "end": 175,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_80@2",
            "content": "As shown in Table 18, truncating the source to 2048 tokens consistently achieves the best performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_80",
            "start": 177,
            "end": 278,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_81@0",
            "content": "During generation, we can obtain the attention weights of each output token towards all input tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_81",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_81@1",
            "content": "Specifically, we average all decoder layers' cross attention weights for the same output token generated at each decoding step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_81",
            "start": 102,
            "end": 228,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_81@2",
            "content": "We then calculate an attention value for that output token on each input sentence, by aggregating the token's attention weights on the list of input tokens that belong to the same sentence by max pooling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_81",
            "start": 230,
            "end": 433,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_81@3",
            "content": "Finally, we can calculate an output-sentence-to-input-sentence attention score, by adding up these attention values for the output tokens that belong to the same sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_81",
            "start": 435,
            "end": 605,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_82@0",
            "content": "Common attention aggregation methods include summation, average-pooling, and max-pooling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_82",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_82@1",
            "content": "We use max-pooling to aggregate attention for same-sentence input tokens, because summation unfairly gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_82",
            "start": 90,
            "end": 398,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_82@2",
            "content": "With max-pooling, we can correctly identify sentences with spiked attention at important phrases, regardless of sentence lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_82",
            "start": 400,
            "end": 528,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_82@3",
            "content": "For attention aggregation on the samesentence output tokens, summation is used and can be viewed as allowing each output token to vote an attention score on all input sentences, so that the input sentence receiving the highest total score is the most relevant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_82",
            "start": 530,
            "end": 789,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_82@4",
            "content": "We conduct trial runs of all aggregation methods on input tokens with summation for output-token aggregation for multiple generation examples, and indeed max-pooling outperforms the other two by identifying more relevant input sentences with the generated sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_82",
            "start": 791,
            "end": 1055,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_82@5",
            "content": "Once we have the attention scores, we can attribute the generation of each output sentence to a few topmost relevant input sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_82",
            "start": 1057,
            "end": 1189,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_82@6",
            "content": "Then, we can draw a color map of the input tokens in the selected sentences based on their relative attention weights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_82",
            "start": 1191,
            "end": 1308,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_83@0",
            "content": "We show examples of the generation results using alternative control sequences on another submission in Table 16.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_83",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_83@1",
            "content": "We can see the effectiveness of controlling the output structure using our proposed method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_83",
            "start": 114,
            "end": 204,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_84@0",
            "content": "For structure similarity, we instruct the judges to label each generated sentence with the closest category.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_84",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_84@1",
            "content": "We then calculate the normalized token-level edit distance between the judge-annotated label sequence and the given control sequence, where each label is considered as a single token, and finally deduct this value from 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_84",
            "start": 109,
            "end": 329,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_85@0",
            "content": "For decision correctness, we evaluate it on a binary scale where 1 indicates complete correctness and 0 otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_85",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_85@1",
            "content": "More specifically, we give 0 if the generation produces either contradictory decisions or a wrong decision, or if the generation does not show enough hints for rejection or acceptance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_85",
            "start": 115,
            "end": 298,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_86@0",
            "content": "Chaitanya Bhatia, Tribikram Pradhan, Metagen: An academic meta-review generation system, 2020, Proceedings of ACM-SIGIR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_86",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_87@0",
            "content": "Jaime Carbonell, Jade Goldstein, The use of mmr, diversity-based reranking for reordering documents and producing summaries, 1998, Proceedings of ACM-SIGIR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_87",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_88@0",
            "content": "Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, Luo Si, Argument pair extraction from peer review and rebuttal via multi-task learning, 2020, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_88",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_89@0",
            "content": "Arman Cohan, Franck Dernoncourt, Soon Doo, Trung Kim, Seokhwan Bui, Walter Kim, Nazli Chang,  Goharian, A discourse-aware attention model for abstractive summarization of long documents, 2018, Proceedings of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_89",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_90@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_90",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_91@0",
            "content": "Yuguang Duan, Canwen Xu, Jiaxin Pei, Jialong Han, Chenliang Li, Pre-train and plug-in: Flexible conditional text generation with variational autoencoders, 2020, Proceedings of the ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_91",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_92@0",
            "content": "G\u00fcnes Erkan,  Dragomir R Radev, Lexrank: Graph-based lexical centrality as salience in text summarization, 2004, Artificial Intelligence Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_92",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_93@0",
            "content": "Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir Radev, Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model, 2019, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_93",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_94@0",
            "content": "Angela Fan, David Grangier, Michael Auli, Controllable abstractive summarization, 2018, Proceedings of WNGT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_94",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_95@0",
            "content": "Angela Fan, Mike Lewis, Yann Dauphin, Hierarchical neural story generation, 2018, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_95",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_96@0",
            "content": "Beatriz Fisas, Francesco Ronzano, Horacio Saggion, A multi-layered annotated corpus of scientific papers, 2016, Proceedings of LREC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_96",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_97@0",
            "content": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, The webnlg challenge: Generating text from rdf data, 2017, Proceedings of INLG, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_97",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_98@0",
            "content": "Max Grusky, Mor Naaman, Yoav Artzi, Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies, 2018, Proceedings of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_98",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_99@0",
            "content": "Karl Moritz Hermann, Tom\u00e1s Kocisk\u1ef3, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend, 2015, Proceedings of NIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_99",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_100@0",
            "content": "Sepp Hochreiter, J\u00fcrgen Schmidhuber, Long short-term memory, 1997, Neural Computation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_100",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_101@0",
            "content": "Xinyu Hua, Mitko Nikolov, Nikhil Badugu, Lu Wang, Argument mining for understanding peer reviews, 2019, Proceedings of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_101",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_102@0",
            "content": "Xinyu Hua, Lu Wang, Sentence-level content planning and style specification for neural text generation, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_102",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_103@0",
            "content": "Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard Hovy, Roy Schwartz, A dataset of peer reviews (peerread): Collection, insights and nlp applications, 2018, Proceedings of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_103",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_104@0",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_104",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_105@0",
            "content": "Wojciech Kry\u015bci\u0144ski, Romain Paulus, Caiming Xiong, Richard Socher, Improving abstraction in text summarization, 2018, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_105",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_106@0",
            "content": "John Lafferty, Andrew Mccallum, Fernando Cn Pereira, Conditional random fields: Probabilistic models for segmenting and labeling sequence data, 2001, Proceedings of ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_106",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_107@0",
            "content": "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer, Neural architectures for named entity recognition, 2016, Proceedings of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_107",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_108@0",
            "content": "Anne Lauscher, Goran Glava\u0161, Kai Eckert, Arguminsci: A tool for analyzing argumentation and rhetorical aspects in scientific writing, 2018, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_108",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_109@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_109",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_110@0",
            "content": "Maria Liakata, Simone Teufel, Advaith Siddharthan, Colin Batchelor, Corpora for the conceptualisation and zoning of scientific papers, 2010, Proceedings of LREC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_110",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_111@0",
            "content": "Yi Liao, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, Tong Zhang, QuaSE: Sequence editing under quantifiable guidance, 2018, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_111",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_112@0",
            "content": "Chin-Yew Lin, Rouge: A package for automatic evaluation of summaries, 2004, Text summarization branches out, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_112",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_113@0",
            "content": "Yang Liu, Mirella Lapata, Hierarchical transformers for multi-document summarization, 2019, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_113",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_114@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_114",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_115@0",
            "content": "Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnab\u00e1s P\u00f3czos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan Black, Shrimai Prabhumoye, Politeness transfer: A tag and generate approach, 2020, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_115",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_116@0",
            "content": "Rada Mihalcea, Paul Tarau, Textrank: Bringing order into text, 2004, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_116",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_117@0",
            "content": "Ramesh Nallapati, Bowen Zhou, \u00c7aglar Cicero Dos Santos, Bing Gul\u00e7ehre,  Xiang, Abstractive text summarization using sequence-to-sequence rnns and beyond, 2016, Proceedings of SIGNLL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_117",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_118@0",
            "content": "Courtney Napoles, Benjamin Matthew R Gormley,  Van Durme, Annotated gigaword, 2012, Proceedings of AKBC-WEKEX, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_118",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_119@0",
            "content": "Shashi Narayan, B Shay, Mirella Cohen,  Lapata, Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization, 2018, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_119",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_120@0",
            "content": "Paul Over, James Yen, An introduction to duc-2004, 2004, Proceedings of DUC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_120",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_121@0",
            "content": "Karolina Owczarzak, Hoa Dang, Overview of the tac 2011 summarization track: Guided task and aesop task, 2011, Proceedings of TAC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_121",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_122@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_122",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_123@0",
            "content": "La Ramshaw, Text chunking using transformation-based learning, 1995, Proceedings of Third Workshop on Very Large Corpora, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_123",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_124@0",
            "content": "Lev Ratinov, Dan Roth, Design challenges and misconceptions in named entity recognition, 2009, Proceedings of CoNLL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_124",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_125@0",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, 2019, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_125",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_126@0",
            "content": "Ehud Reiter, Robert Dale, Building applied natural language generation systems, 1997, Natural Language Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_126",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_127@0",
            "content": "UNKNOWN, None, 2008, The new york times annotated corpus. Linguistic Data Consortium, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_127",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_128@0",
            "content": "Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan Zhao, Shuming Shi, Rui Yan, Semi-supervised text style transfer: Cross projection in latent space, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_128",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_129@0",
            "content": "Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu, Long and diverse text generation with planning-based hierarchical variational model, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_129",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_130@0",
            "content": "Eva Sharma, Chen Li, Lu Wang, Bigpatent: A large-scale dataset for abstractive and coherent summarization, 2019, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_130",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_131@0",
            "content": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola, Style transfer from non-parallel text by cross-alignment, 2017, Proceedings of NIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_131",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_132@0",
            "content": "Shunsuke Takeno, Masaaki Nagata, Kazuhide Yamamoto, Controlling target features in neural machine translation via prefix constraints, 2017, Proceedings of WAT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_132",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_133@0",
            "content": "Jiwei Tan, Xiaojun Wan, Jianguo Xiao, Abstractive document summarization with a graphbased attentional neural model, 2017, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_133",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_134@0",
            "content": "Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric Xing, Zhiting Hu, Targetguided open-domain conversation, 2019, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_134",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_135@0",
            "content": "Simone Teufel, Jean Carletta, Marc Moens, An annotation scheme for discourse-level argumentation in research articles, 1999, Proceedings of EACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_135",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_136@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Proceedings of NIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_136",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_137@0",
            "content": "Yunli Wang, Yu Wu, Lili Mou, Zhoujun Li, Wenhan Chao, Harnessing pre-trained neural networks with rules for formality style transfer, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_137",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_138@0",
            "content": "Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, Transformers: State-of-theart natural language processing, 2020, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_138",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_139@0",
            "content": "Xinyu Xing, Xiaosheng Fan, Xiaojun Wan, Automatic generation of citation texts in scholarly papers: A pilot study, 2020, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_139",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_140@0",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, Pegasus: Pre-training with extracted gap-sentences for abstractive summarization, 2020, Proceedings of ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_140",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "124-ARR_v2_141@0",
            "content": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston, Personalizing dialogue agents: I have a dog, do you have pets too?, 2018, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v2_141",
            "start": 0,
            "end": 178,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_1",
            "tgt_ix": "124-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_1",
            "tgt_ix": "124-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_1",
            "tgt_ix": "124-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_2",
            "tgt_ix": "124-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_3",
            "tgt_ix": "124-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_5",
            "tgt_ix": "124-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_6",
            "tgt_ix": "124-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_7",
            "tgt_ix": "124-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_9",
            "tgt_ix": "124-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_10",
            "tgt_ix": "124-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_11",
            "tgt_ix": "124-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_4",
            "tgt_ix": "124-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_4",
            "tgt_ix": "124-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_4",
            "tgt_ix": "124-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_4",
            "tgt_ix": "124-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_4",
            "tgt_ix": "124-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_4",
            "tgt_ix": "124-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_4",
            "tgt_ix": "124-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_4",
            "tgt_ix": "124-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_4",
            "tgt_ix": "124-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_12",
            "tgt_ix": "124-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_13",
            "tgt_ix": "124-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_13",
            "tgt_ix": "124-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_13",
            "tgt_ix": "124-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_14",
            "tgt_ix": "124-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_15",
            "tgt_ix": "124-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_15",
            "tgt_ix": "124-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_13",
            "tgt_ix": "124-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_16",
            "tgt_ix": "124-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_18",
            "tgt_ix": "124-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_17",
            "tgt_ix": "124-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_17",
            "tgt_ix": "124-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_17",
            "tgt_ix": "124-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_13",
            "tgt_ix": "124-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_19",
            "tgt_ix": "124-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_21",
            "tgt_ix": "124-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_22",
            "tgt_ix": "124-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_23",
            "tgt_ix": "124-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_24",
            "tgt_ix": "124-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_20",
            "tgt_ix": "124-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_20",
            "tgt_ix": "124-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_20",
            "tgt_ix": "124-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_20",
            "tgt_ix": "124-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_20",
            "tgt_ix": "124-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_20",
            "tgt_ix": "124-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_25",
            "tgt_ix": "124-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_26",
            "tgt_ix": "124-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_26",
            "tgt_ix": "124-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_27",
            "tgt_ix": "124-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_29",
            "tgt_ix": "124-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_30",
            "tgt_ix": "124-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_32",
            "tgt_ix": "124-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_28",
            "tgt_ix": "124-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_28",
            "tgt_ix": "124-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_28",
            "tgt_ix": "124-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_28",
            "tgt_ix": "124-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_28",
            "tgt_ix": "124-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_28",
            "tgt_ix": "124-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_33",
            "tgt_ix": "124-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_34",
            "tgt_ix": "124-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_34",
            "tgt_ix": "124-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_35",
            "tgt_ix": "124-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_35",
            "tgt_ix": "124-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_35",
            "tgt_ix": "124-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_34",
            "tgt_ix": "124-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_37",
            "tgt_ix": "124-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_39",
            "tgt_ix": "124-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_40",
            "tgt_ix": "124-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_38",
            "tgt_ix": "124-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_38",
            "tgt_ix": "124-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_38",
            "tgt_ix": "124-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_38",
            "tgt_ix": "124-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_34",
            "tgt_ix": "124-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_41",
            "tgt_ix": "124-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_43",
            "tgt_ix": "124-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_42",
            "tgt_ix": "124-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_42",
            "tgt_ix": "124-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_42",
            "tgt_ix": "124-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_34",
            "tgt_ix": "124-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_44",
            "tgt_ix": "124-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_46",
            "tgt_ix": "124-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_45",
            "tgt_ix": "124-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_45",
            "tgt_ix": "124-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_45",
            "tgt_ix": "124-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_34",
            "tgt_ix": "124-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_47",
            "tgt_ix": "124-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_49",
            "tgt_ix": "124-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_50",
            "tgt_ix": "124-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_52",
            "tgt_ix": "124-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_53",
            "tgt_ix": "124-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_54",
            "tgt_ix": "124-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_51",
            "tgt_ix": "124-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_56",
            "tgt_ix": "124-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_57",
            "tgt_ix": "124-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_58",
            "tgt_ix": "124-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_55",
            "tgt_ix": "124-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_34",
            "tgt_ix": "124-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_59",
            "tgt_ix": "124-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_61",
            "tgt_ix": "124-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_62",
            "tgt_ix": "124-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_60",
            "tgt_ix": "124-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_60",
            "tgt_ix": "124-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_60",
            "tgt_ix": "124-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_60",
            "tgt_ix": "124-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_63",
            "tgt_ix": "124-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_65",
            "tgt_ix": "124-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_66",
            "tgt_ix": "124-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_64",
            "tgt_ix": "124-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_64",
            "tgt_ix": "124-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_64",
            "tgt_ix": "124-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_64",
            "tgt_ix": "124-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_67",
            "tgt_ix": "124-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_68",
            "tgt_ix": "124-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_68",
            "tgt_ix": "124-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_69",
            "tgt_ix": "124-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_73",
            "tgt_ix": "124-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_74",
            "tgt_ix": "124-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_72",
            "tgt_ix": "124-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_76",
            "tgt_ix": "124-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_77",
            "tgt_ix": "124-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_78",
            "tgt_ix": "124-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_75",
            "tgt_ix": "124-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_79",
            "tgt_ix": "124-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_81",
            "tgt_ix": "124-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_80",
            "tgt_ix": "124-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_82",
            "tgt_ix": "124-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_84",
            "tgt_ix": "124-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_83",
            "tgt_ix": "124-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v2_0",
            "tgt_ix": "124-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_1",
            "tgt_ix": "124-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_2",
            "tgt_ix": "124-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_2",
            "tgt_ix": "124-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_2",
            "tgt_ix": "124-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_2",
            "tgt_ix": "124-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_2",
            "tgt_ix": "124-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_2",
            "tgt_ix": "124-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_2",
            "tgt_ix": "124-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_2",
            "tgt_ix": "124-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_3",
            "tgt_ix": "124-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_3",
            "tgt_ix": "124-ARR_v2_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_3",
            "tgt_ix": "124-ARR_v2_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_4",
            "tgt_ix": "124-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_5",
            "tgt_ix": "124-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_5",
            "tgt_ix": "124-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_5",
            "tgt_ix": "124-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_6",
            "tgt_ix": "124-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_6",
            "tgt_ix": "124-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_6",
            "tgt_ix": "124-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_6",
            "tgt_ix": "124-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_7",
            "tgt_ix": "124-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_7",
            "tgt_ix": "124-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_7",
            "tgt_ix": "124-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_7",
            "tgt_ix": "124-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_7",
            "tgt_ix": "124-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_8",
            "tgt_ix": "124-ARR_v2_8@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_9",
            "tgt_ix": "124-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_10",
            "tgt_ix": "124-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_10",
            "tgt_ix": "124-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_11",
            "tgt_ix": "124-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_11",
            "tgt_ix": "124-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_12",
            "tgt_ix": "124-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_12",
            "tgt_ix": "124-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_13",
            "tgt_ix": "124-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_14",
            "tgt_ix": "124-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_14",
            "tgt_ix": "124-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_14",
            "tgt_ix": "124-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_14",
            "tgt_ix": "124-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_15",
            "tgt_ix": "124-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_16",
            "tgt_ix": "124-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_16",
            "tgt_ix": "124-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_16",
            "tgt_ix": "124-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_16",
            "tgt_ix": "124-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_16",
            "tgt_ix": "124-ARR_v2_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_16",
            "tgt_ix": "124-ARR_v2_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_16",
            "tgt_ix": "124-ARR_v2_16@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_16",
            "tgt_ix": "124-ARR_v2_16@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_17",
            "tgt_ix": "124-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_18",
            "tgt_ix": "124-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_18",
            "tgt_ix": "124-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_18",
            "tgt_ix": "124-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_18",
            "tgt_ix": "124-ARR_v2_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_18",
            "tgt_ix": "124-ARR_v2_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_18",
            "tgt_ix": "124-ARR_v2_18@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_19",
            "tgt_ix": "124-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_19",
            "tgt_ix": "124-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_19",
            "tgt_ix": "124-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_19",
            "tgt_ix": "124-ARR_v2_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_20",
            "tgt_ix": "124-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_21",
            "tgt_ix": "124-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_22",
            "tgt_ix": "124-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_22",
            "tgt_ix": "124-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_22",
            "tgt_ix": "124-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_22",
            "tgt_ix": "124-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_22",
            "tgt_ix": "124-ARR_v2_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_22",
            "tgt_ix": "124-ARR_v2_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_23",
            "tgt_ix": "124-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_23",
            "tgt_ix": "124-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_23",
            "tgt_ix": "124-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_23",
            "tgt_ix": "124-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_23",
            "tgt_ix": "124-ARR_v2_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_23",
            "tgt_ix": "124-ARR_v2_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_23",
            "tgt_ix": "124-ARR_v2_23@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_24",
            "tgt_ix": "124-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_24",
            "tgt_ix": "124-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_24",
            "tgt_ix": "124-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_24",
            "tgt_ix": "124-ARR_v2_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_24",
            "tgt_ix": "124-ARR_v2_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_24",
            "tgt_ix": "124-ARR_v2_24@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_25",
            "tgt_ix": "124-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_26",
            "tgt_ix": "124-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_27",
            "tgt_ix": "124-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_27",
            "tgt_ix": "124-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_27",
            "tgt_ix": "124-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_28",
            "tgt_ix": "124-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_29",
            "tgt_ix": "124-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_29",
            "tgt_ix": "124-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_29",
            "tgt_ix": "124-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_30",
            "tgt_ix": "124-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_30",
            "tgt_ix": "124-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_30",
            "tgt_ix": "124-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_30",
            "tgt_ix": "124-ARR_v2_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_30",
            "tgt_ix": "124-ARR_v2_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_31",
            "tgt_ix": "124-ARR_v2_31@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_32",
            "tgt_ix": "124-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_32",
            "tgt_ix": "124-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_32",
            "tgt_ix": "124-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_32",
            "tgt_ix": "124-ARR_v2_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_32",
            "tgt_ix": "124-ARR_v2_32@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_32",
            "tgt_ix": "124-ARR_v2_32@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_32",
            "tgt_ix": "124-ARR_v2_32@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_32",
            "tgt_ix": "124-ARR_v2_32@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_32",
            "tgt_ix": "124-ARR_v2_32@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_33",
            "tgt_ix": "124-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_33",
            "tgt_ix": "124-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_33",
            "tgt_ix": "124-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_33",
            "tgt_ix": "124-ARR_v2_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_33",
            "tgt_ix": "124-ARR_v2_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_34",
            "tgt_ix": "124-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_35",
            "tgt_ix": "124-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_36@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_36@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_36@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_36@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_36@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_36",
            "tgt_ix": "124-ARR_v2_36@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_37",
            "tgt_ix": "124-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_37",
            "tgt_ix": "124-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_37",
            "tgt_ix": "124-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_37",
            "tgt_ix": "124-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_37",
            "tgt_ix": "124-ARR_v2_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_37",
            "tgt_ix": "124-ARR_v2_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_37",
            "tgt_ix": "124-ARR_v2_37@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_38",
            "tgt_ix": "124-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_39",
            "tgt_ix": "124-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_39",
            "tgt_ix": "124-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_39",
            "tgt_ix": "124-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_40",
            "tgt_ix": "124-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_40",
            "tgt_ix": "124-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_40",
            "tgt_ix": "124-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_41",
            "tgt_ix": "124-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_41",
            "tgt_ix": "124-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_41",
            "tgt_ix": "124-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_41",
            "tgt_ix": "124-ARR_v2_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_41",
            "tgt_ix": "124-ARR_v2_41@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_41",
            "tgt_ix": "124-ARR_v2_41@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_41",
            "tgt_ix": "124-ARR_v2_41@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_42",
            "tgt_ix": "124-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_43",
            "tgt_ix": "124-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_43",
            "tgt_ix": "124-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_43",
            "tgt_ix": "124-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_43",
            "tgt_ix": "124-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_43",
            "tgt_ix": "124-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_43",
            "tgt_ix": "124-ARR_v2_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_43",
            "tgt_ix": "124-ARR_v2_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_44",
            "tgt_ix": "124-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_44",
            "tgt_ix": "124-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_44",
            "tgt_ix": "124-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_45",
            "tgt_ix": "124-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_46",
            "tgt_ix": "124-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_46",
            "tgt_ix": "124-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_46",
            "tgt_ix": "124-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_46",
            "tgt_ix": "124-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_46",
            "tgt_ix": "124-ARR_v2_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_46",
            "tgt_ix": "124-ARR_v2_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_47",
            "tgt_ix": "124-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_48",
            "tgt_ix": "124-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_49",
            "tgt_ix": "124-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_50",
            "tgt_ix": "124-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_50",
            "tgt_ix": "124-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_50",
            "tgt_ix": "124-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_50",
            "tgt_ix": "124-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_50",
            "tgt_ix": "124-ARR_v2_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_51",
            "tgt_ix": "124-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_51",
            "tgt_ix": "124-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_52",
            "tgt_ix": "124-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_53",
            "tgt_ix": "124-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_54",
            "tgt_ix": "124-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_55",
            "tgt_ix": "124-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_56",
            "tgt_ix": "124-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_57",
            "tgt_ix": "124-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_58",
            "tgt_ix": "124-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_58",
            "tgt_ix": "124-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_58",
            "tgt_ix": "124-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_59",
            "tgt_ix": "124-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_59",
            "tgt_ix": "124-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_59",
            "tgt_ix": "124-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_59",
            "tgt_ix": "124-ARR_v2_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_59",
            "tgt_ix": "124-ARR_v2_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_59",
            "tgt_ix": "124-ARR_v2_59@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_59",
            "tgt_ix": "124-ARR_v2_59@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_59",
            "tgt_ix": "124-ARR_v2_59@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_59",
            "tgt_ix": "124-ARR_v2_59@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_60",
            "tgt_ix": "124-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_61",
            "tgt_ix": "124-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_61",
            "tgt_ix": "124-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_62",
            "tgt_ix": "124-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_62",
            "tgt_ix": "124-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_63",
            "tgt_ix": "124-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_63",
            "tgt_ix": "124-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_63",
            "tgt_ix": "124-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_63",
            "tgt_ix": "124-ARR_v2_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_63",
            "tgt_ix": "124-ARR_v2_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_64",
            "tgt_ix": "124-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_65",
            "tgt_ix": "124-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_65",
            "tgt_ix": "124-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_65",
            "tgt_ix": "124-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_66",
            "tgt_ix": "124-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_66",
            "tgt_ix": "124-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_66",
            "tgt_ix": "124-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_66",
            "tgt_ix": "124-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_67",
            "tgt_ix": "124-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_67",
            "tgt_ix": "124-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_67",
            "tgt_ix": "124-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_67",
            "tgt_ix": "124-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_68",
            "tgt_ix": "124-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_69",
            "tgt_ix": "124-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_69",
            "tgt_ix": "124-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_69",
            "tgt_ix": "124-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_70",
            "tgt_ix": "124-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_71@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_71@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_71@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_71@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_71",
            "tgt_ix": "124-ARR_v2_71@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_72",
            "tgt_ix": "124-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_72",
            "tgt_ix": "124-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_72",
            "tgt_ix": "124-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_73",
            "tgt_ix": "124-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_73",
            "tgt_ix": "124-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_74",
            "tgt_ix": "124-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_74",
            "tgt_ix": "124-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_74",
            "tgt_ix": "124-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_74",
            "tgt_ix": "124-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_75",
            "tgt_ix": "124-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_75",
            "tgt_ix": "124-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_75",
            "tgt_ix": "124-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_75",
            "tgt_ix": "124-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_75",
            "tgt_ix": "124-ARR_v2_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_75",
            "tgt_ix": "124-ARR_v2_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_75",
            "tgt_ix": "124-ARR_v2_75@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_75",
            "tgt_ix": "124-ARR_v2_75@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_76",
            "tgt_ix": "124-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_77",
            "tgt_ix": "124-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_77",
            "tgt_ix": "124-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_77",
            "tgt_ix": "124-ARR_v2_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_78",
            "tgt_ix": "124-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_78",
            "tgt_ix": "124-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_79",
            "tgt_ix": "124-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_79",
            "tgt_ix": "124-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_79",
            "tgt_ix": "124-ARR_v2_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_80",
            "tgt_ix": "124-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_80",
            "tgt_ix": "124-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_80",
            "tgt_ix": "124-ARR_v2_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_81",
            "tgt_ix": "124-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_81",
            "tgt_ix": "124-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_81",
            "tgt_ix": "124-ARR_v2_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_81",
            "tgt_ix": "124-ARR_v2_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_82",
            "tgt_ix": "124-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_82",
            "tgt_ix": "124-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_82",
            "tgt_ix": "124-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_82",
            "tgt_ix": "124-ARR_v2_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_82",
            "tgt_ix": "124-ARR_v2_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_82",
            "tgt_ix": "124-ARR_v2_82@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_82",
            "tgt_ix": "124-ARR_v2_82@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_83",
            "tgt_ix": "124-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_83",
            "tgt_ix": "124-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_84",
            "tgt_ix": "124-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_84",
            "tgt_ix": "124-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_85",
            "tgt_ix": "124-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_85",
            "tgt_ix": "124-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_86",
            "tgt_ix": "124-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_87",
            "tgt_ix": "124-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_88",
            "tgt_ix": "124-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_89",
            "tgt_ix": "124-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_90",
            "tgt_ix": "124-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_91",
            "tgt_ix": "124-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_92",
            "tgt_ix": "124-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_93",
            "tgt_ix": "124-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_94",
            "tgt_ix": "124-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_95",
            "tgt_ix": "124-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_96",
            "tgt_ix": "124-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_97",
            "tgt_ix": "124-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_98",
            "tgt_ix": "124-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_99",
            "tgt_ix": "124-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_100",
            "tgt_ix": "124-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_101",
            "tgt_ix": "124-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_102",
            "tgt_ix": "124-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_103",
            "tgt_ix": "124-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_104",
            "tgt_ix": "124-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_105",
            "tgt_ix": "124-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_106",
            "tgt_ix": "124-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_107",
            "tgt_ix": "124-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_108",
            "tgt_ix": "124-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_109",
            "tgt_ix": "124-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_110",
            "tgt_ix": "124-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_111",
            "tgt_ix": "124-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_112",
            "tgt_ix": "124-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_113",
            "tgt_ix": "124-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_114",
            "tgt_ix": "124-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_115",
            "tgt_ix": "124-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_116",
            "tgt_ix": "124-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_117",
            "tgt_ix": "124-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_118",
            "tgt_ix": "124-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_119",
            "tgt_ix": "124-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_120",
            "tgt_ix": "124-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_121",
            "tgt_ix": "124-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_122",
            "tgt_ix": "124-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_123",
            "tgt_ix": "124-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_124",
            "tgt_ix": "124-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_125",
            "tgt_ix": "124-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_126",
            "tgt_ix": "124-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_127",
            "tgt_ix": "124-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_128",
            "tgt_ix": "124-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_129",
            "tgt_ix": "124-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_130",
            "tgt_ix": "124-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_131",
            "tgt_ix": "124-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_132",
            "tgt_ix": "124-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_133",
            "tgt_ix": "124-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_134",
            "tgt_ix": "124-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_135",
            "tgt_ix": "124-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_136",
            "tgt_ix": "124-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_137",
            "tgt_ix": "124-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_138",
            "tgt_ix": "124-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_139",
            "tgt_ix": "124-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_140",
            "tgt_ix": "124-ARR_v2_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v2_141",
            "tgt_ix": "124-ARR_v2_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1056,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "124-ARR",
        "version": 2
    }
}