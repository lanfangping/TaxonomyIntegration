{
    "nodes": [
        {
            "ix": "124-ARR_v1_0",
            "content": "MReD: A Meta-Review Dataset for Structure-Controllable Text Generation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_2",
            "content": "When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited. A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences. A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with deep understanding of the domain knowledge. Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc. We present experimental results on start-of-the-art summarization models, and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data. By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "124-ARR_v1_4",
            "content": "Text generation entered a new era because of the development of neural network based generation techniques. Along the dimension of the mapping relation between the input information and the output text, we can roughly group the recent tasks into three clusters: more-to-less, less-to-more, and neck-to-neck. The more-to-less text generation tasks output a concise piece of text from some more abundant input, such as text summarization meta-review: [This paper studies n-step returns in off-policy RL and introduces a novel algorithm which adapts the return's horizon n in function of a notion of policy's age.]\u2190ABSTRACT [Over-all, the reviewers found that the paper presents interesting observations and promising experimental results.]\u2190STRENGTH [However, they also raised concerns in their initial reviews, regarding the clarity of the paper, its theoretical foundations and its positioning (notably regarding the bias/variance tradeoff of uncorrected n-step returns) and parts of the experimental results. ]\u2190WEAKNESS [In the absence of rebuttal or revised manuscript from the authors, not much discussion was triggered.]\u2190REBUTTAL PROCESS [Based on the initial reviews, the AC cannot recommend accepting this paper, but the authors are encouraged to pursue this interesting research direction.]\u2190DECISION Table 1: An example of annotated meta-review. CATE-GORY indicates the category of each sentence. (Tan et al., 2017;Kry\u015bci\u0144ski et al., 2018). The lessto-more generation tasks generate a more abundant output from some obviously simpler input, such as prompt-based story generation (Fan et al., 2018b). The neck-to-neck generation aims at generating an output text which conveys the same quantity of knowledge as the input but in natural language, such as typical RDF triples to text tasks (Gardent et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_5",
            "content": "To some extent, the existing task settings are not so adequate because they do not have deep understanding of the domains they are working on, i.e., domain knowledge. Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail (Nallapati et al., 2016) is composed of the training pairs of news content and news titles. However, it does not tell why a particular piece of news content should have that corresponding title, for example for the same earnings report, why one media emphasizes its new business success in the title, but another emphasizes its net income. Obviously, there is not a standard answer regarding right or wrong. For such cases, if we can specify a control signal, e.g., \"emphasizing new business\", the generated text would make more sense to users using the text generator.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_6",
            "content": "To allow controlling not only the intent of a single generated sentence but also the whole structure of a generated passage, we prepare a new dataset MReD (short for Meta-Review Dataset) with in-depth understanding of the structure of meta-reviews in a peer-reviewing system, namely the open review system of ICLR. MReD for the first time allows a generator to be trained by simultaneously taking the text (i.e. reviews) and the structure control signal as input to generate a meta-review which is not only derivable from the reviews but also complies with the control intent. Thus from the same input text, the trained generator can generate varied outputs according to the given control signal. For example, if the area chair is inclined to accept a borderline paper, he or she may invoke our generator with a structure of \"abstract | strength | decision\" to generate a meta-review, or may use a structure of \"abstract | weakness | suggestion\" otherwise. Note that for ease of preparation and explanation, we ground our dataset in the peer review domain. However, the data preparation methodology and proposed models are transferable to other domains, which is indeed what we hope to motivate with this effort. Specifically, we collect 7,089 meta-reviews of ICLR in recent years (2018 -2021) and fully annotate the dataset. Each sentence in a meta-review is classified into one of the 9 pre-defined intent categories: abstract, strength, weakness, rating summary, area chair (AC) disagreement, rebuttal process, suggestion, decision, and miscellaneous (misc). Table 1 shows an annotated example, where each sentence is classified into a single category that best describes the intent of this sentence. Our MReD is obviously different from previous text generation/summarization datasets because, given the rich annotations of individual meta-review sentences, a model is allowed to learn more sophisticated generation behaviors to control the structure of the generated passage. Our proposed task is also noticeably different from existing controllable text generation tasks (e.g., text style transfer on sentiment polarity (Shen et al., 2017;Liao et al., 2018) and formality (Shang et al., 2019)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_7",
            "content": "To summarize, our contributions are as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_8",
            "content": "(1) We introduce a fully-annotated meta-review dataset to make better use of the domain knowledge for text generation. Thorough data analysis (3) We design simple yet effective control methods that are independent of the model architecture. We show the effectiveness of enforcing different generation structures with a detailed model analysis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_9",
            "content": "We will release our full dataset, code, and detailed settings to the community.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_10",
            "content": "MReD: Meta-Review Dataset",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "124-ARR_v1_11",
            "content": "In this paper, we explore a new task, named the structure-controllable text generation, in a new domain, namely the meta-reviews in the peer reviewing system. Unlike previous datasets that mainly focus on domains like news, meta-review is a worthstudying domain containing essential and highdensity opinions. Specifically, during the peer review process of scientific papers, a senior reviewer or area chair will recommend a decision and manually write a meta-review to summarize the opinions from different reviews written by the reviewers. We first introduce the data collection process and then describe the annotation details, followed by dataset analysis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_12",
            "content": "Data Collection",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "124-ARR_v1_13",
            "content": "We collect the meta-review related data from an online peer reviewing platform for ICLR 2 from 2018 to 2021. Note that the submissions from earlier years are not collected because their meta-reviews are not released. To prepare our dataset for controllable text generation, for each submission, we collect multiple reviews with reviewer ratings and confidence scores, the final meta-review decision, and the meta-review passage. any further annotation, the dataset can already naturally serve the purpose of multi-document summarization (MDS). Compared with those conventional datasets for MDS, such as TAC (Owczarzak and Dang, 2011) and DUC (Over and Yen, 2004), which contain in total a few hundred input articles (equivalent to reviews in MReD), our dataset is more than 10 times larger.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_14",
            "content": "Data Annotation",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "124-ARR_v1_15",
            "content": "As aforementioned, the structure-controllable text generation aims at controlling the structure of the generated passage. Therefore, we need to comprehensively understand the structures of metareviews so as to enable a model to learn how to generate outputs complying with certain structures. Specifically, based on the nature of meta-reviews, we pre-define 9 intent categories: abstract, strength, weakness, suggestion, rebuttal process, rating summary, area chair (AC) disagreement, decision, and miscellaneous (misc). Table 3 shows the definition for each category (see example sentences in Appendix A.1). The identification of category for some sentences is fairly straightforward, while some sentences are relatively ambiguous. Therefore, besides following the definition of each category, the annotators are also required to follow the additional rules as elaborated in Appendix A.2",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_16",
            "content": "For conducting the annotation work, 14 professional data annotators from a data company are initially trained, and 12 of them are selected for the task according to their annotation quality during a trial round. These 12 annotators are fully paid for their work. Each meta-review sentence is independently labeled by 2 different annotators, and a third annotator resolves any disagreement between the first two annotators. We label 45,929 sentences from 7,089 meta-reviews in total, and the Cohen's kappa is 0.778 between the two annotators, showing that the annotation is of quite high quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_17",
            "content": "Data Analysis",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "124-ARR_v1_18",
            "content": "To better understand the MReD dataset, we conduct the following analysis along different dimensions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_19",
            "content": "Sentence distribution across categories. The sentence numbers in different categories are shown in Figure 1, breakdown by the decision (i.e., accept or reject). Among 7,089 submissions, there are 2,368 accepted and 4,721 rejected. Among all submissions and the rejected submissions, \"weakness\" accounts for the largest proportion, while across the accepted ones, \"abstract\" and \"strength\" take up a great proportion. To some extent, these three categories which dominate in meta-reviews could be easily summarized from the reviewers' comments. However, some minor or subjective categories (e.g., \"ac disagreement\") are hard to generate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_20",
            "content": "Breakdown analysis by meta-review lengths and average rating scores. We present the percentage of meta-reviews of different lengths in each score range, as shown in Figure 2. For example, among the meta-reviews that receive the reviewers' average score below 2 (i.e., the first column in the figure), 28% are less than or equal to 50 words, and 38% fall in the length range of 51 to 100 words. We can observe that the meta-reviews tend to be longer for those submissions receiving scores in the middle range, while shorter for those with lower scores or higher scores. This coincides with our commonsense that for high-score and low-score sub- missions, the decision tends to be a clear accept or reject so that meta-reviews can be relatively shorter, while for those borderline submissions, area chairs have to carefully weigh the pros and cons to make the final decision (see Appendix B.1 for borderline submission analysis). As shown in Figure 3, the meta-reviews with more than 150 words generally have a larger proportion of sentences describing \"weakness\" and \"suggestion\" for authors to improve the submissions. Additional analysis on the category breakdown for accepted and rejected papers across the score ranges is shown in Appendix B.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_21",
            "content": "Meta-review patterns. To study the common structures of meta-reviews, we present the transition matrix of different category segments in Figure 4, where the sum of each row is 1. Note that each segment represents the longest consecutive sentences with the same category. We add \"<start>\" and \"<end>\" tokens before and after each metareview accordingly to investigate which categories tend to be at the start/end of the meta-reviews. It is clear to see that \"abstract\" usually positions at the beginning of the meta-review, while \"suggestion\" and \"decision\" usually appear at the end. There are also some clear patterns appearing in the metareviews, such as \"abstract | strength | weakness\", \"rating summary | weakness | rebuttal process\", and \"abstract | weakness | decision\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_22",
            "content": "3 Structure-Controllable Text Generation",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_23",
            "content": "Task Definition",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "124-ARR_v1_24",
            "content": "As aforementioned, in uncontrolled generation, users cannot instruct the model to emphasize on desired aspects. However, in a domain such as meta-reviews, given the same review inputs, one AC may emphasize more on the \"strength\" of the paper following a structure of \"abstract | strength | decision\", whereas another AC may prefer a different structure with more focus on reviewers' opinions and suggestions (i.e., \"rating summary\" and . 00 .00 .38 .32 .11 .00 .09 .01 .03 .05 .01 .00 .01 .00 .47 .06 .01 .14 .06 .11 .04 .10 .00 .00 .04 .00 .05 .01 .25 .17 .22 .05 .21 .00 .01 .10 .21 .00 .01 .13 .07 .21 .05 .22 .00 .01 .07 .10 .06 .00 .09 .10 .27 .11 .20 .00 .01 .06 .09 .09 .01 .00 .11 .36 .06 .21 .00 .00 .02 .03 .02 .00 .05 .00 .15 .05 .68 .00 .01 .02 .04 .01 .00 .01 .09 .00 .04 .79 .00 .03 .14 .20 .07 .01 .09 .06 .14 .00 .26 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 \"suggestion\"). To achieve such flexibility, the task of structure-controllable text generation is defined as: given the text input (i.e., reviews) and a control sequence of the output structure, a model should generate a meta-review which is derivable from the reviews and presents the required structure.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_25",
            "content": "Explored Methods",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "124-ARR_v1_26",
            "content": "As the recent generation works (Vaswani et al., 2017;Liu and Lapata, 2019;Xing et al., 2020) basically adopt an encoder-decoder based architecture and achieve state-of-the-art performance on many tasks and datasets, we primarily investigate the performance of such a framework on our task. Thus in this subsection, we mainly present how to re-organize the input reviews and the control structure as an input sequence of the encoder. We also explore other baselines in the experiments later.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_27",
            "content": "In order to summarize multiple reviews into a meta-review showing a required structure, we explicitly specify the control label sequence that a model should comply with during generation. Specifically, we intuitively add the control sequence in front of the input text. By directly combining both the control and textual information as a single input, our control method is independent of any specially designed encoder and decoder structures. Moreover, by placing the short control sequence in front, an encoder can immediately observe the control signal at the very beginning, thus avoids the possible interference by the subsequent sequence. Moreover, the control sequence in front will never be truncated when the encoder truncates the input to a certain length limit.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_28",
            "content": "Given the multiple review inputs, we need to linearize them into a single input. One simple method to combine multiple inputs for encoder-decoder models is to concatenate all inputs one after another (Fabbri et al., 2019). Beside the text inputs, the review rating is also crucial information for writing meta reviews, which cannot be found in the review passages but exists in the field of rating score. Therefore, we create a rating sentence that consists of the extracted ratings given by the corresponding reviewers and prepend it to our concatenated review texts to obtain the final input. We name this method rate-concat (see Table 4, upper).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_29",
            "content": "We also show explorations with other review combination methods in Appendix C.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_30",
            "content": "As aforementioned, we place the control sequence in front of the re-organized review information. Specifically, we explore two different control methods, namely, sent-ctrl and seg-ctrl. Sent-ctrl uses one control label per target sentence and controls generation on a sentence-level. Note that this method can allow implicit control on the length (i.e., number of sentences) of the generation. Segctrl treats consecutive sentences of the same label as one segment and only uses one label for a single segment. Example inputs of different control settings are shown in Table 4 (lower). For instance, sent-ctrl repeats \"abstract\" in its control sequence whereas seg-ctrl does not. This is because seg-ctrl treats the 1 st and 2 nd target sentences of \"abstract\" as the same segment and only uses a single label to indicate it in the sequence. Additionally, we provide a vanilla setting for uncontrolled generation, unctrl, where no control sequence is used.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_31",
            "content": "Using the above input sequence as the source and the corresponding meta-review as the target, we can train an encoder-decoder model for controllable generation. Many transformer-based models have achieved state-of-the-art performance. Common abstractive summarization models include BART (Lewis et al., 2020), T5 (Raffel et al., 2020) and PEGASUS (Zhang et al., 2020). In this paper we focus on the bart-large-cnn model, one variant of the BART model (results on other pretrained models can be found in Appendix D.1). More specifically, we use the pytorch implementation in the open-source library Hugging Face Transformers (Wolf et al., 2020). Hence, all our future usage of the word \"Transformers\" refers to bart-large-cnn in the Transformers library .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_32",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "124-ARR_v1_33",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "124-ARR_v1_34",
            "content": "Extractive Baselines. We employ three common extractive summarization baselines each of which basically provides a mechanism to rank the input sentences. LexRank (Erkan and Radev, 2004) represents sentences in a graph and uses eigenvector centrality to calculate sentence importance scores. TextRank (Mihalcea and Tarau, 2004) is another graph-based sentence ranking method that obtains vertex scores by running a \"random-surfer model\" until convergence. MMR (Carbonell and Goldstein, 1998) calculates sentence scores by balancing the redundancy score with the information relevance score. After ranking with the above models, we select sentences as output with different strategies according to the controlled and uncontrolled settings. For the uncontrolled setting, we simply select the top k sentences as the generated output, where k is a hyperparameter deciding the size of the generated output. For the controlled setting, we select only the top sentences with the right category labels according to the control sequence. To do so, we employ an LSTM-CRF (Lample et al., 2016) tagger trained on the labeled meta-reviews to predict the sentence labels of each input review. Refer to Appendix D.2 for more details of the tagger.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_35",
            "content": "Generic Sentence Baselines. Considering the nature of meta-reviews, we could imagine some categories may have common phrases inflating the Rouge scores, such as \"This paper proposes ...\" for abstract, and \"I recommend acceptance.\" for decision, etc. To examine such impact, we select sentences that are generic in each category and combine these sentences to generate outputs according to the control sequences. For instance, if the control sequence is \"abstract | strength | decision\", we take the most generic sentences from the categories of \"abstract\", \"strength\" and \"decision\" respectively to form the output. Specifically, we create two generic sentence baselines by obtaining generic sentences from the training data from either the meta-review references (i.e., target) or the input reviews (i.e., source), namely \"Target Generic\" and \"Source Generic\". Moreover, we also study such impact on the high-score and low-score submissions respectively, since an AC may write more succinct meta-reviews for clear-cut papers, as suggested by Figure 2. See Appendix D.3 for more details and results on generic sentence baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_36",
            "content": "Experimental Setting",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "124-ARR_v1_37",
            "content": "To conduct text generation experiments, we preprocess our MReD dataset by filtering to ensure the selected meta-reviews have 20 to 400 words, as certain meta-review passages are extremely short or long. After preprocessing, we obtain 6,693 sourcetarget pairs, for which we randomly split into train, validation, and test sets by a ratio of 8:1:1. We evaluate our generated outputs against the reference meta-reviews using the F 1 scores of ROUGE 1 , ROUGE 2 , and ROUGE L (Lin, 2004) 3 . For the extractive and generic baselines, a key hyperparameter is the sentence number k, which we set to the number of labels in the sent-ctrl control sequence. More setting details are shown in Appendix D.4",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_38",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "124-ARR_v1_39",
            "content": "We show results in Table 5. Only the best settings of rate-concat (Table 12 in Appendix C.1) and input truncation of 2048 tokens (Appendix D.5) for the Transformers are included. Amongst the extractive baselines, TextRank performs the best in both unctrl and sent-ctrl settings. Nevertheless, all controlled methods outperform their unctrl settings (same for the Transformers). This validates our intuition that structure-controlled generation is more suitable for user-subjective writings such as meta-reviews, because the model can better satisfy different structure requirements when supplied with the corresponding control sequences. On the other hand, for the Transformers, sent-ctrl is the best, followed by seg-ctrl. This is most likely due to the former's more fine-grained sentence-level control that provides a clearer structure outline, as compared to the coarser segment-level control. Moreover, the Transformers far outperform the extractive baselines, showing that the extractionbased methods are insufficient for MReD. This also suggests that meta-review writings are different from the input reviews, therefore copying full review sentences to form meta-reviews doesn't work well. This is again validated by the \"Target Generic\" baseline's significant improvement over the \"Source Generic\" baseline, which shows that generic sentences from meta-reviews can suit generation much better than those in reviews. Nevertheless, all Transformers results are still much better than the \"Target Generic\" sentence baseline, showing that despite generic phrases in some categories contributing to Rouge, the Transformers model is capable of capturing content-specific information for each input.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_40",
            "content": "Case Study",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "124-ARR_v1_41",
            "content": "We study some cases for a better understanding of the structure-controllable generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_42",
            "content": "Identify the control label for each sentence. We first evaluate whether the model is able to attend to the correct control label during generation. For each generation step, we obtain the cross attention weights from the decoder's output token towards the control labels and plot them in",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_43",
            "content": "Sent 1 (abstract):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_44",
            "content": "This paper proposes a selfsupervised contrastive learning method for few-shot learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_45",
            "content": "Sent 2 (weakness):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_46",
            "content": "The reviewers agree that the idea is interesting, but have concerns about the clarity of the paper and the lack of comparison to the baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_47",
            "content": "The paper is not suitable for publication at ICLR in its current form.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_48",
            "content": "Table 7: Attention analysis for each output sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_49",
            "content": "decision\". When generating each sentence, we can see that the attention weights of the corresponding control token are the highest, which demonstrates that our model can effectively pay attention to the correct control label and thus generate the content complying with the intent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_50",
            "content": "To understand what information the model attends to when generating each sentence, we aggregate the cross attention weights to obtain the attention scores from each generated sentence towards all input sentences (Appendix D.6). Then, we select the top 3 input sentences with the highest attention scores for each generated sentence, and visualize the normalized attention weights on all tokens in the selected sentences and the control sequence in Table 7. As shown, the model can correctly extract relevant information from the source sentences. For example, it identifies important phrases such as \"interesting\", \"clarity\" and \"lack of comparison to baselines\" when generating \"Sent 2\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_51",
            "content": "Generate varied outputs given different control sequences. To further investigate the effectiveness of the control sequence, we change the control sequence of the above example and re-generate the meta-reviews given the same input reviews. In Table 6, we first show the gold meta-review and the model output using the original control sequence in Row 0 and Row 1, and then show the model outputs with alternative control sequences in Row 2 and Row 3. From the outputs, we can see that indeed each generated sentence corresponds to its control label well. In Row 2, we add an additional control label in the sequence and by repeating the \"abstract\" label, the generator can further elaborate more details of the studied method. This is one key advantage of our sent-ctrl compared to the seg-ctrl, which allows the control of length and the level of the generation details. In Row 3, a very comprehensive control sequence is specified. We can see that the output meta-review is quite fluent and polite to reject the borderline paper. See Appendix D.7 for more examples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_52",
            "content": "Human Evaluation",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "124-ARR_v1_53",
            "content": "In addition to the Rouge evaluation, we ask 3 human judges to manually assess the generation quality of the Transformers models from We grade fluency and content relevance on a scale of 1 to 5, whereas structure similarity and decision correctness are calculated from 0 to 1 (Appendix D.8). For structure similarity, because sent-ctrl and seg-ctrl have different control sequences, we evaluate the two models on sentence-level (sent) and segment-level (seg) structures respectively, and provide both evaluations for unctrl.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_54",
            "content": "As shown in Table 8, both sent-ctrl and seg-ctrl models show significant improvements on the generation structure over the uncontrolled baseline, which affirms the effectiveness of our proposed methods for structure-controllable generation. Sentctrl also has better fluency and decision correctness, suggesting that having a better output structure can benefit the readability and decision generation. For the content relevance, the scores of all methods are reasonably good, and significance tests cannot prove any best model (p > 0.08). Nevertheless, it is possible that the looser control a method applies, the better relevance score it achieves. It is because a tighter control narrows the content that a model can use from the reviews.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_55",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "124-ARR_v1_56",
            "content": "To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018). Datasets for long documents include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016). In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely structurecontrollable text generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_57",
            "content": "Researchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs. Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation. There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_58",
            "content": "A wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality , domains (Takeno et al., 2017) and persona ) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)). Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output. Our task also differs from content planning (Reiter and Dale, 1997;Shao et al., 2019;, which involves explicitly selecting and arranging the input content. Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_59",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "124-ARR_v1_60",
            "content": "This paper introduces a fully-annotated text generation dataset MReD in a new domain, i.e., the meta-reviews in the peer review system, and provides thorough data analysis to better understand the data characteristics. With such rich annotations, we propose simple yet effective methods for structure-controllable text generation. Extensive experimental results are presented as baselines for future study and thorough result analysis is conducted to shed light on the control mechanisms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_61",
            "content": "\"The paper presents/explores/describes/addresses/proposes ...\"",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_62",
            "content": "strength \"The reviewers found the paper interesting.\" \"The method and justification are clear.\" \"The quantitative results are promising.\"",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_63",
            "content": "weakness \"The paper is somewhat incremental ...\" \"... claims are confusing\" \"The main concern is ...\" \"... unfair experimental comparisons ...\" rating summary \"R1 recommends Accept.\" \"All four reviewers ultimately recommended acceptance.\" \"Reviews were somewhat mixed, but also with mixed confidence scores.\"",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_64",
            "content": "ac disagreement \"The area chair considers the remaining concerns by Reviewer 3 as invalid.\" \"I do not agree with the criticism about ...\" \"I disagree with the second point ...\"",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_65",
            "content": "rebuttal process \"The authors have made various improvements to the paper\" \"... remained after the author rebuttal ...\" \"Authors provided convincing feedbacks on this key point.\"",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_66",
            "content": "suggestion \"... more analysis ...\" \"The authors are advised to take into account the issues about ...\"",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_67",
            "content": "decision \"The paper is recommended as a poster presentation.\" \"AC recommends Reject.\" \"I recommend rejection.\"",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_68",
            "content": "miscellaneous \"Thank you for submitting you paper to ICLR.\" \"I've summarized the pros and cons of the reviews below.\"",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_69",
            "content": "Table 9: Category examples of meta-review sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_70",
            "content": "A Data Annotation",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_71",
            "content": "We show category examples in Table 9.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_72",
            "content": "The additional rules for annotation are as follows: First, instead of only labeling the individual sentences per se, the annotators are given a complete paragraph of meta-review to label the sentences with context information. For example, if the area chair writes a sentence providing some extra background knowledge in the discussion of the weakness of the submission, that sentence itself can be considered as \"misc\". However, it should be labeled as \"weakness\" to be consistent in context. Second, not every sentence can be strictly classified into a single category. When a sentence contains information from multiple categories, the annotators should consider its main point and primary purpose. One example is: \"Although the paper discusses an interesting topic and contains potentially interesting idea, its novelty is limited.\" Although the first half of the sentence discusses the strength of the submission, the primary purpose of this sentence is to point out its weakness, and therefore it should be labeled as weakness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_73",
            "content": "Furthermore, there are still some cases where the main point of the sentence is hard to differentiate from multiple categories. We then define a priority order of these 9 categories according to the importance of each category for annotators to follow: decision > rating summary > strength ? = weakness > ac disagreement > rebuttal process > abstract > suggestion > miscellaneous. We use the sign \" ? =\" because there are some rare cases where a sentence contains both \"strength\" and \"weakness\" while there is no obvious emphasis on either, and it is hard to tell whether \"strength\" should have a priority over \"weakness\" or the other way round. We then label this sentence based on the final decision: if this submission is accepted, we label the sentence as \"strength\", and vice versa.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_74",
            "content": "We further analyze the category distribution in borderline papers. As shown in Table 10, for submissions within the score range of [4.5,6), there are 713 accepted submissions and 2,588 rejected submissions. One clear difference is the percentage of \"strength\" and \"weakness\". Another difference is the percentage of \"ac disagreement\", where the accepted papers have four times the value than rejected ones. This suggests that for the accepted borderline papers, the area chair tends to share different opinions with reviewers, and thus deciding to accept the borderline submissions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_75",
            "content": "We further analyze the occurrence of each category for accepted papers and rejected papers separately across different score ranges, as shown in Table 11. For accepted papers, as the score increases, the percentage of meta-reviews having \"weakness\" and \"suggestion\" drops because the high-score submissions are more likely to be accepted. Even the percentage of \"decision\" drops following the same trend. In addition, the proportion of meta-reviews having \"rebuttal process\" is larger for submissions with lower scores. This suggests that the rebuttal process plays an important role in the peer review process, especially in helping the borderline papers to be accepted. On the other hand, for rejected papers, the percentage of meta-reviews having \"strength\" increases as the average score increases. This coincides with our common sense that the submissions receiving higher scores tend to have more strengths. One interesting finding here is that the percentage of \"weakness\" and \"suggestion\" also increases as the average rating score increases. This may be due to two main reasons. First, to reject a submission with higher scores, the area chair has to explain the weakness with more details and provide more suggestions for authors to further improve their submissions. Second, compared to the percentage of \"strength\", \"weakness\" definitely has a larger percentage within any range of rating scores. The difference in the percentage of \"strength\" and \"weakness\" is intuitively different between the accepted papers and the rejected papers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_76",
            "content": "We explore alternative methods to linearize the multiple reviews of the same submission, namely, concat and merge. For the concat, we simply concatenate all reviews one after another according to their reviewers' sequence. For merge, we can obtain the merged content as follows: From all review inputs, we use the longest one as a backbone. We segment all reviews' content on a paragraph level, and encode them using SentenceTransformers (Reimers and Gurevych, 2019). Then, for each paragraph embedding in the non-backbone reviews, we calculate a cosine similarity score with each backbone paragraph embedding, and insert it after the backbone paragraph with which it has the highest similarity score. We repeat the process for all paragraphs in non-backbone reviews to obtain a single passage. Additionally, we provide a baseline setting longestreview, which does not combine reviews but only uses the longest review as the input. Moreover, we add rating sentences in front of the results of concat and merge to obtain rate-concat and rate-merge, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_77",
            "content": "As shown in Table 12, the longest-review setting has the worst performance, thus validating that the review combination methods are necessary in order not to omit important information. rate-concat setting has the best overall performance, which is the setting used throughout the main paper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_78",
            "content": "We provide baselines of uncontrolled generation and controlled generation on MReD using other common Transformer pretrained models in Table 13.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_79",
            "content": "To obtain labels on source input, we train a tagger based on the human-annotated meta-reviews, then use it to predict labels on the input sentences. Specifically, we define the task as a sequence labeling problem and apply the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) We report the F 1 scores for each category as well as the overall micro F 1 and macro F 1 scores in Table 14. Micro F1 is the overall accuracy regardless of the categories, whereas macro F1 is an average of per category accuracy evaluation. Since some of the category labels (eg. \"ac disagreement\") are very rare, their classification accuracy is low. Overall, micro F1 is a more important metric since it suggests general performance. The results stand proof that the majority of the categories have their own characteristics that can be identified from other categories. RoBERTabase is the best performing model, therefore we use this model for review sentence label prediction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_80",
            "content": "Besides the baselines of \"Source Generic\" and \"Target Generic\", we explore subsets of papers with high scores (average reviewers' rating 7) or low scores (average reviewers' rating 3) to obtain 4 additional generic baselines: \"Source High Score\", \"Source Low Score\", \"Target High Score\", \"Target Low Score\". We use \"Target Generic\" as an example to explain how we obtain the generic sentences: We first group all meta-review sentences from the training set according to their label categories, and then re-arrange the sentences in each category using TextRank (our best performing extractive model). Since TextRank ranks the input sentences based on each sentence's content connection with others, sentences with higher rankings are also more general in the sense that they have more shared content with others. Similarly, different sets of generic sentences can be obtained for the other 5 baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_81",
            "content": "After obtaining the generic sentence sets, we can create baseline generations using the sent-ctrl sequence. We avoid using the same sentence twice inside the same generation, so if the same label appears multiple times in a control sequence, we will use the same number of generic sentences for that category down the ranking order.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_82",
            "content": "We show results in Table 15. The low score baselines perform the best amongst both source and target baselines, suggesting that the sentences from low score submissions are more typical for both reviews and meta-reviews.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_83",
            "content": "For preprocessing, besides filtering based on metareview length, we also remove submissions with only one or two reviews, since the majority of the submissions have more than 3 reviews.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_84",
            "content": "For the extractive baselines, recall that under the sent-ctrl setting, the control sequence length is the same as the sentence number of the target metareview. Therefore, to conduct a fair comparison, we set the hyperparameter k equal to the number of Table 17: Source length statistics on all data splits. Max for maximum source length, med for median source length, and avg for average source length. labels in the control sequence for both controlled and uncontrolled extractive baselines, and sent-ctrl is used for all controlled extractive baselines. We also adopt the same k for the generic baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_85",
            "content": "For the Transformers, we first load the pretrained model and then fine-tune it on MReD. All experiments are conducted on single V100 GPUs, using a batch size of 1 in order to fit the large pretrained model on a single GPU. During finetuning, we set the Transformers' hyperparameters of \"minimum_target_length\" to 20, and \"maxi-mum_target_length\" to 400, according to our filter range on the meta-review lengths. For the rest of the hyperparameters, we use the pretrained model's default values. Due to long inputs (see",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_86",
            "content": "By default, the Transformers truncate the source to 1024 tokens. We further investigate the performance of different source truncation lengths using rate-concat. As shown in Table 18, truncating the source to 2048 tokens consistently achieves the best performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_87",
            "content": "During generation, we can obtain the attention weights of each output token towards all input tokens. Specifically, we average all decoder layers' cross attention weights for the same output token generated at each decoding step. We then calculate an attention value for that output token on each input sentence, by aggregating the token's attention weights on the list of input tokens that belong to the same sentence by max pooling. Finally, we can calculate an output-sentence-to-input-sentence attention score, by adding up these attention values for the output tokens that belong to the same sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_88",
            "content": "Common attention aggregation methods include summation, average-pooling, and max-pooling. We use max-pooling to aggregate attention for same-sentence input tokens, because summation gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out. With max-pooling, we can correctly identify sentences with spiked attention at important phrases, regardless of sentence lengths. For attention aggregation on the same-sentence output tokens, summation is used and can be viewed as allowing each output token to vote an attention score on all input sentences, so that the input sentence receiving the highest total score is the most relevant. We conduct trial runs of all aggregation methods on input tokens with summation for output-token aggregation for multiple generation examples, and indeed max-pooling outperforms the other two by identifying more relevant input sentences with the generated sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_89",
            "content": "Once we have the attention scores, we can attribute the generation of each output sentence to a few topmost relevant input sentences. Then, we can draw a color map of the input tokens in the selected sentences based on their relative attention weights.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_90",
            "content": "We show examples of the generation results using alternative control sequences on another submission in Table 16. We can see the effectiveness of controlling the output structure using our proposed method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_91",
            "content": "For structure similarity, we instruct the judges to label each generated sentence with the closest category. We then calculate the normalized token-level edit distance between the judge-annotated label sequence and the given control sequence, then deduct this value from 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_92",
            "content": "For decision correctness, we evaluate it on a binary scale where 1 indicates complete correctness and 0 otherwise. More specifically, we give 0 if the generation produces contradictory decisions and a wrong decision, or the generation does not show enough hints for rejection or acceptance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "124-ARR_v1_93",
            "content": "Chaitanya Bhatia, Tribikram Pradhan, Metagen: An academic meta-review generation system, 2020, Proceedings of ACM-SIGIR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Chaitanya Bhatia",
                    "Tribikram Pradhan"
                ],
                "title": "Metagen: An academic meta-review generation system",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACM-SIGIR",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_94",
            "content": "Jaime Carbonell, Jade Goldstein, The use of mmr, diversity-based reranking for reordering documents and producing summaries, 1998, Proceedings of ACM-SIGIR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Jaime Carbonell",
                    "Jade Goldstein"
                ],
                "title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
                "pub_date": "1998",
                "pub_title": "Proceedings of ACM-SIGIR",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_95",
            "content": "Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, Luo Si, Argument pair extraction from peer review and rebuttal via multi-task learning, 2020, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Liying Cheng",
                    "Lidong Bing",
                    "Qian Yu",
                    "Wei Lu",
                    "Luo Si"
                ],
                "title": "Argument pair extraction from peer review and rebuttal via multi-task learning",
                "pub_date": "2020",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_96",
            "content": "Arman Cohan, Franck Dernoncourt, Soon Doo, Trung Kim, Seokhwan Bui, Walter Kim, Nazli Chang,  Goharian, A discourse-aware attention model for abstractive summarization of long documents, 2018, Proceedings of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Arman Cohan",
                    "Franck Dernoncourt",
                    "Soon Doo",
                    "Trung Kim",
                    "Seokhwan Bui",
                    "Walter Kim",
                    "Nazli Chang",
                    " Goharian"
                ],
                "title": "A discourse-aware attention model for abstractive summarization of long documents",
                "pub_date": "2018",
                "pub_title": "Proceedings of NAACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_97",
            "content": "Yuguang Duan, Canwen Xu, Jiaxin Pei, Jialong Han, Chenliang Li, Pre-train and plug-in: Flexible conditional text generation with variational autoencoders, 2020, Proceedings of the ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Yuguang Duan",
                    "Canwen Xu",
                    "Jiaxin Pei",
                    "Jialong Han",
                    "Chenliang Li"
                ],
                "title": "Pre-train and plug-in: Flexible conditional text generation with variational autoencoders",
                "pub_date": "2020",
                "pub_title": "Proceedings of the ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_98",
            "content": "G\u00fcnes Erkan,  Dragomir R Radev, Lexrank: Graph-based lexical centrality as salience in text summarization, 2004, Artificial Intelligence Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "G\u00fcnes Erkan",
                    " Dragomir R Radev"
                ],
                "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
                "pub_date": "2004",
                "pub_title": "Artificial Intelligence Research",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_99",
            "content": "Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir Radev, Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model, 2019, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Alexander Richard Fabbri",
                    "Irene Li",
                    "Tianwei She",
                    "Suyi Li",
                    "Dragomir Radev"
                ],
                "title": "Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_100",
            "content": "Angela Fan, David Grangier, Michael Auli, Controllable abstractive summarization, 2018, Proceedings of WNGT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Angela Fan",
                    "David Grangier",
                    "Michael Auli"
                ],
                "title": "Controllable abstractive summarization",
                "pub_date": "2018",
                "pub_title": "Proceedings of WNGT",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_101",
            "content": "Angela Fan, Mike Lewis, Yann Dauphin, Hierarchical neural story generation, 2018, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Angela Fan",
                    "Mike Lewis",
                    "Yann Dauphin"
                ],
                "title": "Hierarchical neural story generation",
                "pub_date": "2018",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_102",
            "content": "Beatriz Fisas, Francesco Ronzano, A multi-layered annotated corpus of scientific papers, 2016, Proceedings of LREC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Beatriz Fisas",
                    "Francesco Ronzano"
                ],
                "title": "A multi-layered annotated corpus of scientific papers",
                "pub_date": "2016",
                "pub_title": "Proceedings of LREC",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_103",
            "content": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, The webnlg challenge: Generating text from rdf data, 2017, Proceedings of INLG, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Claire Gardent",
                    "Anastasia Shimorina",
                    "Shashi Narayan",
                    "Laura Perez-Beltrachini"
                ],
                "title": "The webnlg challenge: Generating text from rdf data",
                "pub_date": "2017",
                "pub_title": "Proceedings of INLG",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_104",
            "content": "Max Grusky, Mor Naaman, Yoav Artzi, Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies, 2018, Proceedings of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Max Grusky",
                    "Mor Naaman",
                    "Yoav Artzi"
                ],
                "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
                "pub_date": "2018",
                "pub_title": "Proceedings of NAACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_105",
            "content": "Karl Moritz Hermann, Tom\u00e1s Kocisk\u1ef3, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend, 2015, Proceedings of NIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Karl Moritz Hermann",
                    "Tom\u00e1s Kocisk\u1ef3",
                    "Edward Grefenstette",
                    "Lasse Espeholt",
                    "Will Kay",
                    "Mustafa Suleyman",
                    "Phil Blunsom"
                ],
                "title": "Teaching machines to read and comprehend",
                "pub_date": "2015",
                "pub_title": "Proceedings of NIPS",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_106",
            "content": "Sepp Hochreiter, J\u00fcrgen Schmidhuber, Long short-term memory, 1997, Neural Computation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Sepp Hochreiter",
                    "J\u00fcrgen Schmidhuber"
                ],
                "title": "Long short-term memory",
                "pub_date": "1997",
                "pub_title": "Neural Computation",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_107",
            "content": "Xinyu Hua, Mitko Nikolov, Nikhil Badugu, Lu Wang, Argument mining for understanding peer reviews, 2019, Proceedings of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Xinyu Hua",
                    "Mitko Nikolov",
                    "Nikhil Badugu",
                    "Lu Wang"
                ],
                "title": "Argument mining for understanding peer reviews",
                "pub_date": "2019",
                "pub_title": "Proceedings of NAACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_108",
            "content": "Xinyu Hua, Lu Wang, Sentence-level content planning and style specification for neural text generation, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Xinyu Hua",
                    "Lu Wang"
                ],
                "title": "Sentence-level content planning and style specification for neural text generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_109",
            "content": "Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard Hovy, Roy Schwartz, A dataset of peer reviews (peerread): Collection, insights and nlp applications, 2018, Proceedings of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Dongyeop Kang",
                    "Waleed Ammar",
                    "Bhavana Dalvi",
                    "Madeleine Van Zuylen",
                    "Sebastian Kohlmeier",
                    "Eduard Hovy",
                    "Roy Schwartz"
                ],
                "title": "A dataset of peer reviews (peerread): Collection, insights and nlp applications",
                "pub_date": "2018",
                "pub_title": "Proceedings of NAACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_110",
            "content": ", Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of NAACL-HLT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of NAACL-HLT",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_111",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Adam: A method for stochastic optimization",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_112",
            "content": "Wojciech Kry\u015bci\u0144ski, Romain Paulus, Caiming Xiong, Richard Socher, Improving abstraction in text summarization, 2018, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Wojciech Kry\u015bci\u0144ski",
                    "Romain Paulus",
                    "Caiming Xiong",
                    "Richard Socher"
                ],
                "title": "Improving abstraction in text summarization",
                "pub_date": "2018",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_113",
            "content": "John Lafferty, Andrew Mccallum, Fernando Cn Pereira, Conditional random fields: Probabilistic models for segmenting and labeling sequence data, 2001, Proceedings of ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "John Lafferty",
                    "Andrew Mccallum",
                    "Fernando Cn Pereira"
                ],
                "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
                "pub_date": "2001",
                "pub_title": "Proceedings of ICML",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_114",
            "content": "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer, Neural architectures for named entity recognition, 2016, Proceedings of NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Guillaume Lample",
                    "Miguel Ballesteros",
                    "Sandeep Subramanian",
                    "Kazuya Kawakami",
                    "Chris Dyer"
                ],
                "title": "Neural architectures for named entity recognition",
                "pub_date": "2016",
                "pub_title": "Proceedings of NAACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_115",
            "content": "Anne Lauscher, Goran Glava\u0161, Kai Eckert, Arguminsci: A tool for analyzing argumentation and rhetorical aspects in scientific writing, 2018, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Anne Lauscher",
                    "Goran Glava\u0161",
                    "Kai Eckert"
                ],
                "title": "Arguminsci: A tool for analyzing argumentation and rhetorical aspects in scientific writing",
                "pub_date": "2018",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_116",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal ; Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_117",
            "content": "Maria Liakata, Simone Teufel, Advaith Siddharthan, Colin Batchelor, Corpora for the conceptualisation and zoning of scientific papers, 2010, Proceedings of LREC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Maria Liakata",
                    "Simone Teufel",
                    "Advaith Siddharthan",
                    "Colin Batchelor"
                ],
                "title": "Corpora for the conceptualisation and zoning of scientific papers",
                "pub_date": "2010",
                "pub_title": "Proceedings of LREC",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_118",
            "content": "Yi Liao, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, Tong Zhang, QuaSE: Sequence editing under quantifiable guidance, 2018, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Yi Liao",
                    "Lidong Bing",
                    "Piji Li",
                    "Shuming Shi",
                    "Wai Lam",
                    "Tong Zhang"
                ],
                "title": "QuaSE: Sequence editing under quantifiable guidance",
                "pub_date": "2018",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_119",
            "content": "Chin-Yew Lin, Rouge: A package for automatic evaluation of summaries, 2004, Text summarization branches out, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Chin-Yew Lin"
                ],
                "title": "Rouge: A package for automatic evaluation of summaries",
                "pub_date": "2004",
                "pub_title": "Text summarization branches out",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_120",
            "content": "Yang Liu, Mirella Lapata, Hierarchical transformers for multi-document summarization, 2019, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Yang Liu",
                    "Mirella Lapata"
                ],
                "title": "Hierarchical transformers for multi-document summarization",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_121",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_122",
            "content": "Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnab\u00e1s P\u00f3czos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan Black, Shrimai Prabhumoye, Politeness transfer: A tag and generate approach, 2020, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Aman Madaan",
                    "Amrith Setlur",
                    "Tanmay Parekh",
                    "Barnab\u00e1s P\u00f3czos",
                    "Graham Neubig",
                    "Yiming Yang",
                    "Ruslan Salakhutdinov",
                    "Alan Black",
                    "Shrimai Prabhumoye"
                ],
                "title": "Politeness transfer: A tag and generate approach",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_123",
            "content": "Rada Mihalcea, Paul Tarau, Textrank: Bringing order into text, 2004, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Rada Mihalcea",
                    "Paul Tarau"
                ],
                "title": "Textrank: Bringing order into text",
                "pub_date": "2004",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_124",
            "content": "Ramesh Nallapati, Bowen Zhou, \u00c7aglar Cicero Dos Santos, Bing Gul\u00e7ehre,  Xiang, Abstractive text summarization using sequence-to-sequence rnns and beyond, 2016, Proceedings of SIGNLL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Ramesh Nallapati",
                    "Bowen Zhou",
                    "\u00c7aglar Cicero Dos Santos",
                    "Bing Gul\u00e7ehre",
                    " Xiang"
                ],
                "title": "Abstractive text summarization using sequence-to-sequence rnns and beyond",
                "pub_date": "2016",
                "pub_title": "Proceedings of SIGNLL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_125",
            "content": "Courtney Napoles, Benjamin Matthew R Gormley,  Van Durme, Annotated gigaword, 2012, Proceedings of AKBC-WEKEX, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Courtney Napoles",
                    "Benjamin Matthew R Gormley",
                    " Van Durme"
                ],
                "title": "Annotated gigaword",
                "pub_date": "2012",
                "pub_title": "Proceedings of AKBC-WEKEX",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_126",
            "content": "Shashi Narayan, B Shay, Mirella Cohen,  Lapata, Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization, 2018, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Shashi Narayan",
                    "B Shay",
                    "Mirella Cohen",
                    " Lapata"
                ],
                "title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
                "pub_date": "2018",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_127",
            "content": "Paul Over, James Yen, An introduction to duc-2004, 2004, Proceedings of DUC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Paul Over",
                    "James Yen"
                ],
                "title": "An introduction to duc-2004",
                "pub_date": "2004",
                "pub_title": "Proceedings of DUC",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_128",
            "content": "Karolina Owczarzak, Hoa Dang, Overview of the tac 2011 summarization track: Guided task and aesop task, 2011, Proceedings of TAC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Karolina Owczarzak",
                    "Hoa Dang"
                ],
                "title": "Overview of the tac 2011 summarization track: Guided task and aesop task",
                "pub_date": "2011",
                "pub_title": "Proceedings of TAC",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_129",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter J Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_130",
            "content": "La Ramshaw, Text chunking using transformation-based learning, 1995, Proceedings of Third Workshop on Very Large Corpora, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    " La Ramshaw"
                ],
                "title": "Text chunking using transformation-based learning",
                "pub_date": "1995",
                "pub_title": "Proceedings of Third Workshop on Very Large Corpora",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_131",
            "content": "Lev Ratinov, Dan Roth, Design challenges and misconceptions in named entity recognition, 2009, Proceedings of CoNLL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Lev Ratinov",
                    "Dan Roth"
                ],
                "title": "Design challenges and misconceptions in named entity recognition",
                "pub_date": "2009",
                "pub_title": "Proceedings of CoNLL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_132",
            "content": "Nils Reimers, Iryna Gurevych, Sentencebert: Sentence embeddings using siamese bertnetworks, 2019, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Nils Reimers",
                    "Iryna Gurevych"
                ],
                "title": "Sentencebert: Sentence embeddings using siamese bertnetworks",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_133",
            "content": "Ehud Reiter, Robert Dale, Building applied natural language generation systems, 1997, Natural Language Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Ehud Reiter",
                    "Robert Dale"
                ],
                "title": "Building applied natural language generation systems",
                "pub_date": "1997",
                "pub_title": "Natural Language Engineering",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_134",
            "content": "UNKNOWN, None, 2008, The new york times annotated corpus. Linguistic Data Consortium, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2008",
                "pub_title": "The new york times annotated corpus. Linguistic Data Consortium",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_135",
            "content": "Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan Zhao, Shuming Shi, Rui Yan, Semi-supervised text style transfer: Cross projection in latent space, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Mingyue Shang",
                    "Piji Li",
                    "Zhenxin Fu",
                    "Lidong Bing",
                    "Dongyan Zhao",
                    "Shuming Shi",
                    "Rui Yan"
                ],
                "title": "Semi-supervised text style transfer: Cross projection in latent space",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_136",
            "content": "Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu, Long and diverse text generation with planning-based hierarchical variational model, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Zhihong Shao",
                    "Minlie Huang",
                    "Jiangtao Wen",
                    "Wenfei Xu",
                    "Xiaoyan Zhu"
                ],
                "title": "Long and diverse text generation with planning-based hierarchical variational model",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_137",
            "content": "Eva Sharma, Chen Li, Lu Wang, Bigpatent: A large-scale dataset for abstractive and coherent summarization, 2019, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Eva Sharma",
                    "Chen Li",
                    "Lu Wang"
                ],
                "title": "Bigpatent: A large-scale dataset for abstractive and coherent summarization",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_138",
            "content": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola, Style transfer from non-parallel text by cross-alignment, 2017, Proceedings of NIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Tianxiao Shen",
                    "Tao Lei",
                    "Regina Barzilay",
                    "Tommi Jaakkola"
                ],
                "title": "Style transfer from non-parallel text by cross-alignment",
                "pub_date": "2017",
                "pub_title": "Proceedings of NIPS",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_139",
            "content": "Shunsuke Takeno, Masaaki Nagata, Kazuhide Yamamoto, Controlling target features in neural machine translation via prefix constraints, 2017, Proceedings of WAT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Shunsuke Takeno",
                    "Masaaki Nagata",
                    "Kazuhide Yamamoto"
                ],
                "title": "Controlling target features in neural machine translation via prefix constraints",
                "pub_date": "2017",
                "pub_title": "Proceedings of WAT",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_140",
            "content": "Jiwei Tan, Xiaojun Wan, Jianguo Xiao, Abstractive document summarization with a graphbased attentional neural model, 2017, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Jiwei Tan",
                    "Xiaojun Wan",
                    "Jianguo Xiao"
                ],
                "title": "Abstractive document summarization with a graphbased attentional neural model",
                "pub_date": "2017",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_141",
            "content": "Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric Xing, Zhiting Hu, Targetguided open-domain conversation, 2019, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Jianheng Tang",
                    "Tiancheng Zhao",
                    "Chenyan Xiong",
                    "Xiaodan Liang",
                    "Eric Xing",
                    "Zhiting Hu"
                ],
                "title": "Targetguided open-domain conversation",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_142",
            "content": "Simone Teufel, Jean Carletta, Marc Moens, An annotation scheme for discourse-level argumentation in research articles, 1999, Proceedings of EACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Simone Teufel",
                    "Jean Carletta",
                    "Marc Moens"
                ],
                "title": "An annotation scheme for discourse-level argumentation in research articles",
                "pub_date": "1999",
                "pub_title": "Proceedings of EACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_143",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Proceedings of NIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Proceedings of NIPS",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_144",
            "content": "Yunli Wang, Yu Wu, Lili Mou, Zhoujun Li, Wenhan Chao, Harnessing pre-trained neural networks with rules for formality style transfer, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Yunli Wang",
                    "Yu Wu",
                    "Lili Mou",
                    "Zhoujun Li",
                    "Wenhan Chao"
                ],
                "title": "Harnessing pre-trained neural networks with rules for formality style transfer",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP-IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_145",
            "content": "Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, Transformers: State-of-theart natural language processing, 2020, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Thomas Wolf",
                    "Julien Chaumond",
                    "Lysandre Debut",
                    "Victor Sanh",
                    "Clement Delangue",
                    "Anthony Moi",
                    "Pierric Cistac",
                    "Morgan Funtowicz",
                    "Joe Davison",
                    "Sam Shleifer"
                ],
                "title": "Transformers: State-of-theart natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_146",
            "content": "Xinyu Xing, Xiaosheng Fan, Xiaojun Wan, Automatic generation of citation texts in scholarly papers: A pilot study, 2020, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Xinyu Xing",
                    "Xiaosheng Fan",
                    "Xiaojun Wan"
                ],
                "title": "Automatic generation of citation texts in scholarly papers: A pilot study",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_147",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, Pegasus: Pre-training with extracted gap-sentences for abstractive summarization, 2020, Proceedings of ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Jingqing Zhang",
                    "Yao Zhao",
                    "Mohammad Saleh",
                    "Peter Liu"
                ],
                "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
                "pub_date": "2020",
                "pub_title": "Proceedings of ICML",
                "pub": null
            }
        },
        {
            "ix": "124-ARR_v1_148",
            "content": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston, Personalizing dialogue agents: I have a dog, do you have pets too?, 2018, Proceedings of ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Saizheng Zhang",
                    "Emily Dinan",
                    "Jack Urbanek",
                    "Arthur Szlam",
                    "Douwe Kiela",
                    "Jason Weston"
                ],
                "title": "Personalizing dialogue agents: I have a dog, do you have pets too?",
                "pub_date": "2018",
                "pub_title": "Proceedings of ACL",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "124-ARR_v1_0@0",
            "content": "MReD: A Meta-Review Dataset for Structure-Controllable Text Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_0",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_2@0",
            "content": "When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_2",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_2@1",
            "content": "A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_2",
            "start": 203,
            "end": 362,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_2@2",
            "content": "A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with deep understanding of the domain knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_2",
            "start": 364,
            "end": 548,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_2@3",
            "content": "Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_2",
            "start": 550,
            "end": 839,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_2@4",
            "content": "We present experimental results on start-of-the-art summarization models, and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_2",
            "start": 841,
            "end": 1039,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_2@5",
            "content": "By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_2",
            "start": 1041,
            "end": 1314,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_2@6",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_2",
            "start": 1316,
            "end": 1316,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_4@0",
            "content": "Text generation entered a new era because of the development of neural network based generation techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_4",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_4@1",
            "content": "Along the dimension of the mapping relation between the input information and the output text, we can roughly group the recent tasks into three clusters: more-to-less, less-to-more, and neck-to-neck.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_4",
            "start": 108,
            "end": 306,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_4@2",
            "content": "The more-to-less text generation tasks output a concise piece of text from some more abundant input, such as text summarization meta-review: [This paper studies n-step returns in off-policy RL and introduces a novel algorithm which adapts the return's horizon n in function of a notion of policy's age.]\u2190ABSTRACT [Over-all, the reviewers found that the paper presents interesting observations and promising experimental results.]\u2190STRENGTH [However, they also raised concerns in their initial reviews, regarding the clarity of the paper, its theoretical foundations and its positioning (notably regarding the bias/variance tradeoff of uncorrected n-step returns) and parts of the experimental results. ]\u2190WEAKNESS [In the absence of rebuttal or revised manuscript from the authors, not much discussion was triggered.]\u2190REBUTTAL PROCESS [Based on the initial reviews, the AC cannot recommend accepting this paper, but the authors are encouraged to pursue this interesting research direction.]\u2190DECISION Table 1: An example of annotated meta-review.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_4",
            "start": 308,
            "end": 1350,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_4@3",
            "content": "CATE-GORY indicates the category of each sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_4",
            "start": 1352,
            "end": 1401,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_4@4",
            "content": "(Tan et al., 2017;Kry\u015bci\u0144ski et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_4",
            "start": 1403,
            "end": 1445,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_4@5",
            "content": "The lessto-more generation tasks generate a more abundant output from some obviously simpler input, such as prompt-based story generation (Fan et al., 2018b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_4",
            "start": 1447,
            "end": 1604,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_4@6",
            "content": "The neck-to-neck generation aims at generating an output text which conveys the same quantity of knowledge as the input but in natural language, such as typical RDF triples to text tasks (Gardent et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_4",
            "start": 1606,
            "end": 1815,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_5@0",
            "content": "To some extent, the existing task settings are not so adequate because they do not have deep understanding of the domains they are working on, i.e., domain knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_5",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_5@1",
            "content": "Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail (Nallapati et al., 2016) is composed of the training pairs of news content and news titles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_5",
            "start": 167,
            "end": 348,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_5@2",
            "content": "However, it does not tell why a particular piece of news content should have that corresponding title, for example for the same earnings report, why one media emphasizes its new business success in the title, but another emphasizes its net income.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_5",
            "start": 350,
            "end": 596,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_5@3",
            "content": "Obviously, there is not a standard answer regarding right or wrong.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_5",
            "start": 598,
            "end": 664,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_5@4",
            "content": "For such cases, if we can specify a control signal, e.g., \"emphasizing new business\", the generated text would make more sense to users using the text generator.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_5",
            "start": 666,
            "end": 826,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@0",
            "content": "To allow controlling not only the intent of a single generated sentence but also the whole structure of a generated passage, we prepare a new dataset MReD (short for Meta-Review Dataset) with in-depth understanding of the structure of meta-reviews in a peer-reviewing system, namely the open review system of ICLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 0,
            "end": 313,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@1",
            "content": "MReD for the first time allows a generator to be trained by simultaneously taking the text (i.e. reviews) and the structure control signal as input to generate a meta-review which is not only derivable from the reviews but also complies with the control intent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 315,
            "end": 575,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@2",
            "content": "Thus from the same input text, the trained generator can generate varied outputs according to the given control signal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 577,
            "end": 695,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@3",
            "content": "For example, if the area chair is inclined to accept a borderline paper, he or she may invoke our generator with a structure of \"abstract | strength | decision\" to generate a meta-review, or may use a structure of \"abstract | weakness | suggestion\" otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 697,
            "end": 955,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@4",
            "content": "Note that for ease of preparation and explanation, we ground our dataset in the peer review domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 957,
            "end": 1055,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@5",
            "content": "However, the data preparation methodology and proposed models are transferable to other domains, which is indeed what we hope to motivate with this effort.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 1057,
            "end": 1211,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@6",
            "content": "Specifically, we collect 7,089 meta-reviews of ICLR in recent years (2018 -2021) and fully annotate the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 1213,
            "end": 1324,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@7",
            "content": "Each sentence in a meta-review is classified into one of the 9 pre-defined intent categories: abstract, strength, weakness, rating summary, area chair (AC) disagreement, rebuttal process, suggestion, decision, and miscellaneous (misc).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 1326,
            "end": 1560,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@8",
            "content": "Table 1 shows an annotated example, where each sentence is classified into a single category that best describes the intent of this sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 1562,
            "end": 1702,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@9",
            "content": "Our MReD is obviously different from previous text generation/summarization datasets because, given the rich annotations of individual meta-review sentences, a model is allowed to learn more sophisticated generation behaviors to control the structure of the generated passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 1704,
            "end": 1979,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_6@10",
            "content": "Our proposed task is also noticeably different from existing controllable text generation tasks (e.g., text style transfer on sentiment polarity (Shen et al., 2017;Liao et al., 2018) and formality (Shang et al., 2019)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_6",
            "start": 1981,
            "end": 2298,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_7@0",
            "content": "To summarize, our contributions are as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_7",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_8@0",
            "content": "(1) We introduce a fully-annotated meta-review dataset to make better use of the domain knowledge for text generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_8",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_8@1",
            "content": "Thorough data analysis (3) We design simple yet effective control methods that are independent of the model architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_8",
            "start": 119,
            "end": 239,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_8@2",
            "content": "We show the effectiveness of enforcing different generation structures with a detailed model analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_8",
            "start": 241,
            "end": 342,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_9@0",
            "content": "We will release our full dataset, code, and detailed settings to the community.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_9",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_10@0",
            "content": "MReD: Meta-Review Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_10",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_11@0",
            "content": "In this paper, we explore a new task, named the structure-controllable text generation, in a new domain, namely the meta-reviews in the peer reviewing system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_11",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_11@1",
            "content": "Unlike previous datasets that mainly focus on domains like news, meta-review is a worthstudying domain containing essential and highdensity opinions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_11",
            "start": 159,
            "end": 307,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_11@2",
            "content": "Specifically, during the peer review process of scientific papers, a senior reviewer or area chair will recommend a decision and manually write a meta-review to summarize the opinions from different reviews written by the reviewers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_11",
            "start": 309,
            "end": 540,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_11@3",
            "content": "We first introduce the data collection process and then describe the annotation details, followed by dataset analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_11",
            "start": 542,
            "end": 659,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_12@0",
            "content": "Data Collection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_12",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_13@0",
            "content": "We collect the meta-review related data from an online peer reviewing platform for ICLR 2 from 2018 to 2021.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_13",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_13@1",
            "content": "Note that the submissions from earlier years are not collected because their meta-reviews are not released.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_13",
            "start": 109,
            "end": 215,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_13@2",
            "content": "To prepare our dataset for controllable text generation, for each submission, we collect multiple reviews with reviewer ratings and confidence scores, the final meta-review decision, and the meta-review passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_13",
            "start": 217,
            "end": 427,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_13@3",
            "content": "any further annotation, the dataset can already naturally serve the purpose of multi-document summarization (MDS).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_13",
            "start": 429,
            "end": 542,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_13@4",
            "content": "Compared with those conventional datasets for MDS, such as TAC (Owczarzak and Dang, 2011) and DUC (Over and Yen, 2004), which contain in total a few hundred input articles (equivalent to reviews in MReD), our dataset is more than 10 times larger.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_13",
            "start": 544,
            "end": 789,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_14@0",
            "content": "Data Annotation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_14",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_15@0",
            "content": "As aforementioned, the structure-controllable text generation aims at controlling the structure of the generated passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_15",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_15@1",
            "content": "Therefore, we need to comprehensively understand the structures of metareviews so as to enable a model to learn how to generate outputs complying with certain structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_15",
            "start": 122,
            "end": 291,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_15@2",
            "content": "Specifically, based on the nature of meta-reviews, we pre-define 9 intent categories: abstract, strength, weakness, suggestion, rebuttal process, rating summary, area chair (AC) disagreement, decision, and miscellaneous (misc).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_15",
            "start": 293,
            "end": 519,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_15@3",
            "content": "Table 3 shows the definition for each category (see example sentences in Appendix A.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_15",
            "start": 521,
            "end": 607,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_15@4",
            "content": "The identification of category for some sentences is fairly straightforward, while some sentences are relatively ambiguous.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_15",
            "start": 609,
            "end": 731,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_15@5",
            "content": "Therefore, besides following the definition of each category, the annotators are also required to follow the additional rules as elaborated in Appendix A.2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_15",
            "start": 733,
            "end": 887,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_16@0",
            "content": "For conducting the annotation work, 14 professional data annotators from a data company are initially trained, and 12 of them are selected for the task according to their annotation quality during a trial round.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_16",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_16@1",
            "content": "These 12 annotators are fully paid for their work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_16",
            "start": 212,
            "end": 261,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_16@2",
            "content": "Each meta-review sentence is independently labeled by 2 different annotators, and a third annotator resolves any disagreement between the first two annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_16",
            "start": 263,
            "end": 421,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_16@3",
            "content": "We label 45,929 sentences from 7,089 meta-reviews in total, and the Cohen's kappa is 0.778 between the two annotators, showing that the annotation is of quite high quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_16",
            "start": 423,
            "end": 594,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_17@0",
            "content": "Data Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_17",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_18@0",
            "content": "To better understand the MReD dataset, we conduct the following analysis along different dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_18",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_19@0",
            "content": "Sentence distribution across categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_19",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_19@1",
            "content": "The sentence numbers in different categories are shown in Figure 1, breakdown by the decision (i.e., accept or reject).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_19",
            "start": 41,
            "end": 159,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_19@2",
            "content": "Among 7,089 submissions, there are 2,368 accepted and 4,721 rejected.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_19",
            "start": 161,
            "end": 229,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_19@3",
            "content": "Among all submissions and the rejected submissions, \"weakness\" accounts for the largest proportion, while across the accepted ones, \"abstract\" and \"strength\" take up a great proportion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_19",
            "start": 231,
            "end": 415,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_19@4",
            "content": "To some extent, these three categories which dominate in meta-reviews could be easily summarized from the reviewers' comments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_19",
            "start": 417,
            "end": 542,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_19@5",
            "content": "However, some minor or subjective categories (e.g., \"ac disagreement\") are hard to generate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_19",
            "start": 544,
            "end": 635,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_20@0",
            "content": "Breakdown analysis by meta-review lengths and average rating scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_20",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_20@1",
            "content": "We present the percentage of meta-reviews of different lengths in each score range, as shown in Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_20",
            "start": 69,
            "end": 173,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_20@2",
            "content": "For example, among the meta-reviews that receive the reviewers' average score below 2 (i.e., the first column in the figure), 28% are less than or equal to 50 words, and 38% fall in the length range of 51 to 100 words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_20",
            "start": 175,
            "end": 392,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_20@3",
            "content": "We can observe that the meta-reviews tend to be longer for those submissions receiving scores in the middle range, while shorter for those with lower scores or higher scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_20",
            "start": 394,
            "end": 567,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_20@4",
            "content": "This coincides with our commonsense that for high-score and low-score sub- missions, the decision tends to be a clear accept or reject so that meta-reviews can be relatively shorter, while for those borderline submissions, area chairs have to carefully weigh the pros and cons to make the final decision (see Appendix B.1 for borderline submission analysis).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_20",
            "start": 569,
            "end": 926,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_20@5",
            "content": "As shown in Figure 3, the meta-reviews with more than 150 words generally have a larger proportion of sentences describing \"weakness\" and \"suggestion\" for authors to improve the submissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_20",
            "start": 928,
            "end": 1117,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_20@6",
            "content": "Additional analysis on the category breakdown for accepted and rejected papers across the score ranges is shown in Appendix B.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_20",
            "start": 1119,
            "end": 1246,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_21@0",
            "content": "Meta-review patterns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_21",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_21@1",
            "content": "To study the common structures of meta-reviews, we present the transition matrix of different category segments in Figure 4, where the sum of each row is 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_21",
            "start": 22,
            "end": 177,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_21@2",
            "content": "Note that each segment represents the longest consecutive sentences with the same category.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_21",
            "start": 179,
            "end": 269,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_21@3",
            "content": "We add \"<start>\" and \"<end>\" tokens before and after each metareview accordingly to investigate which categories tend to be at the start/end of the meta-reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_21",
            "start": 271,
            "end": 431,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_21@4",
            "content": "It is clear to see that \"abstract\" usually positions at the beginning of the meta-review, while \"suggestion\" and \"decision\" usually appear at the end.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_21",
            "start": 433,
            "end": 582,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_21@5",
            "content": "There are also some clear patterns appearing in the metareviews, such as \"abstract | strength | weakness\", \"rating summary | weakness | rebuttal process\", and \"abstract | weakness | decision\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_21",
            "start": 584,
            "end": 775,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_22@0",
            "content": "3 Structure-Controllable Text Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_22",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_23@0",
            "content": "Task Definition",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_23",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_24@0",
            "content": "As aforementioned, in uncontrolled generation, users cannot instruct the model to emphasize on desired aspects.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_24",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_24@1",
            "content": "However, in a domain such as meta-reviews, given the same review inputs, one AC may emphasize more on the \"strength\" of the paper following a structure of \"abstract | strength | decision\", whereas another AC may prefer a different structure with more focus on reviewers' opinions and suggestions (i.e., \"rating summary\" and . 00 .00 .38 .32 .11 .00 .09 .01 .03 .05 .01 .00 .01 .00 .47 .06 .01 .14 .06 .11 .04 .10 .00 .00 .04 .00 .05 .01 .25 .17 .22 .05 .21 .00 .01 .10 .21 .00 .01 .13 .07 .21 .05 .22 .00 .01 .07 .10 .06 .00 .09 .10 .27 .11 .20 .00 .01 .06 .09 .09 .01 .00 .11 .36 .06 .21 .00 .00 .02 .03 .02 .00 .05 .00 .15 .05 .68 .00 .01 .02 .04 .01 .00 .01 .09 .00 .04 .79 .00 .03 .14 .20 .07 .01 .09 .06 .14 .00 .26 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 .00 \"suggestion\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_24",
            "start": 112,
            "end": 890,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_24@2",
            "content": "To achieve such flexibility, the task of structure-controllable text generation is defined as: given the text input (i.e., reviews) and a control sequence of the output structure, a model should generate a meta-review which is derivable from the reviews and presents the required structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_24",
            "start": 892,
            "end": 1181,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_25@0",
            "content": "Explored Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_25",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_26@0",
            "content": "As the recent generation works (Vaswani et al., 2017;Liu and Lapata, 2019;Xing et al., 2020) basically adopt an encoder-decoder based architecture and achieve state-of-the-art performance on many tasks and datasets, we primarily investigate the performance of such a framework on our task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_26",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_26@1",
            "content": "Thus in this subsection, we mainly present how to re-organize the input reviews and the control structure as an input sequence of the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_26",
            "start": 290,
            "end": 431,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_26@2",
            "content": "We also explore other baselines in the experiments later.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_26",
            "start": 433,
            "end": 489,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_27@0",
            "content": "In order to summarize multiple reviews into a meta-review showing a required structure, we explicitly specify the control label sequence that a model should comply with during generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_27",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_27@1",
            "content": "Specifically, we intuitively add the control sequence in front of the input text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_27",
            "start": 188,
            "end": 268,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_27@2",
            "content": "By directly combining both the control and textual information as a single input, our control method is independent of any specially designed encoder and decoder structures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_27",
            "start": 270,
            "end": 442,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_27@3",
            "content": "Moreover, by placing the short control sequence in front, an encoder can immediately observe the control signal at the very beginning, thus avoids the possible interference by the subsequent sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_27",
            "start": 444,
            "end": 643,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_27@4",
            "content": "Moreover, the control sequence in front will never be truncated when the encoder truncates the input to a certain length limit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_27",
            "start": 645,
            "end": 771,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_28@0",
            "content": "Given the multiple review inputs, we need to linearize them into a single input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_28",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_28@1",
            "content": "One simple method to combine multiple inputs for encoder-decoder models is to concatenate all inputs one after another (Fabbri et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_28",
            "start": 81,
            "end": 221,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_28@2",
            "content": "Beside the text inputs, the review rating is also crucial information for writing meta reviews, which cannot be found in the review passages but exists in the field of rating score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_28",
            "start": 223,
            "end": 403,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_28@3",
            "content": "Therefore, we create a rating sentence that consists of the extracted ratings given by the corresponding reviewers and prepend it to our concatenated review texts to obtain the final input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_28",
            "start": 405,
            "end": 593,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_28@4",
            "content": "We name this method rate-concat (see Table 4, upper).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_28",
            "start": 595,
            "end": 647,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_29@0",
            "content": "We also show explorations with other review combination methods in Appendix C.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_29",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_30@0",
            "content": "As aforementioned, we place the control sequence in front of the re-organized review information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_30",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_30@1",
            "content": "Specifically, we explore two different control methods, namely, sent-ctrl and seg-ctrl.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_30",
            "start": 98,
            "end": 184,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_30@2",
            "content": "Sent-ctrl uses one control label per target sentence and controls generation on a sentence-level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_30",
            "start": 186,
            "end": 282,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_30@3",
            "content": "Note that this method can allow implicit control on the length (i.e., number of sentences) of the generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_30",
            "start": 284,
            "end": 392,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_30@4",
            "content": "Segctrl treats consecutive sentences of the same label as one segment and only uses one label for a single segment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_30",
            "start": 394,
            "end": 508,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_30@5",
            "content": "Example inputs of different control settings are shown in Table 4 (lower).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_30",
            "start": 510,
            "end": 583,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_30@6",
            "content": "For instance, sent-ctrl repeats \"abstract\" in its control sequence whereas seg-ctrl does not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_30",
            "start": 585,
            "end": 677,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_30@7",
            "content": "This is because seg-ctrl treats the 1 st and 2 nd target sentences of \"abstract\" as the same segment and only uses a single label to indicate it in the sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_30",
            "start": 679,
            "end": 839,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_30@8",
            "content": "Additionally, we provide a vanilla setting for uncontrolled generation, unctrl, where no control sequence is used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_30",
            "start": 841,
            "end": 954,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_31@0",
            "content": "Using the above input sequence as the source and the corresponding meta-review as the target, we can train an encoder-decoder model for controllable generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_31",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_31@1",
            "content": "Many transformer-based models have achieved state-of-the-art performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_31",
            "start": 161,
            "end": 233,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_31@2",
            "content": "Common abstractive summarization models include BART (Lewis et al., 2020), T5 (Raffel et al., 2020) and PEGASUS (Zhang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_31",
            "start": 235,
            "end": 367,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_31@3",
            "content": "In this paper we focus on the bart-large-cnn model, one variant of the BART model (results on other pretrained models can be found in Appendix D.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_31",
            "start": 369,
            "end": 516,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_31@4",
            "content": "More specifically, we use the pytorch implementation in the open-source library Hugging Face Transformers (Wolf et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_31",
            "start": 518,
            "end": 643,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_31@5",
            "content": "Hence, all our future usage of the word \"Transformers\" refers to bart-large-cnn in the Transformers library .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_31",
            "start": 645,
            "end": 753,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_32@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_32",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_33@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_33",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_34@0",
            "content": "Extractive Baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_34",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_34@1",
            "content": "We employ three common extractive summarization baselines each of which basically provides a mechanism to rank the input sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_34",
            "start": 22,
            "end": 152,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_34@2",
            "content": "LexRank (Erkan and Radev, 2004) represents sentences in a graph and uses eigenvector centrality to calculate sentence importance scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_34",
            "start": 154,
            "end": 289,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_34@3",
            "content": "TextRank (Mihalcea and Tarau, 2004) is another graph-based sentence ranking method that obtains vertex scores by running a \"random-surfer model\" until convergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_34",
            "start": 291,
            "end": 453,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_34@4",
            "content": "MMR (Carbonell and Goldstein, 1998) calculates sentence scores by balancing the redundancy score with the information relevance score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_34",
            "start": 455,
            "end": 588,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_34@5",
            "content": "After ranking with the above models, we select sentences as output with different strategies according to the controlled and uncontrolled settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_34",
            "start": 590,
            "end": 736,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_34@6",
            "content": "For the uncontrolled setting, we simply select the top k sentences as the generated output, where k is a hyperparameter deciding the size of the generated output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_34",
            "start": 738,
            "end": 899,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_34@7",
            "content": "For the controlled setting, we select only the top sentences with the right category labels according to the control sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_34",
            "start": 901,
            "end": 1026,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_34@8",
            "content": "To do so, we employ an LSTM-CRF (Lample et al., 2016) tagger trained on the labeled meta-reviews to predict the sentence labels of each input review.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_34",
            "start": 1028,
            "end": 1176,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_34@9",
            "content": "Refer to Appendix D.2 for more details of the tagger.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_34",
            "start": 1178,
            "end": 1230,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_35@0",
            "content": "Generic Sentence Baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_35",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_35@1",
            "content": "Considering the nature of meta-reviews, we could imagine some categories may have common phrases inflating the Rouge scores, such as \"This paper proposes ...\" for abstract, and \"I recommend acceptance.\" for decision, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_35",
            "start": 28,
            "end": 248,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_35@2",
            "content": "To examine such impact, we select sentences that are generic in each category and combine these sentences to generate outputs according to the control sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_35",
            "start": 250,
            "end": 410,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_35@3",
            "content": "For instance, if the control sequence is \"abstract | strength | decision\", we take the most generic sentences from the categories of \"abstract\", \"strength\" and \"decision\" respectively to form the output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_35",
            "start": 412,
            "end": 614,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_35@4",
            "content": "Specifically, we create two generic sentence baselines by obtaining generic sentences from the training data from either the meta-review references (i.e., target) or the input reviews (i.e., source), namely \"Target Generic\" and \"Source Generic\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_35",
            "start": 616,
            "end": 860,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_35@5",
            "content": "Moreover, we also study such impact on the high-score and low-score submissions respectively, since an AC may write more succinct meta-reviews for clear-cut papers, as suggested by Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_35",
            "start": 862,
            "end": 1051,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_35@6",
            "content": "See Appendix D.3 for more details and results on generic sentence baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_35",
            "start": 1053,
            "end": 1128,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_36@0",
            "content": "Experimental Setting",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_36",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_37@0",
            "content": "To conduct text generation experiments, we preprocess our MReD dataset by filtering to ensure the selected meta-reviews have 20 to 400 words, as certain meta-review passages are extremely short or long.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_37",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_37@1",
            "content": "After preprocessing, we obtain 6,693 sourcetarget pairs, for which we randomly split into train, validation, and test sets by a ratio of 8:1:1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_37",
            "start": 203,
            "end": 345,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_37@2",
            "content": "We evaluate our generated outputs against the reference meta-reviews using the F 1 scores of ROUGE 1 , ROUGE 2 , and ROUGE L (Lin, 2004) 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_37",
            "start": 347,
            "end": 486,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_37@3",
            "content": "For the extractive and generic baselines, a key hyperparameter is the sentence number k, which we set to the number of labels in the sent-ctrl control sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_37",
            "start": 488,
            "end": 647,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_37@4",
            "content": "More setting details are shown in Appendix D.4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_37",
            "start": 649,
            "end": 694,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_38@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_38",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_39@0",
            "content": "We show results in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_39",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_39@1",
            "content": "Only the best settings of rate-concat (Table 12 in Appendix C.1) and input truncation of 2048 tokens (Appendix D.5) for the Transformers are included.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_39",
            "start": 28,
            "end": 177,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_39@2",
            "content": "Amongst the extractive baselines, TextRank performs the best in both unctrl and sent-ctrl settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_39",
            "start": 179,
            "end": 277,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_39@3",
            "content": "Nevertheless, all controlled methods outperform their unctrl settings (same for the Transformers).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_39",
            "start": 279,
            "end": 376,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_39@4",
            "content": "This validates our intuition that structure-controlled generation is more suitable for user-subjective writings such as meta-reviews, because the model can better satisfy different structure requirements when supplied with the corresponding control sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_39",
            "start": 378,
            "end": 636,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_39@5",
            "content": "On the other hand, for the Transformers, sent-ctrl is the best, followed by seg-ctrl.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_39",
            "start": 638,
            "end": 722,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_39@6",
            "content": "This is most likely due to the former's more fine-grained sentence-level control that provides a clearer structure outline, as compared to the coarser segment-level control.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_39",
            "start": 724,
            "end": 896,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_39@7",
            "content": "Moreover, the Transformers far outperform the extractive baselines, showing that the extractionbased methods are insufficient for MReD. This also suggests that meta-review writings are different from the input reviews, therefore copying full review sentences to form meta-reviews doesn't work well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_39",
            "start": 898,
            "end": 1195,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_39@8",
            "content": "This is again validated by the \"Target Generic\" baseline's significant improvement over the \"Source Generic\" baseline, which shows that generic sentences from meta-reviews can suit generation much better than those in reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_39",
            "start": 1197,
            "end": 1422,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_39@9",
            "content": "Nevertheless, all Transformers results are still much better than the \"Target Generic\" sentence baseline, showing that despite generic phrases in some categories contributing to Rouge, the Transformers model is capable of capturing content-specific information for each input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_39",
            "start": 1424,
            "end": 1699,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_40@0",
            "content": "Case Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_40",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_41@0",
            "content": "We study some cases for a better understanding of the structure-controllable generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_41",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_42@0",
            "content": "Identify the control label for each sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_42",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_42@1",
            "content": "We first evaluate whether the model is able to attend to the correct control label during generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_42",
            "start": 46,
            "end": 146,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_42@2",
            "content": "For each generation step, we obtain the cross attention weights from the decoder's output token towards the control labels and plot them in",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_42",
            "start": 148,
            "end": 286,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_43@0",
            "content": "Sent 1 (abstract):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_43",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_44@0",
            "content": "This paper proposes a selfsupervised contrastive learning method for few-shot learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_44",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_45@0",
            "content": "Sent 2 (weakness):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_45",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_46@0",
            "content": "The reviewers agree that the idea is interesting, but have concerns about the clarity of the paper and the lack of comparison to the baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_46",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_47@0",
            "content": "The paper is not suitable for publication at ICLR in its current form.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_47",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_48@0",
            "content": "Table 7: Attention analysis for each output sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_48",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_49@0",
            "content": "decision\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_49",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_49@1",
            "content": "When generating each sentence, we can see that the attention weights of the corresponding control token are the highest, which demonstrates that our model can effectively pay attention to the correct control label and thus generate the content complying with the intent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_49",
            "start": 11,
            "end": 280,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_50@0",
            "content": "To understand what information the model attends to when generating each sentence, we aggregate the cross attention weights to obtain the attention scores from each generated sentence towards all input sentences (Appendix D.6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_50",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_50@1",
            "content": "Then, we select the top 3 input sentences with the highest attention scores for each generated sentence, and visualize the normalized attention weights on all tokens in the selected sentences and the control sequence in Table 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_50",
            "start": 228,
            "end": 455,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_50@2",
            "content": "As shown, the model can correctly extract relevant information from the source sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_50",
            "start": 457,
            "end": 545,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_50@3",
            "content": "For example, it identifies important phrases such as \"interesting\", \"clarity\" and \"lack of comparison to baselines\" when generating \"Sent 2\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_50",
            "start": 547,
            "end": 687,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_51@0",
            "content": "Generate varied outputs given different control sequences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_51",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_51@1",
            "content": "To further investigate the effectiveness of the control sequence, we change the control sequence of the above example and re-generate the meta-reviews given the same input reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_51",
            "start": 59,
            "end": 238,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_51@2",
            "content": "In Table 6, we first show the gold meta-review and the model output using the original control sequence in Row 0 and Row 1, and then show the model outputs with alternative control sequences in Row 2 and Row 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_51",
            "start": 240,
            "end": 449,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_51@3",
            "content": "From the outputs, we can see that indeed each generated sentence corresponds to its control label well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_51",
            "start": 451,
            "end": 553,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_51@4",
            "content": "In Row 2, we add an additional control label in the sequence and by repeating the \"abstract\" label, the generator can further elaborate more details of the studied method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_51",
            "start": 555,
            "end": 725,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_51@5",
            "content": "This is one key advantage of our sent-ctrl compared to the seg-ctrl, which allows the control of length and the level of the generation details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_51",
            "start": 727,
            "end": 870,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_51@6",
            "content": "In Row 3, a very comprehensive control sequence is specified.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_51",
            "start": 872,
            "end": 932,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_51@7",
            "content": "We can see that the output meta-review is quite fluent and polite to reject the borderline paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_51",
            "start": 934,
            "end": 1030,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_51@8",
            "content": "See Appendix D.7 for more examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_51",
            "start": 1032,
            "end": 1066,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_52@0",
            "content": "Human Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_52",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_53@0",
            "content": "In addition to the Rouge evaluation, we ask 3 human judges to manually assess the generation quality of the Transformers models from We grade fluency and content relevance on a scale of 1 to 5, whereas structure similarity and decision correctness are calculated from 0 to 1 (Appendix D.8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_53",
            "start": 0,
            "end": 289,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_53@1",
            "content": "For structure similarity, because sent-ctrl and seg-ctrl have different control sequences, we evaluate the two models on sentence-level (sent) and segment-level (seg) structures respectively, and provide both evaluations for unctrl.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_53",
            "start": 291,
            "end": 522,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_54@0",
            "content": "As shown in Table 8, both sent-ctrl and seg-ctrl models show significant improvements on the generation structure over the uncontrolled baseline, which affirms the effectiveness of our proposed methods for structure-controllable generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_54",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_54@1",
            "content": "Sentctrl also has better fluency and decision correctness, suggesting that having a better output structure can benefit the readability and decision generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_54",
            "start": 241,
            "end": 400,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_54@2",
            "content": "For the content relevance, the scores of all methods are reasonably good, and significance tests cannot prove any best model (p > 0.08).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_54",
            "start": 402,
            "end": 537,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_54@3",
            "content": "Nevertheless, it is possible that the looser control a method applies, the better relevance score it achieves.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_54",
            "start": 539,
            "end": 648,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_54@4",
            "content": "It is because a tighter control narrows the content that a model can use from the reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_54",
            "start": 650,
            "end": 739,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_55@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_55",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_56@0",
            "content": "To facilitate the study of text summarization, earlier datasets are mostly in the news domain with relatively short input passages, such as NYT (Sandhaus, 2008), Gigaword (Napoles et al., 2012), CNN/Daily Mail (Hermann et al., 2015), NEWSROOM (Grusky et al., 2018) and XSUM (Narayan et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_56",
            "start": 0,
            "end": 296,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_56@1",
            "content": "Datasets for long documents include Sharma et al. (2019), Cohan et al. (2018), andFisas et al. (2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_56",
            "start": 298,
            "end": 399,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_56@2",
            "content": "In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely structurecontrollable text generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_56",
            "start": 401,
            "end": 695,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_57@0",
            "content": "Researchers recently explore the peer review domain data for a few tasks, such as PeerRead (Kang et al., 2018) for paper decision predictions, AM-PERE for proposition classification in reviews, and RR (Cheng et al., 2020) for paired-argument extraction from review-rebuttal pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_57",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_57@1",
            "content": "Additionally, a meta-review dataset is introduced by Bhatia et al. (2020) without any annotation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_57",
            "start": 281,
            "end": 377,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_57@2",
            "content": "There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_57",
            "start": 379,
            "end": 549,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_58@0",
            "content": "A wide range of control perspectives has been explored in controllable generation, including style control (e.g., sentiments (Duan et al., 2020), politeness (Madaan et al., 2020), formality , domains (Takeno et al., 2017) and persona ) and content control (e.g., length (Duan et al., 2020), entities (Fan et al., 2018a), and keywords (Tang et al., 2019)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_58",
            "start": 0,
            "end": 354,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_58@1",
            "content": "Our structure-controlled generation differs from these works as we control the high-level output structure, rather than the specific styles or the surface details of which keywords to include in the generated output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_58",
            "start": 356,
            "end": 571,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_58@2",
            "content": "Our task also differs from content planning (Reiter and Dale, 1997;Shao et al., 2019;, which involves explicitly selecting and arranging the input content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_58",
            "start": 573,
            "end": 727,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_58@3",
            "content": "Instead, we provide the model with the high-level control labels, and let the model decide on its own the relevant styles and contents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_58",
            "start": 729,
            "end": 863,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_59@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_59",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_60@0",
            "content": "This paper introduces a fully-annotated text generation dataset MReD in a new domain, i.e., the meta-reviews in the peer review system, and provides thorough data analysis to better understand the data characteristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_60",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_60@1",
            "content": "With such rich annotations, we propose simple yet effective methods for structure-controllable text generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_60",
            "start": 219,
            "end": 329,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_60@2",
            "content": "Extensive experimental results are presented as baselines for future study and thorough result analysis is conducted to shed light on the control mechanisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_60",
            "start": 331,
            "end": 487,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_61@0",
            "content": "\"The paper presents/explores/describes/addresses/proposes ...\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_61",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_62@0",
            "content": "strength \"The reviewers found the paper interesting.\" \"The method and justification are clear.\" \"The quantitative results are promising.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_62",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_63@0",
            "content": "weakness \"The paper is somewhat incremental ...\" \"... claims are confusing\" \"The main concern is ...\" \"... unfair experimental comparisons ...\" rating summary \"R1 recommends Accept.\" \"All four reviewers ultimately recommended acceptance.\" \"Reviews were somewhat mixed, but also with mixed confidence scores.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_63",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_64@0",
            "content": "ac disagreement \"The area chair considers the remaining concerns by Reviewer 3 as invalid.\" \"I do not agree with the criticism about ...\" \"I disagree with the second point ...\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_64",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_65@0",
            "content": "rebuttal process \"The authors have made various improvements to the paper\" \"... remained after the author rebuttal ...\" \"Authors provided convincing feedbacks on this key point.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_65",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_66@0",
            "content": "suggestion \"... more analysis ...\" \"The authors are advised to take into account the issues about ...\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_66",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_67@0",
            "content": "decision \"The paper is recommended as a poster presentation.\" \"AC recommends Reject.\" \"I recommend rejection.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_67",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_68@0",
            "content": "miscellaneous \"Thank you for submitting you paper to ICLR.\" \"I've summarized the pros and cons of the reviews below.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_68",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_69@0",
            "content": "Table 9: Category examples of meta-review sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_69",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_70@0",
            "content": "A Data Annotation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_70",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_71@0",
            "content": "We show category examples in Table 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_71",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_72@0",
            "content": "The additional rules for annotation are as follows: First, instead of only labeling the individual sentences per se, the annotators are given a complete paragraph of meta-review to label the sentences with context information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_72",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_72@1",
            "content": "For example, if the area chair writes a sentence providing some extra background knowledge in the discussion of the weakness of the submission, that sentence itself can be considered as \"misc\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_72",
            "start": 227,
            "end": 419,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_72@2",
            "content": "However, it should be labeled as \"weakness\" to be consistent in context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_72",
            "start": 421,
            "end": 492,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_72@3",
            "content": "Second, not every sentence can be strictly classified into a single category.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_72",
            "start": 494,
            "end": 570,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_72@4",
            "content": "When a sentence contains information from multiple categories, the annotators should consider its main point and primary purpose.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_72",
            "start": 572,
            "end": 700,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_72@5",
            "content": "One example is: \"Although the paper discusses an interesting topic and contains potentially interesting idea, its novelty is limited.\" Although the first half of the sentence discusses the strength of the submission, the primary purpose of this sentence is to point out its weakness, and therefore it should be labeled as weakness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_72",
            "start": 702,
            "end": 1032,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_73@0",
            "content": "Furthermore, there are still some cases where the main point of the sentence is hard to differentiate from multiple categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_73",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_73@1",
            "content": "We then define a priority order of these 9 categories according to the importance of each category for annotators to follow: decision > rating summary > strength ?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_73",
            "start": 128,
            "end": 290,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_73@2",
            "content": "= weakness > ac disagreement > rebuttal process > abstract > suggestion > miscellaneous.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_73",
            "start": 292,
            "end": 379,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_73@3",
            "content": "We use the sign \" ? =\" because there are some rare cases where a sentence contains both \"strength\" and \"weakness\" while there is no obvious emphasis on either, and it is hard to tell whether \"strength\" should have a priority over \"weakness\" or the other way round.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_73",
            "start": 381,
            "end": 644,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_73@4",
            "content": "We then label this sentence based on the final decision: if this submission is accepted, we label the sentence as \"strength\", and vice versa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_73",
            "start": 646,
            "end": 786,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_74@0",
            "content": "We further analyze the category distribution in borderline papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_74",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_74@1",
            "content": "As shown in Table 10, for submissions within the score range of [4.5,6), there are 713 accepted submissions and 2,588 rejected submissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_74",
            "start": 67,
            "end": 205,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_74@2",
            "content": "One clear difference is the percentage of \"strength\" and \"weakness\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_74",
            "start": 207,
            "end": 274,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_74@3",
            "content": "Another difference is the percentage of \"ac disagreement\", where the accepted papers have four times the value than rejected ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_74",
            "start": 276,
            "end": 405,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_74@4",
            "content": "This suggests that for the accepted borderline papers, the area chair tends to share different opinions with reviewers, and thus deciding to accept the borderline submissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_74",
            "start": 407,
            "end": 581,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@0",
            "content": "We further analyze the occurrence of each category for accepted papers and rejected papers separately across different score ranges, as shown in Table 11.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@1",
            "content": "For accepted papers, as the score increases, the percentage of meta-reviews having \"weakness\" and \"suggestion\" drops because the high-score submissions are more likely to be accepted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 155,
            "end": 337,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@2",
            "content": "Even the percentage of \"decision\" drops following the same trend.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 339,
            "end": 403,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@3",
            "content": "In addition, the proportion of meta-reviews having \"rebuttal process\" is larger for submissions with lower scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 405,
            "end": 518,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@4",
            "content": "This suggests that the rebuttal process plays an important role in the peer review process, especially in helping the borderline papers to be accepted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 520,
            "end": 670,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@5",
            "content": "On the other hand, for rejected papers, the percentage of meta-reviews having \"strength\" increases as the average score increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 672,
            "end": 801,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@6",
            "content": "This coincides with our common sense that the submissions receiving higher scores tend to have more strengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 803,
            "end": 912,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@7",
            "content": "One interesting finding here is that the percentage of \"weakness\" and \"suggestion\" also increases as the average rating score increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 914,
            "end": 1049,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@8",
            "content": "This may be due to two main reasons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 1051,
            "end": 1086,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@9",
            "content": "First, to reject a submission with higher scores, the area chair has to explain the weakness with more details and provide more suggestions for authors to further improve their submissions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 1088,
            "end": 1276,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@10",
            "content": "Second, compared to the percentage of \"strength\", \"weakness\" definitely has a larger percentage within any range of rating scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 1278,
            "end": 1407,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_75@11",
            "content": "The difference in the percentage of \"strength\" and \"weakness\" is intuitively different between the accepted papers and the rejected papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_75",
            "start": 1409,
            "end": 1547,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_76@0",
            "content": "We explore alternative methods to linearize the multiple reviews of the same submission, namely, concat and merge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_76",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_76@1",
            "content": "For the concat, we simply concatenate all reviews one after another according to their reviewers' sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_76",
            "start": 115,
            "end": 221,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_76@2",
            "content": "For merge, we can obtain the merged content as follows: From all review inputs, we use the longest one as a backbone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_76",
            "start": 223,
            "end": 339,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_76@3",
            "content": "We segment all reviews' content on a paragraph level, and encode them using SentenceTransformers (Reimers and Gurevych, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_76",
            "start": 341,
            "end": 466,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_76@4",
            "content": "Then, for each paragraph embedding in the non-backbone reviews, we calculate a cosine similarity score with each backbone paragraph embedding, and insert it after the backbone paragraph with which it has the highest similarity score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_76",
            "start": 468,
            "end": 700,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_76@5",
            "content": "We repeat the process for all paragraphs in non-backbone reviews to obtain a single passage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_76",
            "start": 702,
            "end": 793,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_76@6",
            "content": "Additionally, we provide a baseline setting longestreview, which does not combine reviews but only uses the longest review as the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_76",
            "start": 795,
            "end": 930,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_76@7",
            "content": "Moreover, we add rating sentences in front of the results of concat and merge to obtain rate-concat and rate-merge, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_76",
            "start": 932,
            "end": 1060,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_77@0",
            "content": "As shown in Table 12, the longest-review setting has the worst performance, thus validating that the review combination methods are necessary in order not to omit important information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_77",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_77@1",
            "content": "rate-concat setting has the best overall performance, which is the setting used throughout the main paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_77",
            "start": 186,
            "end": 291,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_78@0",
            "content": "We provide baselines of uncontrolled generation and controlled generation on MReD using other common Transformer pretrained models in Table 13.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_78",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_79@0",
            "content": "To obtain labels on source input, we train a tagger based on the human-annotated meta-reviews, then use it to predict labels on the input sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_79",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_79@1",
            "content": "Specifically, we define the task as a sequence labeling problem and apply the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) We report the F 1 scores for each category as well as the overall micro F 1 and macro F 1 scores in Table 14.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_79",
            "start": 149,
            "end": 400,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_79@2",
            "content": "Micro F1 is the overall accuracy regardless of the categories, whereas macro F1 is an average of per category accuracy evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_79",
            "start": 402,
            "end": 531,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_79@3",
            "content": "Since some of the category labels (eg. \"ac disagreement\") are very rare, their classification accuracy is low.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_79",
            "start": 533,
            "end": 642,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_79@4",
            "content": "Overall, micro F1 is a more important metric since it suggests general performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_79",
            "start": 644,
            "end": 726,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_79@5",
            "content": "The results stand proof that the majority of the categories have their own characteristics that can be identified from other categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_79",
            "start": 728,
            "end": 863,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_79@6",
            "content": "RoBERTabase is the best performing model, therefore we use this model for review sentence label prediction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_79",
            "start": 865,
            "end": 971,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_80@0",
            "content": "Besides the baselines of \"Source Generic\" and \"Target Generic\", we explore subsets of papers with high scores (average reviewers' rating 7) or low scores (average reviewers' rating 3) to obtain 4 additional generic baselines: \"Source High Score\", \"Source Low Score\", \"Target High Score\", \"Target Low Score\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_80",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_80@1",
            "content": "We use \"Target Generic\" as an example to explain how we obtain the generic sentences: We first group all meta-review sentences from the training set according to their label categories, and then re-arrange the sentences in each category using TextRank (our best performing extractive model).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_80",
            "start": 308,
            "end": 598,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_80@2",
            "content": "Since TextRank ranks the input sentences based on each sentence's content connection with others, sentences with higher rankings are also more general in the sense that they have more shared content with others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_80",
            "start": 600,
            "end": 810,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_80@3",
            "content": "Similarly, different sets of generic sentences can be obtained for the other 5 baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_80",
            "start": 812,
            "end": 900,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_81@0",
            "content": "After obtaining the generic sentence sets, we can create baseline generations using the sent-ctrl sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_81",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_81@1",
            "content": "We avoid using the same sentence twice inside the same generation, so if the same label appears multiple times in a control sequence, we will use the same number of generic sentences for that category down the ranking order.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_81",
            "start": 108,
            "end": 331,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_82@0",
            "content": "We show results in Table 15.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_82",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_82@1",
            "content": "The low score baselines perform the best amongst both source and target baselines, suggesting that the sentences from low score submissions are more typical for both reviews and meta-reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_82",
            "start": 29,
            "end": 219,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_83@0",
            "content": "For preprocessing, besides filtering based on metareview length, we also remove submissions with only one or two reviews, since the majority of the submissions have more than 3 reviews.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_83",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_84@0",
            "content": "For the extractive baselines, recall that under the sent-ctrl setting, the control sequence length is the same as the sentence number of the target metareview.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_84",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_84@1",
            "content": "Therefore, to conduct a fair comparison, we set the hyperparameter k equal to the number of Table 17: Source length statistics on all data splits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_84",
            "start": 160,
            "end": 305,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_84@2",
            "content": "Max for maximum source length, med for median source length, and avg for average source length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_84",
            "start": 307,
            "end": 401,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_84@3",
            "content": "labels in the control sequence for both controlled and uncontrolled extractive baselines, and sent-ctrl is used for all controlled extractive baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_84",
            "start": 403,
            "end": 554,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_84@4",
            "content": "We also adopt the same k for the generic baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_84",
            "start": 556,
            "end": 606,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_85@0",
            "content": "For the Transformers, we first load the pretrained model and then fine-tune it on MReD. All experiments are conducted on single V100 GPUs, using a batch size of 1 in order to fit the large pretrained model on a single GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_85",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_85@1",
            "content": "During finetuning, we set the Transformers' hyperparameters of \"minimum_target_length\" to 20, and \"maxi-mum_target_length\" to 400, according to our filter range on the meta-review lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_85",
            "start": 223,
            "end": 410,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_85@2",
            "content": "For the rest of the hyperparameters, we use the pretrained model's default values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_85",
            "start": 412,
            "end": 493,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_85@3",
            "content": "Due to long inputs (see",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_85",
            "start": 495,
            "end": 517,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_86@0",
            "content": "By default, the Transformers truncate the source to 1024 tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_86",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_86@1",
            "content": "We further investigate the performance of different source truncation lengths using rate-concat.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_86",
            "start": 65,
            "end": 160,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_86@2",
            "content": "As shown in Table 18, truncating the source to 2048 tokens consistently achieves the best performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_86",
            "start": 162,
            "end": 263,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_87@0",
            "content": "During generation, we can obtain the attention weights of each output token towards all input tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_87",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_87@1",
            "content": "Specifically, we average all decoder layers' cross attention weights for the same output token generated at each decoding step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_87",
            "start": 102,
            "end": 228,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_87@2",
            "content": "We then calculate an attention value for that output token on each input sentence, by aggregating the token's attention weights on the list of input tokens that belong to the same sentence by max pooling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_87",
            "start": 230,
            "end": 433,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_87@3",
            "content": "Finally, we can calculate an output-sentence-to-input-sentence attention score, by adding up these attention values for the output tokens that belong to the same sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_87",
            "start": 435,
            "end": 605,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_88@0",
            "content": "Common attention aggregation methods include summation, average-pooling, and max-pooling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_88",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_88@1",
            "content": "We use max-pooling to aggregate attention for same-sentence input tokens, because summation gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_88",
            "start": 90,
            "end": 389,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_88@2",
            "content": "With max-pooling, we can correctly identify sentences with spiked attention at important phrases, regardless of sentence lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_88",
            "start": 391,
            "end": 519,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_88@3",
            "content": "For attention aggregation on the same-sentence output tokens, summation is used and can be viewed as allowing each output token to vote an attention score on all input sentences, so that the input sentence receiving the highest total score is the most relevant.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_88",
            "start": 521,
            "end": 781,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_88@4",
            "content": "We conduct trial runs of all aggregation methods on input tokens with summation for output-token aggregation for multiple generation examples, and indeed max-pooling outperforms the other two by identifying more relevant input sentences with the generated sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_88",
            "start": 783,
            "end": 1047,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_89@0",
            "content": "Once we have the attention scores, we can attribute the generation of each output sentence to a few topmost relevant input sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_89",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_89@1",
            "content": "Then, we can draw a color map of the input tokens in the selected sentences based on their relative attention weights.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_89",
            "start": 134,
            "end": 251,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_90@0",
            "content": "We show examples of the generation results using alternative control sequences on another submission in Table 16.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_90",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_90@1",
            "content": "We can see the effectiveness of controlling the output structure using our proposed method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_90",
            "start": 114,
            "end": 204,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_91@0",
            "content": "For structure similarity, we instruct the judges to label each generated sentence with the closest category.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_91",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_91@1",
            "content": "We then calculate the normalized token-level edit distance between the judge-annotated label sequence and the given control sequence, then deduct this value from 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_91",
            "start": 109,
            "end": 272,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_92@0",
            "content": "For decision correctness, we evaluate it on a binary scale where 1 indicates complete correctness and 0 otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_92",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_92@1",
            "content": "More specifically, we give 0 if the generation produces contradictory decisions and a wrong decision, or the generation does not show enough hints for rejection or acceptance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_92",
            "start": 115,
            "end": 289,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_93@0",
            "content": "Chaitanya Bhatia, Tribikram Pradhan, Metagen: An academic meta-review generation system, 2020, Proceedings of ACM-SIGIR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_93",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_94@0",
            "content": "Jaime Carbonell, Jade Goldstein, The use of mmr, diversity-based reranking for reordering documents and producing summaries, 1998, Proceedings of ACM-SIGIR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_94",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_95@0",
            "content": "Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, Luo Si, Argument pair extraction from peer review and rebuttal via multi-task learning, 2020, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_95",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_96@0",
            "content": "Arman Cohan, Franck Dernoncourt, Soon Doo, Trung Kim, Seokhwan Bui, Walter Kim, Nazli Chang,  Goharian, A discourse-aware attention model for abstractive summarization of long documents, 2018, Proceedings of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_96",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_97@0",
            "content": "Yuguang Duan, Canwen Xu, Jiaxin Pei, Jialong Han, Chenliang Li, Pre-train and plug-in: Flexible conditional text generation with variational autoencoders, 2020, Proceedings of the ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_97",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_98@0",
            "content": "G\u00fcnes Erkan,  Dragomir R Radev, Lexrank: Graph-based lexical centrality as salience in text summarization, 2004, Artificial Intelligence Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_98",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_99@0",
            "content": "Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir Radev, Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model, 2019, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_99",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_100@0",
            "content": "Angela Fan, David Grangier, Michael Auli, Controllable abstractive summarization, 2018, Proceedings of WNGT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_100",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_101@0",
            "content": "Angela Fan, Mike Lewis, Yann Dauphin, Hierarchical neural story generation, 2018, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_101",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_102@0",
            "content": "Beatriz Fisas, Francesco Ronzano, A multi-layered annotated corpus of scientific papers, 2016, Proceedings of LREC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_102",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_103@0",
            "content": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, The webnlg challenge: Generating text from rdf data, 2017, Proceedings of INLG, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_103",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_104@0",
            "content": "Max Grusky, Mor Naaman, Yoav Artzi, Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies, 2018, Proceedings of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_104",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_105@0",
            "content": "Karl Moritz Hermann, Tom\u00e1s Kocisk\u1ef3, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend, 2015, Proceedings of NIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_105",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_106@0",
            "content": "Sepp Hochreiter, J\u00fcrgen Schmidhuber, Long short-term memory, 1997, Neural Computation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_106",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_107@0",
            "content": "Xinyu Hua, Mitko Nikolov, Nikhil Badugu, Lu Wang, Argument mining for understanding peer reviews, 2019, Proceedings of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_107",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_108@0",
            "content": "Xinyu Hua, Lu Wang, Sentence-level content planning and style specification for neural text generation, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_108",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_109@0",
            "content": "Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard Hovy, Roy Schwartz, A dataset of peer reviews (peerread): Collection, insights and nlp applications, 2018, Proceedings of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_109",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_110@0",
            "content": ", Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of NAACL-HLT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_110",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_111@0",
            "content": "UNKNOWN, None, 2014, Adam: A method for stochastic optimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_111",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_112@0",
            "content": "Wojciech Kry\u015bci\u0144ski, Romain Paulus, Caiming Xiong, Richard Socher, Improving abstraction in text summarization, 2018, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_112",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_113@0",
            "content": "John Lafferty, Andrew Mccallum, Fernando Cn Pereira, Conditional random fields: Probabilistic models for segmenting and labeling sequence data, 2001, Proceedings of ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_113",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_114@0",
            "content": "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer, Neural architectures for named entity recognition, 2016, Proceedings of NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_114",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_115@0",
            "content": "Anne Lauscher, Goran Glava\u0161, Kai Eckert, Arguminsci: A tool for analyzing argumentation and rhetorical aspects in scientific writing, 2018, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_115",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_116@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_116",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_117@0",
            "content": "Maria Liakata, Simone Teufel, Advaith Siddharthan, Colin Batchelor, Corpora for the conceptualisation and zoning of scientific papers, 2010, Proceedings of LREC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_117",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_118@0",
            "content": "Yi Liao, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, Tong Zhang, QuaSE: Sequence editing under quantifiable guidance, 2018, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_118",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_119@0",
            "content": "Chin-Yew Lin, Rouge: A package for automatic evaluation of summaries, 2004, Text summarization branches out, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_119",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_120@0",
            "content": "Yang Liu, Mirella Lapata, Hierarchical transformers for multi-document summarization, 2019, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_120",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_121@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_121",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_122@0",
            "content": "Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnab\u00e1s P\u00f3czos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan Black, Shrimai Prabhumoye, Politeness transfer: A tag and generate approach, 2020, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_122",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_123@0",
            "content": "Rada Mihalcea, Paul Tarau, Textrank: Bringing order into text, 2004, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_123",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_124@0",
            "content": "Ramesh Nallapati, Bowen Zhou, \u00c7aglar Cicero Dos Santos, Bing Gul\u00e7ehre,  Xiang, Abstractive text summarization using sequence-to-sequence rnns and beyond, 2016, Proceedings of SIGNLL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_124",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_125@0",
            "content": "Courtney Napoles, Benjamin Matthew R Gormley,  Van Durme, Annotated gigaword, 2012, Proceedings of AKBC-WEKEX, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_125",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_126@0",
            "content": "Shashi Narayan, B Shay, Mirella Cohen,  Lapata, Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization, 2018, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_126",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_127@0",
            "content": "Paul Over, James Yen, An introduction to duc-2004, 2004, Proceedings of DUC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_127",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_128@0",
            "content": "Karolina Owczarzak, Hoa Dang, Overview of the tac 2011 summarization track: Guided task and aesop task, 2011, Proceedings of TAC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_128",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_129@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_129",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_130@0",
            "content": "La Ramshaw, Text chunking using transformation-based learning, 1995, Proceedings of Third Workshop on Very Large Corpora, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_130",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_131@0",
            "content": "Lev Ratinov, Dan Roth, Design challenges and misconceptions in named entity recognition, 2009, Proceedings of CoNLL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_131",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_132@0",
            "content": "Nils Reimers, Iryna Gurevych, Sentencebert: Sentence embeddings using siamese bertnetworks, 2019, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_132",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_133@0",
            "content": "Ehud Reiter, Robert Dale, Building applied natural language generation systems, 1997, Natural Language Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_133",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_134@0",
            "content": "UNKNOWN, None, 2008, The new york times annotated corpus. Linguistic Data Consortium, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_134",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_135@0",
            "content": "Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan Zhao, Shuming Shi, Rui Yan, Semi-supervised text style transfer: Cross projection in latent space, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_135",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_136@0",
            "content": "Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu, Long and diverse text generation with planning-based hierarchical variational model, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_136",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_137@0",
            "content": "Eva Sharma, Chen Li, Lu Wang, Bigpatent: A large-scale dataset for abstractive and coherent summarization, 2019, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_137",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_138@0",
            "content": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola, Style transfer from non-parallel text by cross-alignment, 2017, Proceedings of NIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_138",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_139@0",
            "content": "Shunsuke Takeno, Masaaki Nagata, Kazuhide Yamamoto, Controlling target features in neural machine translation via prefix constraints, 2017, Proceedings of WAT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_139",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_140@0",
            "content": "Jiwei Tan, Xiaojun Wan, Jianguo Xiao, Abstractive document summarization with a graphbased attentional neural model, 2017, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_140",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_141@0",
            "content": "Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric Xing, Zhiting Hu, Targetguided open-domain conversation, 2019, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_141",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_142@0",
            "content": "Simone Teufel, Jean Carletta, Marc Moens, An annotation scheme for discourse-level argumentation in research articles, 1999, Proceedings of EACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_142",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_143@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Proceedings of NIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_143",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_144@0",
            "content": "Yunli Wang, Yu Wu, Lili Mou, Zhoujun Li, Wenhan Chao, Harnessing pre-trained neural networks with rules for formality style transfer, 2019, Proceedings of EMNLP-IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_144",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_145@0",
            "content": "Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, Transformers: State-of-theart natural language processing, 2020, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_145",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_146@0",
            "content": "Xinyu Xing, Xiaosheng Fan, Xiaojun Wan, Automatic generation of citation texts in scholarly papers: A pilot study, 2020, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_146",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_147@0",
            "content": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, Pegasus: Pre-training with extracted gap-sentences for abstractive summarization, 2020, Proceedings of ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_147",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "124-ARR_v1_148@0",
            "content": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston, Personalizing dialogue agents: I have a dog, do you have pets too?, 2018, Proceedings of ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "124-ARR_v1_148",
            "start": 0,
            "end": 178,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "124-ARR_v1_0",
            "tgt_ix": "124-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_0",
            "tgt_ix": "124-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_1",
            "tgt_ix": "124-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_1",
            "tgt_ix": "124-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_0",
            "tgt_ix": "124-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_2",
            "tgt_ix": "124-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_4",
            "tgt_ix": "124-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_5",
            "tgt_ix": "124-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_7",
            "tgt_ix": "124-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_8",
            "tgt_ix": "124-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_3",
            "tgt_ix": "124-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_3",
            "tgt_ix": "124-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_3",
            "tgt_ix": "124-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_3",
            "tgt_ix": "124-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_3",
            "tgt_ix": "124-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_3",
            "tgt_ix": "124-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_3",
            "tgt_ix": "124-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_0",
            "tgt_ix": "124-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_9",
            "tgt_ix": "124-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_10",
            "tgt_ix": "124-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_10",
            "tgt_ix": "124-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_10",
            "tgt_ix": "124-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_11",
            "tgt_ix": "124-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_12",
            "tgt_ix": "124-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_12",
            "tgt_ix": "124-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_10",
            "tgt_ix": "124-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_13",
            "tgt_ix": "124-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_15",
            "tgt_ix": "124-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_14",
            "tgt_ix": "124-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_14",
            "tgt_ix": "124-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_14",
            "tgt_ix": "124-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_10",
            "tgt_ix": "124-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_16",
            "tgt_ix": "124-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_18",
            "tgt_ix": "124-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_19",
            "tgt_ix": "124-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_20",
            "tgt_ix": "124-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_21",
            "tgt_ix": "124-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_17",
            "tgt_ix": "124-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_17",
            "tgt_ix": "124-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_17",
            "tgt_ix": "124-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_17",
            "tgt_ix": "124-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_17",
            "tgt_ix": "124-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_17",
            "tgt_ix": "124-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_0",
            "tgt_ix": "124-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_22",
            "tgt_ix": "124-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_23",
            "tgt_ix": "124-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_23",
            "tgt_ix": "124-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_0",
            "tgt_ix": "124-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_24",
            "tgt_ix": "124-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_26",
            "tgt_ix": "124-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_27",
            "tgt_ix": "124-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_28",
            "tgt_ix": "124-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_29",
            "tgt_ix": "124-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_30",
            "tgt_ix": "124-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_25",
            "tgt_ix": "124-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_25",
            "tgt_ix": "124-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_25",
            "tgt_ix": "124-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_25",
            "tgt_ix": "124-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_25",
            "tgt_ix": "124-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_25",
            "tgt_ix": "124-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_25",
            "tgt_ix": "124-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_0",
            "tgt_ix": "124-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_31",
            "tgt_ix": "124-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_32",
            "tgt_ix": "124-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_32",
            "tgt_ix": "124-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_33",
            "tgt_ix": "124-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_33",
            "tgt_ix": "124-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_33",
            "tgt_ix": "124-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_32",
            "tgt_ix": "124-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_35",
            "tgt_ix": "124-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_36",
            "tgt_ix": "124-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_36",
            "tgt_ix": "124-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_32",
            "tgt_ix": "124-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_37",
            "tgt_ix": "124-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_38",
            "tgt_ix": "124-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_38",
            "tgt_ix": "124-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_32",
            "tgt_ix": "124-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_41",
            "tgt_ix": "124-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_43",
            "tgt_ix": "124-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_44",
            "tgt_ix": "124-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_45",
            "tgt_ix": "124-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_42",
            "tgt_ix": "124-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_47",
            "tgt_ix": "124-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_48",
            "tgt_ix": "124-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_46",
            "tgt_ix": "124-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_50",
            "tgt_ix": "124-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_49",
            "tgt_ix": "124-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_32",
            "tgt_ix": "124-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_51",
            "tgt_ix": "124-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_53",
            "tgt_ix": "124-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_52",
            "tgt_ix": "124-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_52",
            "tgt_ix": "124-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_52",
            "tgt_ix": "124-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_0",
            "tgt_ix": "124-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_54",
            "tgt_ix": "124-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_56",
            "tgt_ix": "124-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_57",
            "tgt_ix": "124-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_55",
            "tgt_ix": "124-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_55",
            "tgt_ix": "124-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_55",
            "tgt_ix": "124-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_55",
            "tgt_ix": "124-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_0",
            "tgt_ix": "124-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_58",
            "tgt_ix": "124-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_61",
            "tgt_ix": "124-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_62",
            "tgt_ix": "124-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_63",
            "tgt_ix": "124-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_64",
            "tgt_ix": "124-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_65",
            "tgt_ix": "124-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_66",
            "tgt_ix": "124-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_67",
            "tgt_ix": "124-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_68",
            "tgt_ix": "124-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_69",
            "tgt_ix": "124-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_60",
            "tgt_ix": "124-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_70",
            "tgt_ix": "124-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_72",
            "tgt_ix": "124-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_71",
            "tgt_ix": "124-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_73",
            "tgt_ix": "124-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_74",
            "tgt_ix": "124-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_76",
            "tgt_ix": "124-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_77",
            "tgt_ix": "124-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_78",
            "tgt_ix": "124-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_80",
            "tgt_ix": "124-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_81",
            "tgt_ix": "124-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_79",
            "tgt_ix": "124-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_83",
            "tgt_ix": "124-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_84",
            "tgt_ix": "124-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_82",
            "tgt_ix": "124-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_85",
            "tgt_ix": "124-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_87",
            "tgt_ix": "124-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_88",
            "tgt_ix": "124-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_86",
            "tgt_ix": "124-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_89",
            "tgt_ix": "124-ARR_v1_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_91",
            "tgt_ix": "124-ARR_v1_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_90",
            "tgt_ix": "124-ARR_v1_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "124-ARR_v1_0",
            "tgt_ix": "124-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_1",
            "tgt_ix": "124-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_2",
            "tgt_ix": "124-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_2",
            "tgt_ix": "124-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_2",
            "tgt_ix": "124-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_2",
            "tgt_ix": "124-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_2",
            "tgt_ix": "124-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_2",
            "tgt_ix": "124-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_2",
            "tgt_ix": "124-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_3",
            "tgt_ix": "124-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_4",
            "tgt_ix": "124-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_4",
            "tgt_ix": "124-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_4",
            "tgt_ix": "124-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_4",
            "tgt_ix": "124-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_4",
            "tgt_ix": "124-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_4",
            "tgt_ix": "124-ARR_v1_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_4",
            "tgt_ix": "124-ARR_v1_4@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_5",
            "tgt_ix": "124-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_5",
            "tgt_ix": "124-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_5",
            "tgt_ix": "124-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_5",
            "tgt_ix": "124-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_5",
            "tgt_ix": "124-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_6",
            "tgt_ix": "124-ARR_v1_6@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_7",
            "tgt_ix": "124-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_8",
            "tgt_ix": "124-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_8",
            "tgt_ix": "124-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_8",
            "tgt_ix": "124-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_9",
            "tgt_ix": "124-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_10",
            "tgt_ix": "124-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_11",
            "tgt_ix": "124-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_11",
            "tgt_ix": "124-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_11",
            "tgt_ix": "124-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_11",
            "tgt_ix": "124-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_12",
            "tgt_ix": "124-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_13",
            "tgt_ix": "124-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_13",
            "tgt_ix": "124-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_13",
            "tgt_ix": "124-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_13",
            "tgt_ix": "124-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_13",
            "tgt_ix": "124-ARR_v1_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_14",
            "tgt_ix": "124-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_15",
            "tgt_ix": "124-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_15",
            "tgt_ix": "124-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_15",
            "tgt_ix": "124-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_15",
            "tgt_ix": "124-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_15",
            "tgt_ix": "124-ARR_v1_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_15",
            "tgt_ix": "124-ARR_v1_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_16",
            "tgt_ix": "124-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_16",
            "tgt_ix": "124-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_16",
            "tgt_ix": "124-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_16",
            "tgt_ix": "124-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_17",
            "tgt_ix": "124-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_18",
            "tgt_ix": "124-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_19",
            "tgt_ix": "124-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_19",
            "tgt_ix": "124-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_19",
            "tgt_ix": "124-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_19",
            "tgt_ix": "124-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_19",
            "tgt_ix": "124-ARR_v1_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_19",
            "tgt_ix": "124-ARR_v1_19@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_20",
            "tgt_ix": "124-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_20",
            "tgt_ix": "124-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_20",
            "tgt_ix": "124-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_20",
            "tgt_ix": "124-ARR_v1_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_20",
            "tgt_ix": "124-ARR_v1_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_20",
            "tgt_ix": "124-ARR_v1_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_20",
            "tgt_ix": "124-ARR_v1_20@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_21",
            "tgt_ix": "124-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_21",
            "tgt_ix": "124-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_21",
            "tgt_ix": "124-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_21",
            "tgt_ix": "124-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_21",
            "tgt_ix": "124-ARR_v1_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_21",
            "tgt_ix": "124-ARR_v1_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_22",
            "tgt_ix": "124-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_23",
            "tgt_ix": "124-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_24",
            "tgt_ix": "124-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_24",
            "tgt_ix": "124-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_24",
            "tgt_ix": "124-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_25",
            "tgt_ix": "124-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_26",
            "tgt_ix": "124-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_26",
            "tgt_ix": "124-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_26",
            "tgt_ix": "124-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_27",
            "tgt_ix": "124-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_27",
            "tgt_ix": "124-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_27",
            "tgt_ix": "124-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_27",
            "tgt_ix": "124-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_27",
            "tgt_ix": "124-ARR_v1_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_28",
            "tgt_ix": "124-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_28",
            "tgt_ix": "124-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_28",
            "tgt_ix": "124-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_28",
            "tgt_ix": "124-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_28",
            "tgt_ix": "124-ARR_v1_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_29",
            "tgt_ix": "124-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_30",
            "tgt_ix": "124-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_30",
            "tgt_ix": "124-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_30",
            "tgt_ix": "124-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_30",
            "tgt_ix": "124-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_30",
            "tgt_ix": "124-ARR_v1_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_30",
            "tgt_ix": "124-ARR_v1_30@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_30",
            "tgt_ix": "124-ARR_v1_30@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_30",
            "tgt_ix": "124-ARR_v1_30@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_30",
            "tgt_ix": "124-ARR_v1_30@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_31",
            "tgt_ix": "124-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_31",
            "tgt_ix": "124-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_31",
            "tgt_ix": "124-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_31",
            "tgt_ix": "124-ARR_v1_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_31",
            "tgt_ix": "124-ARR_v1_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_31",
            "tgt_ix": "124-ARR_v1_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_32",
            "tgt_ix": "124-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_33",
            "tgt_ix": "124-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_34@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_34@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_34@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_34@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_34",
            "tgt_ix": "124-ARR_v1_34@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_35",
            "tgt_ix": "124-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_35",
            "tgt_ix": "124-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_35",
            "tgt_ix": "124-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_35",
            "tgt_ix": "124-ARR_v1_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_35",
            "tgt_ix": "124-ARR_v1_35@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_35",
            "tgt_ix": "124-ARR_v1_35@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_35",
            "tgt_ix": "124-ARR_v1_35@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_36",
            "tgt_ix": "124-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_37",
            "tgt_ix": "124-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_37",
            "tgt_ix": "124-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_37",
            "tgt_ix": "124-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_37",
            "tgt_ix": "124-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_37",
            "tgt_ix": "124-ARR_v1_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_38",
            "tgt_ix": "124-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_39@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_39@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_39@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_39@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_39",
            "tgt_ix": "124-ARR_v1_39@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_40",
            "tgt_ix": "124-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_41",
            "tgt_ix": "124-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_42",
            "tgt_ix": "124-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_42",
            "tgt_ix": "124-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_42",
            "tgt_ix": "124-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_43",
            "tgt_ix": "124-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_44",
            "tgt_ix": "124-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_45",
            "tgt_ix": "124-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_46",
            "tgt_ix": "124-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_47",
            "tgt_ix": "124-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_48",
            "tgt_ix": "124-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_49",
            "tgt_ix": "124-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_49",
            "tgt_ix": "124-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_50",
            "tgt_ix": "124-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_50",
            "tgt_ix": "124-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_50",
            "tgt_ix": "124-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_50",
            "tgt_ix": "124-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_51",
            "tgt_ix": "124-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_51",
            "tgt_ix": "124-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_51",
            "tgt_ix": "124-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_51",
            "tgt_ix": "124-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_51",
            "tgt_ix": "124-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_51",
            "tgt_ix": "124-ARR_v1_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_51",
            "tgt_ix": "124-ARR_v1_51@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_51",
            "tgt_ix": "124-ARR_v1_51@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_51",
            "tgt_ix": "124-ARR_v1_51@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_52",
            "tgt_ix": "124-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_53",
            "tgt_ix": "124-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_53",
            "tgt_ix": "124-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_54",
            "tgt_ix": "124-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_54",
            "tgt_ix": "124-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_54",
            "tgt_ix": "124-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_54",
            "tgt_ix": "124-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_54",
            "tgt_ix": "124-ARR_v1_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_55",
            "tgt_ix": "124-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_56",
            "tgt_ix": "124-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_56",
            "tgt_ix": "124-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_56",
            "tgt_ix": "124-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_57",
            "tgt_ix": "124-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_57",
            "tgt_ix": "124-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_57",
            "tgt_ix": "124-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_58",
            "tgt_ix": "124-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_58",
            "tgt_ix": "124-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_58",
            "tgt_ix": "124-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_58",
            "tgt_ix": "124-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_59",
            "tgt_ix": "124-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_60",
            "tgt_ix": "124-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_60",
            "tgt_ix": "124-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_60",
            "tgt_ix": "124-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_61",
            "tgt_ix": "124-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_62",
            "tgt_ix": "124-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_63",
            "tgt_ix": "124-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_64",
            "tgt_ix": "124-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_65",
            "tgt_ix": "124-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_66",
            "tgt_ix": "124-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_67",
            "tgt_ix": "124-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_68",
            "tgt_ix": "124-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_69",
            "tgt_ix": "124-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_70",
            "tgt_ix": "124-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_71",
            "tgt_ix": "124-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_72",
            "tgt_ix": "124-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_72",
            "tgt_ix": "124-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_72",
            "tgt_ix": "124-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_72",
            "tgt_ix": "124-ARR_v1_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_72",
            "tgt_ix": "124-ARR_v1_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_72",
            "tgt_ix": "124-ARR_v1_72@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_73",
            "tgt_ix": "124-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_73",
            "tgt_ix": "124-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_73",
            "tgt_ix": "124-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_73",
            "tgt_ix": "124-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_73",
            "tgt_ix": "124-ARR_v1_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_74",
            "tgt_ix": "124-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_74",
            "tgt_ix": "124-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_74",
            "tgt_ix": "124-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_74",
            "tgt_ix": "124-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_74",
            "tgt_ix": "124-ARR_v1_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_75",
            "tgt_ix": "124-ARR_v1_75@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_76",
            "tgt_ix": "124-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_76",
            "tgt_ix": "124-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_76",
            "tgt_ix": "124-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_76",
            "tgt_ix": "124-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_76",
            "tgt_ix": "124-ARR_v1_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_76",
            "tgt_ix": "124-ARR_v1_76@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_76",
            "tgt_ix": "124-ARR_v1_76@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_76",
            "tgt_ix": "124-ARR_v1_76@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_77",
            "tgt_ix": "124-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_77",
            "tgt_ix": "124-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_78",
            "tgt_ix": "124-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_79",
            "tgt_ix": "124-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_79",
            "tgt_ix": "124-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_79",
            "tgt_ix": "124-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_79",
            "tgt_ix": "124-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_79",
            "tgt_ix": "124-ARR_v1_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_79",
            "tgt_ix": "124-ARR_v1_79@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_79",
            "tgt_ix": "124-ARR_v1_79@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_80",
            "tgt_ix": "124-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_80",
            "tgt_ix": "124-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_80",
            "tgt_ix": "124-ARR_v1_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_80",
            "tgt_ix": "124-ARR_v1_80@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_81",
            "tgt_ix": "124-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_81",
            "tgt_ix": "124-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_82",
            "tgt_ix": "124-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_82",
            "tgt_ix": "124-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_83",
            "tgt_ix": "124-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_84",
            "tgt_ix": "124-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_84",
            "tgt_ix": "124-ARR_v1_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_84",
            "tgt_ix": "124-ARR_v1_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_84",
            "tgt_ix": "124-ARR_v1_84@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_84",
            "tgt_ix": "124-ARR_v1_84@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_85",
            "tgt_ix": "124-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_85",
            "tgt_ix": "124-ARR_v1_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_85",
            "tgt_ix": "124-ARR_v1_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_85",
            "tgt_ix": "124-ARR_v1_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_86",
            "tgt_ix": "124-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_86",
            "tgt_ix": "124-ARR_v1_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_86",
            "tgt_ix": "124-ARR_v1_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_87",
            "tgt_ix": "124-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_87",
            "tgt_ix": "124-ARR_v1_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_87",
            "tgt_ix": "124-ARR_v1_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_87",
            "tgt_ix": "124-ARR_v1_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_88",
            "tgt_ix": "124-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_88",
            "tgt_ix": "124-ARR_v1_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_88",
            "tgt_ix": "124-ARR_v1_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_88",
            "tgt_ix": "124-ARR_v1_88@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_88",
            "tgt_ix": "124-ARR_v1_88@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_89",
            "tgt_ix": "124-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_89",
            "tgt_ix": "124-ARR_v1_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_90",
            "tgt_ix": "124-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_90",
            "tgt_ix": "124-ARR_v1_90@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_91",
            "tgt_ix": "124-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_91",
            "tgt_ix": "124-ARR_v1_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_92",
            "tgt_ix": "124-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_92",
            "tgt_ix": "124-ARR_v1_92@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_93",
            "tgt_ix": "124-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_94",
            "tgt_ix": "124-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_95",
            "tgt_ix": "124-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_96",
            "tgt_ix": "124-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_97",
            "tgt_ix": "124-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_98",
            "tgt_ix": "124-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_99",
            "tgt_ix": "124-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_100",
            "tgt_ix": "124-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_101",
            "tgt_ix": "124-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_102",
            "tgt_ix": "124-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_103",
            "tgt_ix": "124-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_104",
            "tgt_ix": "124-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_105",
            "tgt_ix": "124-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_106",
            "tgt_ix": "124-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_107",
            "tgt_ix": "124-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_108",
            "tgt_ix": "124-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_109",
            "tgt_ix": "124-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_110",
            "tgt_ix": "124-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_111",
            "tgt_ix": "124-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_112",
            "tgt_ix": "124-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_113",
            "tgt_ix": "124-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_114",
            "tgt_ix": "124-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_115",
            "tgt_ix": "124-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_116",
            "tgt_ix": "124-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_117",
            "tgt_ix": "124-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_118",
            "tgt_ix": "124-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_119",
            "tgt_ix": "124-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_120",
            "tgt_ix": "124-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_121",
            "tgt_ix": "124-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_122",
            "tgt_ix": "124-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_123",
            "tgt_ix": "124-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_124",
            "tgt_ix": "124-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_125",
            "tgt_ix": "124-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_126",
            "tgt_ix": "124-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_127",
            "tgt_ix": "124-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_128",
            "tgt_ix": "124-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_129",
            "tgt_ix": "124-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_130",
            "tgt_ix": "124-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_131",
            "tgt_ix": "124-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_132",
            "tgt_ix": "124-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_133",
            "tgt_ix": "124-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_134",
            "tgt_ix": "124-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_135",
            "tgt_ix": "124-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_136",
            "tgt_ix": "124-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_137",
            "tgt_ix": "124-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_138",
            "tgt_ix": "124-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_139",
            "tgt_ix": "124-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_140",
            "tgt_ix": "124-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_141",
            "tgt_ix": "124-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_142",
            "tgt_ix": "124-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_143",
            "tgt_ix": "124-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_144",
            "tgt_ix": "124-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_145",
            "tgt_ix": "124-ARR_v1_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_146",
            "tgt_ix": "124-ARR_v1_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_147",
            "tgt_ix": "124-ARR_v1_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "124-ARR_v1_148",
            "tgt_ix": "124-ARR_v1_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1737,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "124-ARR",
        "version": 1
    }
}