{'doc_id': '2001.04063', 'context': 'In this paper , we present a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale dataset (160GB)  respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.', 'domain': 'arxiv', 'before_edit': 'In this paper , we present a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism.', 'after_edit': 'This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '2001.04063', 'context': 'In this paper , we present a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale dataset (160GB)  respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.', 'domain': 'arxiv', 'before_edit': ' Instead of the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time step.', 'after_edit': ' Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time step.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'fluency']}
{'doc_id': '2001.04063', 'context': 'In this paper , we present a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale dataset (160GB)  respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.', 'domain': 'arxiv', 'before_edit': ' Instead of the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time step.', 'after_edit': ' Instead of the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step.', 'label': 'fluency', 'raw_intents': ['fluency', 'coherence', 'fluency']}
{'doc_id': '2001.04063', 'context': 'In this paper , we present a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale dataset (160GB)  respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.', 'domain': 'arxiv', 'before_edit': ' We pre-train ProphetNet using a base scale dataset (16GB) and a large scale dataset (160GB)  respectively.', 'after_edit': ' We pre-train ProphetNet using a base scale dataset (16GB) and a large-scale dataset (160GB)  respectively.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2001.04063', 'context': 'In this paper , we present a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale dataset (160GB)  respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.', 'domain': 'arxiv', 'before_edit': ' We pre-train ProphetNet using a base scale dataset (16GB) and a large scale dataset (160GB)  respectively.', 'after_edit': ' We pre-train ProphetNet using a base scale dataset (16GB) and a large scale dataset (160GB) , respectively.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2001.07676', 'context': 'Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases which help the language model understand the given task. Theses phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, regular supervised training is performed on the resulting training set. On several tasks , we show that PET outperforms both supervised training and unsupervised approaches in low-resource settings by a large margin.', 'domain': 'arxiv', 'before_edit': ' While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases which help the language model understand the given task.', 'after_edit': ' While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2001.07676', 'context': 'Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases which help the language model understand the given task. Theses phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, regular supervised training is performed on the resulting training set. On several tasks , we show that PET outperforms both supervised training and unsupervised approaches in low-resource settings by a large margin.', 'domain': 'arxiv', 'before_edit': ' Theses phrases are then used to assign soft labels to a large set of unlabeled examples.', 'after_edit': ' These phrases are then used to assign soft labels to a large set of unlabeled examples.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2001.07676', 'context': 'Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases which help the language model understand the given task. Theses phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, regular supervised training is performed on the resulting training set. On several tasks , we show that PET outperforms both supervised training and unsupervised approaches in low-resource settings by a large margin.', 'domain': 'arxiv', 'before_edit': ' On several tasks , we show that PET outperforms both supervised training and unsupervised approaches in low-resource settings by a large margin.', 'after_edit': ' For several tasks and languages, PET outperforms both supervised training and unsupervised approaches in low-resource settings by a large margin.', 'label': 'clarity', 'raw_intents': ['clarity', 'meaning-changed', 'coherence']}
{'doc_id': '2004.12765', 'context': 'Automatic humor detection has interesting use cases in modern technologies, such as chatbots and virtual assistants. Based on the general linguistic structure of humor, in this paper, we propose a novel approach for detecting humor in short texts by using BERT sentence embedding . Our proposed method uses BERT to generate embeddings for sentences of a given text and uses these embeddings as inputs for parallel lines of hidden layers in a neural network. These lines are finally concatenated to predict the target value. For evaluation purposes, we created a new dataset for humor detection consisting of 200k formal short texts (100k positive and 100k negative). Experimental results show that our proposed method can determine humor in short texts with accuracy and an F1-score of 98.2 percent. Our 8-layer model with 110M parameters outperforms all baseline models with a large margin, showing the importance of utilizing linguistic structure  in machine learning models.', 'domain': 'arxiv', 'before_edit': ' Based on the general linguistic structure of humor, in this paper, we propose a novel approach for detecting humor in short texts by using BERT sentence embedding .', 'after_edit': ' In this paper, we propose a novel approach for detecting humor in short texts by using BERT sentence embedding .', 'label': 'coherence', 'raw_intents': ['coherence', 'clarity', 'coherence']}
{'doc_id': '2004.12765', 'context': 'Automatic humor detection has interesting use cases in modern technologies, such as chatbots and virtual assistants. Based on the general linguistic structure of humor, in this paper, we propose a novel approach for detecting humor in short texts by using BERT sentence embedding . Our proposed method uses BERT to generate embeddings for sentences of a given text and uses these embeddings as inputs for parallel lines of hidden layers in a neural network. These lines are finally concatenated to predict the target value. For evaluation purposes, we created a new dataset for humor detection consisting of 200k formal short texts (100k positive and 100k negative). Experimental results show that our proposed method can determine humor in short texts with accuracy and an F1-score of 98.2 percent. Our 8-layer model with 110M parameters outperforms all baseline models with a large margin, showing the importance of utilizing linguistic structure  in machine learning models.', 'domain': 'arxiv', 'before_edit': ' Based on the general linguistic structure of humor, in this paper, we propose a novel approach for detecting humor in short texts by using BERT sentence embedding .', 'after_edit': ' Based on the general linguistic structure of humor, in this paper, we propose a novel approach for detecting humor in short texts based on the general linguistic structure of humor .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'meaning-changed']}
{'doc_id': '2004.12765', 'context': 'Automatic humor detection has interesting use cases in modern technologies, such as chatbots and virtual assistants. Based on the general linguistic structure of humor, in this paper, we propose a novel approach for detecting humor in short texts by using BERT sentence embedding . Our proposed method uses BERT to generate embeddings for sentences of a given text and uses these embeddings as inputs for parallel lines of hidden layers in a neural network. These lines are finally concatenated to predict the target value. For evaluation purposes, we created a new dataset for humor detection consisting of 200k formal short texts (100k positive and 100k negative). Experimental results show that our proposed method can determine humor in short texts with accuracy and an F1-score of 98.2 percent. Our 8-layer model with 110M parameters outperforms all baseline models with a large margin, showing the importance of utilizing linguistic structure  in machine learning models.', 'domain': 'arxiv', 'before_edit': ' Our proposed method uses BERT to generate embeddings for sentences of a given text and uses these embeddings as inputs for parallel lines of hidden layers in a neural network.', 'after_edit': ' Our proposed method uses BERT to generate embeddings for sentences of a given text and uses these embeddings as inputs of parallel lines of hidden layers in a neural network.', 'label': 'fluency', 'raw_intents': ['fluency', 'clarity', 'fluency']}
{'doc_id': '2004.12765', 'context': 'Automatic humor detection has interesting use cases in modern technologies, such as chatbots and virtual assistants. Based on the general linguistic structure of humor, in this paper, we propose a novel approach for detecting humor in short texts by using BERT sentence embedding . Our proposed method uses BERT to generate embeddings for sentences of a given text and uses these embeddings as inputs for parallel lines of hidden layers in a neural network. These lines are finally concatenated to predict the target value. For evaluation purposes, we created a new dataset for humor detection consisting of 200k formal short texts (100k positive and 100k negative). Experimental results show that our proposed method can determine humor in short texts with accuracy and an F1-score of 98.2 percent. Our 8-layer model with 110M parameters outperforms all baseline models with a large margin, showing the importance of utilizing linguistic structure  in machine learning models.', 'domain': 'arxiv', 'before_edit': ' Our 8-layer model with 110M parameters outperforms all baseline models with a large margin, showing the importance of utilizing linguistic structure  in machine learning models.', 'after_edit': ' Our 8-layer model with 110M parameters outperforms the baseline models with a large margin, showing the importance of utilizing linguistic structure  in machine learning models.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2004.12765', 'context': 'Automatic humor detection has interesting use cases in modern technologies, such as chatbots and virtual assistants. Based on the general linguistic structure of humor, in this paper, we propose a novel approach for detecting humor in short texts by using BERT sentence embedding . Our proposed method uses BERT to generate embeddings for sentences of a given text and uses these embeddings as inputs for parallel lines of hidden layers in a neural network. These lines are finally concatenated to predict the target value. For evaluation purposes, we created a new dataset for humor detection consisting of 200k formal short texts (100k positive and 100k negative). Experimental results show that our proposed method can determine humor in short texts with accuracy and an F1-score of 98.2 percent. Our 8-layer model with 110M parameters outperforms all baseline models with a large margin, showing the importance of utilizing linguistic structure  in machine learning models.', 'domain': 'arxiv', 'before_edit': ' Our 8-layer model with 110M parameters outperforms all baseline models with a large margin, showing the importance of utilizing linguistic structure  in machine learning models.', 'after_edit': ' Our 8-layer model with 110M parameters outperforms all baseline models with a large margin, showing the importance of utilizing linguistic structure of texts in machine learning models.', 'label': 'clarity', 'raw_intents': ['clarity', 'coherence', 'clarity']}
{'doc_id': '2004.14519', 'context': 'The Arabic language is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL)  and Relation Extraction (RE). A few multilingual pre-trained models have been proposed and show good performance for Arabic, however, most experiment results are reported on language understanding tasks, such as natural language inference, question answering and sentiment analysis. Their performance on the IE tasks is less known, in particular, the cross-lingual transfer capability from English to Arabic. In this work, we pre-train a Gigaword-based bilingual language model (GigaBERT) to study these two distant languages as well as zero-short transfer learning on the information extraction tasks. Our GigaBERT model can outperform mBERT and XLM-R-base on NER, POS and ARL tasks, with regarding to the per-language and /or zero-transfer performance.We make our pre-trained models publicly available at URL to facilitate the research of this field.', 'domain': None, 'before_edit': 'The Arabic language is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL)  and Relation Extraction (RE).', 'after_edit': 'Arabic is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL)  and Relation Extraction (RE).', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2004.14519', 'context': 'The Arabic language is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL)  and Relation Extraction (RE). A few multilingual pre-trained models have been proposed and show good performance for Arabic, however, most experiment results are reported on language understanding tasks, such as natural language inference, question answering and sentiment analysis. Their performance on the IE tasks is less known, in particular, the cross-lingual transfer capability from English to Arabic. In this work, we pre-train a Gigaword-based bilingual language model (GigaBERT) to study these two distant languages as well as zero-short transfer learning on the information extraction tasks. Our GigaBERT model can outperform mBERT and XLM-R-base on NER, POS and ARL tasks, with regarding to the per-language and /or zero-transfer performance.We make our pre-trained models publicly available at URL to facilitate the research of this field.', 'domain': None, 'before_edit': 'The Arabic language is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL)  and Relation Extraction (RE).', 'after_edit': 'The Arabic language is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL) , and Relation Extraction (RE).', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2004.14519', 'context': 'The Arabic language is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL)  and Relation Extraction (RE). A few multilingual pre-trained models have been proposed and show good performance for Arabic, however, most experiment results are reported on language understanding tasks, such as natural language inference, question answering and sentiment analysis. Their performance on the IE tasks is less known, in particular, the cross-lingual transfer capability from English to Arabic. In this work, we pre-train a Gigaword-based bilingual language model (GigaBERT) to study these two distant languages as well as zero-short transfer learning on the information extraction tasks. Our GigaBERT model can outperform mBERT and XLM-R-base on NER, POS and ARL tasks, with regarding to the per-language and /or zero-transfer performance.We make our pre-trained models publicly available at URL to facilitate the research of this field.', 'domain': None, 'before_edit': ' In this work, we pre-train a Gigaword-based bilingual language model (GigaBERT) to study these two distant languages as well as zero-short transfer learning on the information extraction tasks.', 'after_edit': ' In this work, we pre-train a Gigaword-based bilingual language model (GigaBERT) to study these two distant languages as well as zero-short transfer learning on various IE tasks.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2004.14519', 'context': 'The Arabic language is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL)  and Relation Extraction (RE). A few multilingual pre-trained models have been proposed and show good performance for Arabic, however, most experiment results are reported on language understanding tasks, such as natural language inference, question answering and sentiment analysis. Their performance on the IE tasks is less known, in particular, the cross-lingual transfer capability from English to Arabic. In this work, we pre-train a Gigaword-based bilingual language model (GigaBERT) to study these two distant languages as well as zero-short transfer learning on the information extraction tasks. Our GigaBERT model can outperform mBERT and XLM-R-base on NER, POS and ARL tasks, with regarding to the per-language and /or zero-transfer performance.We make our pre-trained models publicly available at URL to facilitate the research of this field.', 'domain': None, 'before_edit': ' Our GigaBERT model can outperform mBERT and XLM-R-base on NER, POS and ARL tasks, with regarding to the per-language and /or zero-transfer performance.We make our pre-trained models publicly available at URL to facilitate the research of this field.', 'after_edit': ' Our GigaBERT outperforms multilingual BERT and and monolingual AraBERT on these tasks, in both supervised and zero-shot learning settings. our pre-trained models publicly available at URL to facilitate the research of this field.', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2004.14519', 'context': 'The Arabic language is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL)  and Relation Extraction (RE). A few multilingual pre-trained models have been proposed and show good performance for Arabic, however, most experiment results are reported on language understanding tasks, such as natural language inference, question answering and sentiment analysis. Their performance on the IE tasks is less known, in particular, the cross-lingual transfer capability from English to Arabic. In this work, we pre-train a Gigaword-based bilingual language model (GigaBERT) to study these two distant languages as well as zero-short transfer learning on the information extraction tasks. Our GigaBERT model can outperform mBERT and XLM-R-base on NER, POS and ARL tasks, with regarding to the per-language and /or zero-transfer performance.We make our pre-trained models publicly available at URL to facilitate the research of this field.', 'domain': None, 'before_edit': 'We make our pre-trained models publicly available at URL to facilitate the research of this field.', 'after_edit': 'We makeWe have made our pre-trained models publicly available at URL to facilitate the research of this field.', 'label': 'clarity', 'raw_intents': ['coherence', 'clarity', 'clarity']}
{'doc_id': '2004.14519', 'context': 'The Arabic language is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL)  and Relation Extraction (RE). A few multilingual pre-trained models have been proposed and show good performance for Arabic, however, most experiment results are reported on language understanding tasks, such as natural language inference, question answering and sentiment analysis. Their performance on the IE tasks is less known, in particular, the cross-lingual transfer capability from English to Arabic. In this work, we pre-train a Gigaword-based bilingual language model (GigaBERT) to study these two distant languages as well as zero-short transfer learning on the information extraction tasks. Our GigaBERT model can outperform mBERT and XLM-R-base on NER, POS and ARL tasks, with regarding to the per-language and /or zero-transfer performance.We make our pre-trained models publicly available at URL to facilitate the research of this field.', 'domain': None, 'before_edit': 'We make our pre-trained models publicly available at URL to facilitate the research of this field.', 'after_edit': 'We make our pre-trained models publicly available at URL ', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2004.14623', 'context': "In adversarial (challenge) testing, we pose hard generalization tasks in order to gain insights into the solutions found by our models. What properties must a system have in order to succeed at these hard tasks? In this paper, we argue that an essential factor is the ability to form modular representations . Our central contribution is a definition of what it means for a representation to be modular and an experimental method for assessing the extent to which a system's solution is modular in this general sense . Our work is grounded empirically in a new challenge Natural Language Inference dataset designed to assess systems on their ability to reason about entailment and negation. We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.", 'domain': 'arxiv', 'before_edit': 'In adversarial (challenge) testing, we pose hard generalization tasks in order to gain insights into the solutions found by our models.', 'after_edit': 'In adversarial  testing, we pose hard generalization tasks in order to gain insights into the solutions found by our models.', 'label': 'coherence', 'raw_intents': ['style', 'coherence', 'coherence']}
{'doc_id': '2004.14623', 'context': "In adversarial (challenge) testing, we pose hard generalization tasks in order to gain insights into the solutions found by our models. What properties must a system have in order to succeed at these hard tasks? In this paper, we argue that an essential factor is the ability to form modular representations . Our central contribution is a definition of what it means for a representation to be modular and an experimental method for assessing the extent to which a system's solution is modular in this general sense . Our work is grounded empirically in a new challenge Natural Language Inference dataset designed to assess systems on their ability to reason about entailment and negation. We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.", 'domain': 'arxiv', 'before_edit': ' What properties must a system have in order to succeed at these hard tasks? In this paper, we argue that an essential factor is the ability to form modular representations .', 'after_edit': ' What properties must a system have in order to succeed at these hard behavioral tasks? We argue that an essential factor is the ability to form modular representations .', 'label': 'clarity', 'raw_intents': ['others', 'clarity', 'clarity']}
{'doc_id': '2004.14623', 'context': "In adversarial (challenge) testing, we pose hard generalization tasks in order to gain insights into the solutions found by our models. What properties must a system have in order to succeed at these hard tasks? In this paper, we argue that an essential factor is the ability to form modular representations . Our central contribution is a definition of what it means for a representation to be modular and an experimental method for assessing the extent to which a system's solution is modular in this general sense . Our work is grounded empirically in a new challenge Natural Language Inference dataset designed to assess systems on their ability to reason about entailment and negation. We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.", 'domain': 'arxiv', 'before_edit': ' In this paper, we argue that an essential factor is the ability to form modular representations .', 'after_edit': ' In this paper, we argue that an essential factor is modular internal structure .', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2004.14623', 'context': "In adversarial (challenge) testing, we pose hard generalization tasks in order to gain insights into the solutions found by our models. What properties must a system have in order to succeed at these hard tasks? In this paper, we argue that an essential factor is the ability to form modular representations . Our central contribution is a definition of what it means for a representation to be modular and an experimental method for assessing the extent to which a system's solution is modular in this general sense . Our work is grounded empirically in a new challenge Natural Language Inference dataset designed to assess systems on their ability to reason about entailment and negation. We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.", 'domain': 'arxiv', 'before_edit': " Our central contribution is a definition of what it means for a representation to be modular and an experimental method for assessing the extent to which a system's solution is modular in this general sense .", 'after_edit': " Our central contribution is a new experimental method called 'interchange interventions', in which systematic manipulations of model-internal states are related to causal effects on their outputs, thereby allowing us to identify modular structure .", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'meaning-changed']}
{'doc_id': '2004.14623', 'context': "In adversarial (challenge) testing, we pose hard generalization tasks in order to gain insights into the solutions found by our models. What properties must a system have in order to succeed at these hard tasks? In this paper, we argue that an essential factor is the ability to form modular representations . Our central contribution is a definition of what it means for a representation to be modular and an experimental method for assessing the extent to which a system's solution is modular in this general sense . Our work is grounded empirically in a new challenge Natural Language Inference dataset designed to assess systems on their ability to reason about entailment and negation. We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.", 'domain': 'arxiv', 'before_edit': ' We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.', 'after_edit': ' We find that a BERT model  is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'coherence']}
{'doc_id': '2004.14623', 'context': "In adversarial (challenge) testing, we pose hard generalization tasks in order to gain insights into the solutions found by our models. What properties must a system have in order to succeed at these hard tasks? In this paper, we argue that an essential factor is the ability to form modular representations . Our central contribution is a definition of what it means for a representation to be modular and an experimental method for assessing the extent to which a system's solution is modular in this general sense . Our work is grounded empirically in a new challenge Natural Language Inference dataset designed to assess systems on their ability to reason about entailment and negation. We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.", 'domain': 'arxiv', 'before_edit': ' We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.', 'after_edit': ' We find that a BERT model with fine-tuning is strikingly successful at the systematic generalization task we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.', 'label': 'clarity', 'raw_intents': ['clarity', 'meaning-changed', 'clarity']}
{'doc_id': '2004.14623', 'context': "In adversarial (challenge) testing, we pose hard generalization tasks in order to gain insights into the solutions found by our models. What properties must a system have in order to succeed at these hard tasks? In this paper, we argue that an essential factor is the ability to form modular representations . Our central contribution is a definition of what it means for a representation to be modular and an experimental method for assessing the extent to which a system's solution is modular in this general sense . Our work is grounded empirically in a new challenge Natural Language Inference dataset designed to assess systems on their ability to reason about entailment and negation. We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.", 'domain': 'arxiv', 'before_edit': ' We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations help us to understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.', 'after_edit': ' We find that a BERT model with fine-tuning is strikingly successful at the hard generalization tasks we pose using this dataset, and our active manipulations of model-internal vectors help us understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': 'For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers .', 'after_edit': 'In the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers .', 'label': 'coherence', 'raw_intents': ['coherence', 'fluency', 'coherence']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': 'For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers .', 'after_edit': 'For the automatic evaluation of generative question answering (GenQA ) systems, it is essential to assess the correctness of the generated answers .', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': 'For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers .', 'after_edit': 'For the automatic evaluation of Generative Question Answering (genQA ) systems, it is difficult to assess the correctness of the generated answers .', 'label': 'clarity', 'raw_intents': ['style', 'clarity', 'clarity']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': 'For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments .', 'after_edit': 'For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of generated answers due to the free-form of the answer .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': ' Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness.', 'after_edit': ' Moreover, there is a lack of benchmark datasets to evaluate the suitability of existing metrics in terms of the correctness.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': ' Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness.', 'after_edit': ' Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of  correctness.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': ' To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets.', 'after_edit': ' To study a better metric for GenQA, we first create high-quality human judgments of correctness on two standard genQA datasets.', 'label': 'clarity', 'raw_intents': ['clarity', 'coherence', 'clarity']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': ' To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets.', 'after_edit': ' To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard GenQA datasets.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'clarity']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': ' Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments.', 'after_edit': ' Using our human-evaluation datasets, we show that widely used n-gram similarity  do not correlate with human judgments.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': ' Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments.', 'after_edit': ' Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity metrics do not correlate with human judgments.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': " To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth .", 'after_edit': " To alleviate this problem, we propose a new metric for evaluating the correctness of GenQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth .", 'label': 'fluency', 'raw_intents': ['fluency', 'clarity', 'fluency']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': " To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth .", 'after_edit': " To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, our new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth .", 'label': 'clarity', 'raw_intents': ['clarity', 'fluency', 'clarity']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': " To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth .", 'after_edit': " To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights to each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth .", 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'coherence']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': " To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth .", 'after_edit': " To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the human judge's ground-truth .", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': " To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth .", 'after_edit': ' To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the reference answer .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.00192', 'context': "For the automatic evaluation of Generative Question Answering (genQA ) systems, it is essential to assess the correctness of the generated answers . However, n-gram similarity metrics, which are widely used to compare generated texts and references, are prone to misjudge fact-based assessments . Moreover, there is a lack of benchmark datasets to measure the quality of metrics in terms of the correctness. To study a better metric for genQA, we collect high-quality human judgments of correctness on two standard genQA datasets. Using our human-evaluation datasets, we show that existing metrics based on n-gram similarity  do not correlate with human judgments. To alleviate this problem, we propose a new metric for evaluating the correctness of genQA . Specifically, the new metric assigns different weights on each token via keyphrase prediction, thereby judging whether a predicted answer sentence captures the key meaning of the human judge's ground-truth . Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .", 'domain': 'arxiv', 'before_edit': ' Our proposed metric shows a significantly higher correlation with human judgment than widely used existing metrics .', 'after_edit': ' Our proposed metric shows a significantly higher correlation with human judgments than existing metrics in various datasets .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'Vision, as a central component of human perception, plays a fundamental role in shaping natural language . To better understand how text models are connected to our visual perceptions , we propose a method for examining the similarities between neural representations extracted from words in text and objects in images . Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain . We find that representations from models trained on purely textual data, such as BERT, can be nontrivially mapped to those of a vision model. Such mappings generalize to object categories that were never seen by the probe during training, unlike mappings learned from permuted or random representations . Moreover, we find that the context surrounding objects in sentences greatly impacts performance. Finally, we show that humans significantly outperform all examined models , suggesting considerable room for improvement in representation learning and grounding .', 'domain': 'arxiv', 'before_edit': 'Vision, as a central component of human perception, plays a fundamental role in shaping natural language . To better understand how text models are connected to our visual perceptions , we propose a method for examining the similarities between neural representations extracted from words in text and objects in images .', 'after_edit': 'While large-scale language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work , we propose a method for examining the similarities between neural representations extracted from words in text and objects in images .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'meaning-changed']}
{'doc_id': '2005.00619', 'context': 'Vision, as a central component of human perception, plays a fundamental role in shaping natural language . To better understand how text models are connected to our visual perceptions , we propose a method for examining the similarities between neural representations extracted from words in text and objects in images . Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain . We find that representations from models trained on purely textual data, such as BERT, can be nontrivially mapped to those of a vision model. Such mappings generalize to object categories that were never seen by the probe during training, unlike mappings learned from permuted or random representations . Moreover, we find that the context surrounding objects in sentences greatly impacts performance. Finally, we show that humans significantly outperform all examined models , suggesting considerable room for improvement in representation learning and grounding .', 'domain': 'arxiv', 'before_edit': ' To better understand how text models are connected to our visual perceptions , we propose a method for examining the similarities between neural representations extracted from words in text and objects in images .', 'after_edit': ' To better understand how text models are connected to our visual perceptions , we propose a method for characterizing how language representations of concrete nouns relate to the physical appearance of the objects they refer to .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'clarity']}
{'doc_id': '2005.00619', 'context': 'Vision, as a central component of human perception, plays a fundamental role in shaping natural language . To better understand how text models are connected to our visual perceptions , we propose a method for examining the similarities between neural representations extracted from words in text and objects in images . Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain . We find that representations from models trained on purely textual data, such as BERT, can be nontrivially mapped to those of a vision model. Such mappings generalize to object categories that were never seen by the probe during training, unlike mappings learned from permuted or random representations . Moreover, we find that the context surrounding objects in sentences greatly impacts performance. Finally, we show that humans significantly outperform all examined models , suggesting considerable room for improvement in representation learning and grounding .', 'domain': 'arxiv', 'before_edit': ' Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain .', 'after_edit': ' Our approach uses a  probing model that learns to map language representations of concrete words to the visual domain .', 'label': 'coherence', 'raw_intents': ['fluency', 'coherence', 'coherence']}
{'doc_id': '2005.00619', 'context': 'Vision, as a central component of human perception, plays a fundamental role in shaping natural language . To better understand how text models are connected to our visual perceptions , we propose a method for examining the similarities between neural representations extracted from words in text and objects in images . Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain . We find that representations from models trained on purely textual data, such as BERT, can be nontrivially mapped to those of a vision model. Such mappings generalize to object categories that were never seen by the probe during training, unlike mappings learned from permuted or random representations . Moreover, we find that the context surrounding objects in sentences greatly impacts performance. Finally, we show that humans significantly outperform all examined models , suggesting considerable room for improvement in representation learning and grounding .', 'domain': 'arxiv', 'before_edit': ' Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain .', 'after_edit': ' Our approach uses a lightweight probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'Vision, as a central component of human perception, plays a fundamental role in shaping natural language . To better understand how text models are connected to our visual perceptions , we propose a method for examining the similarities between neural representations extracted from words in text and objects in images . Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain . We find that representations from models trained on purely textual data, such as BERT, can be nontrivially mapped to those of a vision model. Such mappings generalize to object categories that were never seen by the probe during training, unlike mappings learned from permuted or random representations . Moreover, we find that the context surrounding objects in sentences greatly impacts performance. Finally, we show that humans significantly outperform all examined models , suggesting considerable room for improvement in representation learning and grounding .', 'domain': 'arxiv', 'before_edit': ' Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain . We find that representations from models trained on purely textual data, such as BERT, can be nontrivially mapped to those of a vision model. Such mappings generalize to object categories that were never seen by the probe during training, unlike mappings learned from permuted or random representations .', 'after_edit': ' Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain , finding representations of many language models to be useful in retrieving semantically aligned image patches. In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2005.00619', 'context': 'Vision, as a central component of human perception, plays a fundamental role in shaping natural language . To better understand how text models are connected to our visual perceptions , we propose a method for examining the similarities between neural representations extracted from words in text and objects in images . Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain . We find that representations from models trained on purely textual data, such as BERT, can be nontrivially mapped to those of a vision model. Such mappings generalize to object categories that were never seen by the probe during training, unlike mappings learned from permuted or random representations . Moreover, we find that the context surrounding objects in sentences greatly impacts performance. Finally, we show that humans significantly outperform all examined models , suggesting considerable room for improvement in representation learning and grounding .', 'domain': 'arxiv', 'before_edit': ' Moreover, we find that the context surrounding objects in sentences greatly impacts performance.', 'after_edit': ' ', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'others']}
{'doc_id': '2005.00619', 'context': 'Vision, as a central component of human perception, plays a fundamental role in shaping natural language . To better understand how text models are connected to our visual perceptions , we propose a method for examining the similarities between neural representations extracted from words in text and objects in images . Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain . We find that representations from models trained on purely textual data, such as BERT, can be nontrivially mapped to those of a vision model. Such mappings generalize to object categories that were never seen by the probe during training, unlike mappings learned from permuted or random representations . Moreover, we find that the context surrounding objects in sentences greatly impacts performance. Finally, we show that humans significantly outperform all examined models , suggesting considerable room for improvement in representation learning and grounding .', 'domain': 'arxiv', 'before_edit': ' Finally, we show that humans significantly outperform all examined models , suggesting considerable room for improvement in representation learning and grounding .', 'after_edit': ' Finally, we show that the examined models substantially under-perform humans in retrieval. Altogether, our findings shed new empirical insights on language grounding, suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'label': 'clarity', 'raw_intents': ['clarity', 'meaning-changed', 'clarity']}
{'doc_id': '2005.00619', 'context': 'While large-scale  language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches . In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval  . Altogether, our findings shed new empirical insights on language grounding , suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'domain': 'arxiv', 'before_edit': 'While large-scale  language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations.', 'after_edit': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'coherence', 'meaning-changed']}
{'doc_id': '2005.00619', 'context': 'While large-scale  language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches . In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval  . Altogether, our findings shed new empirical insights on language grounding , suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'domain': 'arxiv', 'before_edit': ' In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to.', 'after_edit': ' In this work, we characterize how contextual representations of concrete nouns  relate to the physical appearance of the objects they refer to.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'While large-scale  language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches . In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval  . Altogether, our findings shed new empirical insights on language grounding , suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'domain': 'arxiv', 'before_edit': ' In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to.', 'after_edit': ' In this work, we propose a method for characterizing how language representations of concrete nouns extracted by trained language models relate to the physical appearance of the objects they refer to.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2005.00619', 'context': 'While large-scale  language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches . In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval  . Altogether, our findings shed new empirical insights on language grounding , suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'domain': 'arxiv', 'before_edit': ' In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to.', 'after_edit': ' In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical properties of the objects they refer to.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'While large-scale  language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches . In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval  . Altogether, our findings shed new empirical insights on language grounding , suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'domain': 'arxiv', 'before_edit': ' Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations.', 'after_edit': ' Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations.', 'label': 'style', 'raw_intents': ['style', 'style', 'style']}
{'doc_id': '2005.00619', 'context': 'While large-scale  language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches . In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval  . Altogether, our findings shed new empirical insights on language grounding , suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'domain': 'arxiv', 'before_edit': ' We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches .', 'after_edit': ' We show that many recent language models yield representations that are useful in retrieving semantically aligned image patches .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'While large-scale  language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches . In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval  . Altogether, our findings shed new empirical insights on language grounding , suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'domain': 'arxiv', 'before_edit': ' We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches . In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval  .', 'after_edit': ' We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches , and explore the role of context in this process. Much weaker results are found in control experiments, attesting the selectivity of the probe. All examined models greatly under-perform humans in retrieval  .', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'While large-scale  language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches . In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval  . Altogether, our findings shed new empirical insights on language grounding , suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'domain': 'arxiv', 'before_edit': ' Finally, we show that the examined models substantially under-perform humans in retrieval  .', 'after_edit': ' Finally, we show that the examined models substantially under-perform humans in retrieval , highlighting substantial room for future progress .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'coherence']}
{'doc_id': '2005.00619', 'context': 'While large-scale  language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns  relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches . In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval  . Altogether, our findings shed new empirical insights on language grounding , suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'domain': 'arxiv', 'before_edit': ' Altogether, our findings shed new empirical insights on language grounding , suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress .', 'after_edit': ' Altogether, our findings shed new empirical insights on language grounding and its materialization in contextual language models .', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we characterize how contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to. Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations. We show that many recent language models yield representations that are useful in retrieving semantically aligned image patches , and explore the role of context in this process. Much weaker results are found in control experiments, attesting the selectivity of the probe. All examined models greatly under-perform humans in retrieval, highlighting substantial room for future progress. Altogether, our findings shed new empirical insights on language grounding and its materialization in contextual language models.', 'domain': 'arxiv', 'before_edit': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations.', 'after_edit': 'The success of large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations.', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '2005.00619', 'context': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we characterize how contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to. Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations. We show that many recent language models yield representations that are useful in retrieving semantically aligned image patches , and explore the role of context in this process. Much weaker results are found in control experiments, attesting the selectivity of the probe. All examined models greatly under-perform humans in retrieval, highlighting substantial room for future progress. Altogether, our findings shed new empirical insights on language grounding and its materialization in contextual language models.', 'domain': 'arxiv', 'before_edit': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations.', 'after_edit': 'While large-scale contextual language models has attracted great interest in probing what is encoded in their representations.', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we characterize how contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to. Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations. We show that many recent language models yield representations that are useful in retrieving semantically aligned image patches , and explore the role of context in this process. Much weaker results are found in control experiments, attesting the selectivity of the probe. All examined models greatly under-perform humans in retrieval, highlighting substantial room for future progress. Altogether, our findings shed new empirical insights on language grounding and its materialization in contextual language models.', 'domain': 'arxiv', 'before_edit': ' In this work, we characterize how contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to.', 'after_edit': ' In this work, we consider a new question: to what extent contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to.', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we characterize how contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to. Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations. We show that many recent language models yield representations that are useful in retrieving semantically aligned image patches , and explore the role of context in this process. Much weaker results are found in control experiments, attesting the selectivity of the probe. All examined models greatly under-perform humans in retrieval, highlighting substantial room for future progress. Altogether, our findings shed new empirical insights on language grounding and its materialization in contextual language models.', 'domain': 'arxiv', 'before_edit': ' In this work, we characterize how contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to. Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations.', 'after_edit': ' In this work, we characterize how contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that examines how effective these language representations are in discerning between different visual representations.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'meaning-changed']}
{'doc_id': '2005.00619', 'context': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we characterize how contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to. Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations. We show that many recent language models yield representations that are useful in retrieving semantically aligned image patches , and explore the role of context in this process. Much weaker results are found in control experiments, attesting the selectivity of the probe. All examined models greatly under-perform humans in retrieval, highlighting substantial room for future progress. Altogether, our findings shed new empirical insights on language grounding and its materialization in contextual language models.', 'domain': 'arxiv', 'before_edit': ' Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations.', 'after_edit': ' Our approach uses a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations.', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we characterize how contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to. Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations. We show that many recent language models yield representations that are useful in retrieving semantically aligned image patches , and explore the role of context in this process. Much weaker results are found in control experiments, attesting the selectivity of the probe. All examined models greatly under-perform humans in retrieval, highlighting substantial room for future progress. Altogether, our findings shed new empirical insights on language grounding and its materialization in contextual language models.', 'domain': 'arxiv', 'before_edit': ' We show that many recent language models yield representations that are useful in retrieving semantically aligned image patches , and explore the role of context in this process.', 'after_edit': ' Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process.', 'label': 'style', 'raw_intents': ['style', 'style', 'style']}
{'doc_id': '2005.00619', 'context': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we characterize how contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to. Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations. We show that many recent language models yield representations that are useful in retrieving semantically aligned image patches , and explore the role of context in this process. Much weaker results are found in control experiments, attesting the selectivity of the probe. All examined models greatly under-perform humans in retrieval, highlighting substantial room for future progress. Altogether, our findings shed new empirical insights on language grounding and its materialization in contextual language models.', 'domain': 'arxiv', 'before_edit': ' Much weaker results are found in control experiments, attesting the selectivity of the probe. All examined models greatly under-perform humans in retrieval, highlighting substantial room for future progress.', 'after_edit': ' Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans in retrieval, highlighting substantial room for future progress.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.00619', 'context': 'While large-scale contextual language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we characterize how contextual representations of concrete nouns extracted by trained language models relate to the physical properties of the objects they refer to. Our approach uses a probing model that examines how effective these language representations are in discerning between different visual representations. We show that many recent language models yield representations that are useful in retrieving semantically aligned image patches , and explore the role of context in this process. Much weaker results are found in control experiments, attesting the selectivity of the probe. All examined models greatly under-perform humans in retrieval, highlighting substantial room for future progress. Altogether, our findings shed new empirical insights on language grounding and its materialization in contextual language models.', 'domain': 'arxiv', 'before_edit': ' All examined models greatly under-perform humans in retrieval, highlighting substantial room for future progress. Altogether, our findings shed new empirical insights on language grounding and its materialization in contextual language models.', 'after_edit': ' All examined models greatly under-perform humans . We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.03954', 'context': "We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user's interests and feedback. To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), which contains multiple sequential dialogs for every pair of a recommendation seeker (user) and a recommender (bot). In each dialog, the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies. Dataset and codes are publicly available at URL", 'domain': 'arxiv', 'before_edit': "We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user's interests and feedback.", 'after_edit': "We propose a new task of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user's interests and feedback.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.03954', 'context': "We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user's interests and feedback. To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), which contains multiple sequential dialogs for every pair of a recommendation seeker (user) and a recommender (bot). In each dialog, the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies. Dataset and codes are publicly available at URL", 'domain': 'arxiv', 'before_edit': "We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user's interests and feedback.", 'after_edit': "We focus on the study of conversational recommendation over multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user's interests and feedback.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.03954', 'context': "We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user's interests and feedback. To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), which contains multiple sequential dialogs for every pair of a recommendation seeker (user) and a recommender (bot). In each dialog, the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies. Dataset and codes are publicly available at URL", 'domain': 'arxiv', 'before_edit': ' To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), which contains multiple sequential dialogs for every pair of a recommendation seeker (user) and a recommender (bot).', 'after_edit': ' To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), which contains multiple sequential dialogs for every pair of a recommendation seeker (user) and a recommender (bot).', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences .', 'after_edit': ' However, it is often overlooked that the decoder obtains only a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences .', 'after_edit': ' However, it is often overlooked that the decoder only has a single view of the source sequences, i.e. , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences .', 'label': 'clarity', 'raw_intents': ['clarity', 'style', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences .', 'after_edit': ' However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer . Although those representations are supposed to be a  global view of source sequences .', 'label': 'coherence', 'raw_intents': ['coherence', 'meaning-changed', 'coherence']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences .', 'after_edit': ' However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a comprehensive, global view of source sequences .', 'label': 'clarity', 'raw_intents': ['coherence', 'clarity', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information .', 'after_edit': ' However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences , such practice keeps the decoders from concrete, fine-grained , local source information .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'fluency']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' Such implementation hinders the decoder from concrete, fine-grained , local source information .', 'after_edit': ' Such implementation hinders the decoder from concrete, fine-grained source information generated by other encoder layers .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers .', 'after_edit': ' In this work, we propose to encourage the decoder to take the full advantage of the multi-level source representations for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers .', 'label': 'clarity', 'raw_intents': ['clarity', 'meaning-changed', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers .', 'after_edit': ' In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding . Concretely, different views of the source sequences are presented to different decoder layers .', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'after_edit': ' In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers and multiple strategies are explored to route the source representations. In particular, the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'label': 'clarity', 'raw_intents': ['clarity', 'meaning-changed', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'after_edit': ' We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on the neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'label': 'fluency', 'raw_intents': ['fluency', 'coherence', 'fluency']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'after_edit': ' We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task , surpassing the previous state-of-the-art architecture on three machine translation datasets.', 'label': 'clarity', 'raw_intents': ['coherence', 'clarity', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoder and the decoder. However, it is often overlooked that the decoder only has a single view of the source sequences, that is , the representations generated by the last encoder layer , which is supposed to be a  global view of source sequences . Such implementation hinders the decoder from concrete, fine-grained , local source information . In this work, we explore to reuse the representations from different encoder layers for layer-wise cross-view decoding , that is, different views of the source sequences are presented to different decoder layers . We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'domain': 'arxiv', 'before_edit': ' We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three machine translation datasets.', 'after_edit': ' We investigate multiple , representative strategies for cross-view coding, of which the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on  neural machine translation task . Especially, GCA surpasses the previous state-of-the-art architecture on three benchmark datasets.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoderand the decoder. However, it is often overlooked that the decoder obtains only a single view of the source sequences, i.e., the representations generated by the last encoder layer . Although those representations are supposed to be a comprehensive, global view of source sequences, such practice keeps the decoders from concrete, fine-grained source information generated by other encoder layers . In this work, we propose to encourage the decoder to take the full advantage of the multi-level source representations for layer-wise cross-view decoding . Concretely, different views of the source sequences are presented to different decoder layers and multiple strategies are explored to route the source representations. In particular, the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on the neural machine translation task, surpassing the previous state-of-the-art architecture on three benchmark datasets .', 'domain': 'arxiv', 'before_edit': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoderand the decoder. However, it is often overlooked that the decoder obtains only a single view of the source sequences, i.e., the representations generated by the last encoder layer .', 'after_edit': 'In sequence-to-sequence learning, the decoder relies on the attention mechanism to efficiently extract information from the encoder. While it is common practice to draw information from only the last encoder layer, recent work has proposed to use representations from different encoder layers for diversified levels of information. Nonetheless, the decoder still obtains only a single view of the source sequences, i.e., the representations generated by the last encoder layer .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoderand the decoder. However, it is often overlooked that the decoder obtains only a single view of the source sequences, i.e., the representations generated by the last encoder layer . Although those representations are supposed to be a comprehensive, global view of source sequences, such practice keeps the decoders from concrete, fine-grained source information generated by other encoder layers . In this work, we propose to encourage the decoder to take the full advantage of the multi-level source representations for layer-wise cross-view decoding . Concretely, different views of the source sequences are presented to different decoder layers and multiple strategies are explored to route the source representations. In particular, the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on the neural machine translation task, surpassing the previous state-of-the-art architecture on three benchmark datasets .', 'domain': 'arxiv', 'before_edit': ' However, it is often overlooked that the decoder obtains only a single view of the source sequences, i.e., the representations generated by the last encoder layer . Although those representations are supposed to be a comprehensive, global view of source sequences, such practice keeps the decoders from concrete, fine-grained source information generated by other encoder layers .', 'after_edit': ' However, it is often overlooked that the decoder obtains only a single view of the source sequences, which might lead to insufficient training of the encoder layer stack due to the hierarchy bypassing problem .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoderand the decoder. However, it is often overlooked that the decoder obtains only a single view of the source sequences, i.e., the representations generated by the last encoder layer . Although those representations are supposed to be a comprehensive, global view of source sequences, such practice keeps the decoders from concrete, fine-grained source information generated by other encoder layers . In this work, we propose to encourage the decoder to take the full advantage of the multi-level source representations for layer-wise cross-view decoding . Concretely, different views of the source sequences are presented to different decoder layers and multiple strategies are explored to route the source representations. In particular, the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on the neural machine translation task, surpassing the previous state-of-the-art architecture on three benchmark datasets .', 'domain': 'arxiv', 'before_edit': ' In this work, we propose to encourage the decoder to take the full advantage of the multi-level source representations for layer-wise cross-view decoding . Concretely, different views of the source sequences are presented to different decoder layers and multiple strategies are explored to route the source representations.', 'after_edit': ' In this work, we propose  layer-wise cross-view decoding . Concretely, different views of the source sequences are presented to different decoder layers and multiple strategies are explored to route the source representations.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning, the attention mechanism has been a great success in bridging the information between the encoderand the decoder. However, it is often overlooked that the decoder obtains only a single view of the source sequences, i.e., the representations generated by the last encoder layer . Although those representations are supposed to be a comprehensive, global view of source sequences, such practice keeps the decoders from concrete, fine-grained source information generated by other encoder layers . In this work, we propose to encourage the decoder to take the full advantage of the multi-level source representations for layer-wise cross-view decoding . Concretely, different views of the source sequences are presented to different decoder layers and multiple strategies are explored to route the source representations. In particular, the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on the neural machine translation task, surpassing the previous state-of-the-art architecture on three benchmark datasets .', 'domain': 'arxiv', 'before_edit': ' In this work, we propose to encourage the decoder to take the full advantage of the multi-level source representations for layer-wise cross-view decoding . Concretely, different views of the source sequences are presented to different decoder layers and multiple strategies are explored to route the source representations. In particular, the granularity consistent attention (GCA) strategy proves the most efficient and effective in the experiments on the neural machine translation task, surpassing the previous state-of-the-art architecture on three benchmark datasets .', 'after_edit': ' In this work, we propose to encourage the decoder to take the full advantage of the multi-level source representations for layer-wise cross-view decoding , where for each decoder layer, together with the representations from the last encoder layer, which serve as a global view, those from other encoder layers are supplemented for a stereoscopic view of the source sequences. Systematic experiments show that we successfully address the hierarchy bypassing problem and substantially improve the performance of sequence-to-sequence learning with deep representations on diverse tasks .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'clarity', 'meaning-changed']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning,  the decoder relies on the attention mechanism to efficiently extract information from the encoder. While it is common practice to draw information from only the last encoder layer, recent work has proposed to use representations from different encoder layers for diversified levels of information. Nonetheless, the decoder still obtains only a single view of the source sequences, which might lead to insufficient training of the encoder layer stack due to the hierarchy bypassing problem. In this work, we propose layer-wise cross-view decoding, where for each decoder layer, together with the representations from the last encoder layer, which serve as a global view, those from other encoder layers are supplemented for a stereoscopic view of the source sequences. Systematic experiments  show that we successfully address the hierarchy bypassing problem and substantially improve the performance of sequence-to-sequence learning with deep representations on diverse tasks  .', 'domain': 'arxiv', 'before_edit': 'In sequence-to-sequence learning,  the decoder relies on the attention mechanism to efficiently extract information from the encoder.', 'after_edit': 'In sequence-to-sequence learning, e.g., natural language generation, the decoder relies on the attention mechanism to efficiently extract information from the encoder.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning,  the decoder relies on the attention mechanism to efficiently extract information from the encoder. While it is common practice to draw information from only the last encoder layer, recent work has proposed to use representations from different encoder layers for diversified levels of information. Nonetheless, the decoder still obtains only a single view of the source sequences, which might lead to insufficient training of the encoder layer stack due to the hierarchy bypassing problem. In this work, we propose layer-wise cross-view decoding, where for each decoder layer, together with the representations from the last encoder layer, which serve as a global view, those from other encoder layers are supplemented for a stereoscopic view of the source sequences. Systematic experiments  show that we successfully address the hierarchy bypassing problem and substantially improve the performance of sequence-to-sequence learning with deep representations on diverse tasks  .', 'domain': 'arxiv', 'before_edit': ' In this work, we propose layer-wise cross-view decoding, where for each decoder layer, together with the representations from the last encoder layer, which serve as a global view, those from other encoder layers are supplemented for a stereoscopic view of the source sequences.', 'after_edit': ' In this work, we propose layer-wise multi-view decoding, where for each decoder layer, together with the representations from the last encoder layer, which serve as a global view, those from other encoder layers are supplemented for a stereoscopic view of the source sequences.', 'label': 'clarity', 'raw_intents': ['style', 'clarity', 'clarity']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning,  the decoder relies on the attention mechanism to efficiently extract information from the encoder. While it is common practice to draw information from only the last encoder layer, recent work has proposed to use representations from different encoder layers for diversified levels of information. Nonetheless, the decoder still obtains only a single view of the source sequences, which might lead to insufficient training of the encoder layer stack due to the hierarchy bypassing problem. In this work, we propose layer-wise cross-view decoding, where for each decoder layer, together with the representations from the last encoder layer, which serve as a global view, those from other encoder layers are supplemented for a stereoscopic view of the source sequences. Systematic experiments  show that we successfully address the hierarchy bypassing problem and substantially improve the performance of sequence-to-sequence learning with deep representations on diverse tasks  .', 'domain': 'arxiv', 'before_edit': ' Systematic experiments  show that we successfully address the hierarchy bypassing problem and substantially improve the performance of sequence-to-sequence learning with deep representations on diverse tasks  .', 'after_edit': ' Systematic experiments and analyses show that we successfully address the hierarchy bypassing problem and substantially improve the performance of sequence-to-sequence learning with deep representations on diverse tasks  .', 'label': 'coherence', 'raw_intents': ['meaning-changed', 'coherence', 'coherence']}
{'doc_id': '2005.08081', 'context': 'In sequence-to-sequence learning,  the decoder relies on the attention mechanism to efficiently extract information from the encoder. While it is common practice to draw information from only the last encoder layer, recent work has proposed to use representations from different encoder layers for diversified levels of information. Nonetheless, the decoder still obtains only a single view of the source sequences, which might lead to insufficient training of the encoder layer stack due to the hierarchy bypassing problem. In this work, we propose layer-wise cross-view decoding, where for each decoder layer, together with the representations from the last encoder layer, which serve as a global view, those from other encoder layers are supplemented for a stereoscopic view of the source sequences. Systematic experiments  show that we successfully address the hierarchy bypassing problem and substantially improve the performance of sequence-to-sequence learning with deep representations on diverse tasks  .', 'domain': 'arxiv', 'before_edit': ' Systematic experiments  show that we successfully address the hierarchy bypassing problem and substantially improve the performance of sequence-to-sequence learning with deep representations on diverse tasks  .', 'after_edit': ' Systematic experiments  show that we successfully address the hierarchy bypassing problem and substantially improve the performance of sequence-to-sequence learning with deep representations on diverse tasks , i.e., machine translation, abstractive summarization and image captioning. In particular, our approach surpasses the previous state-of-the-art models on three benchmark machine translation datasets .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2005.14108', 'context': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others. However, in recent years it has been shown that these models possess weakness to noises which forces the model to misclassify. This issue has been studied profoundly in  image and audio domain. Very little has been studied on this issue with respect to textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome in order to move forward in this field.', 'domain': 'arxiv', 'before_edit': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others.', 'after_edit': 'Deep learning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2005.14108', 'context': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others. However, in recent years it has been shown that these models possess weakness to noises which forces the model to misclassify. This issue has been studied profoundly in  image and audio domain. Very little has been studied on this issue with respect to textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome in order to move forward in this field.', 'domain': 'arxiv', 'before_edit': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others.', 'after_edit': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis , and many others.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2005.14108', 'context': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others. However, in recent years it has been shown that these models possess weakness to noises which forces the model to misclassify. This issue has been studied profoundly in  image and audio domain. Very little has been studied on this issue with respect to textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome in order to move forward in this field.', 'domain': 'arxiv', 'before_edit': ' However, in recent years it has been shown that these models possess weakness to noises which forces the model to misclassify.', 'after_edit': ' However, in recent years it has been shown that these models possess weakness to noises which force the model to misclassify.', 'label': 'fluency', 'raw_intents': ['clarity', 'fluency', 'fluency']}
{'doc_id': '2005.14108', 'context': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others. However, in recent years it has been shown that these models possess weakness to noises which forces the model to misclassify. This issue has been studied profoundly in  image and audio domain. Very little has been studied on this issue with respect to textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome in order to move forward in this field.', 'domain': 'arxiv', 'before_edit': ' This issue has been studied profoundly in  image and audio domain.', 'after_edit': ' This issue has been studied profoundly in the image and audio domain.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2005.14108', 'context': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others. However, in recent years it has been shown that these models possess weakness to noises which forces the model to misclassify. This issue has been studied profoundly in  image and audio domain. Very little has been studied on this issue with respect to textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome in order to move forward in this field.', 'domain': 'arxiv', 'before_edit': ' Very little has been studied on this issue with respect to textual data.', 'after_edit': ' Very little has been studied on this issue concerning textual data.', 'label': 'clarity', 'raw_intents': ['coherence', 'clarity', 'clarity']}
{'doc_id': '2005.14108', 'context': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others. However, in recent years it has been shown that these models possess weakness to noises which forces the model to misclassify. This issue has been studied profoundly in  image and audio domain. Very little has been studied on this issue with respect to textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome in order to move forward in this field.', 'domain': 'arxiv', 'before_edit': ' In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea.', 'after_edit': ' In this manuscript , we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2005.14108', 'context': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others. However, in recent years it has been shown that these models possess weakness to noises which forces the model to misclassify. This issue has been studied profoundly in  image and audio domain. Very little has been studied on this issue with respect to textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome in order to move forward in this field.', 'domain': 'arxiv', 'before_edit': ' In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea.', 'after_edit': ' In this manuscript  we accumulated and analyzed different attacking techniques and various defense models on how to overcome this issue in order to provide a more comprehensive idea.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2005.14108', 'context': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others. However, in recent years it has been shown that these models possess weakness to noises which forces the model to misclassify. This issue has been studied profoundly in  image and audio domain. Very little has been studied on this issue with respect to textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome in order to move forward in this field.', 'domain': 'arxiv', 'before_edit': ' In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea.', 'after_edit': ' In this manuscript  we accumulated and analyzed different attacking techniques , various defense models to provide a more comprehensive idea.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2005.14108', 'context': 'Deep leaning models have been used widely for various purposes in recent years in object recognition, self-driving cars, face recognition, speech recognition, sentiment analysis  and many others. However, in recent years it has been shown that these models possess weakness to noises which forces the model to misclassify. This issue has been studied profoundly in  image and audio domain. Very little has been studied on this issue with respect to textual data. Even less survey on this topic has been performed to understand different types of attacks and defense techniques. In this manuscript  we accumulated and analyzed different attacking techniques , various defense models on how to overcome this issue in order to provide a more comprehensive idea. Later we point out some of the interesting findings of all papers and challenges that need to be overcome in order to move forward in this field.', 'domain': 'arxiv', 'before_edit': ' Later we point out some of the interesting findings of all papers and challenges that need to be overcome in order to move forward in this field.', 'after_edit': ' Later we point out some of the interesting findings of all papers and challenges that need to be overcome  to move forward in this field.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'coherence']}
{'doc_id': '2006.00632', 'context': 'Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early approaches in traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future intelligent NLP.', 'domain': 'arxiv', 'before_edit': 'Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks.', 'after_edit': 'Deep neural networks excel at learning from labeled data and achieve state-of-the-art resultson a wide array of Natural Language Processing tasks.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'others']}
{'doc_id': '2006.00632', 'context': 'Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early approaches in traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future intelligent NLP.', 'domain': 'arxiv', 'before_edit': ' We outline methods, from early approaches in traditional non-neural methods to pre-trained model transfer.', 'after_edit': ' We outline methods, from early  traditional non-neural methods to pre-trained model transfer.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2006.00632', 'context': 'Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early approaches in traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future intelligent NLP.', 'domain': 'arxiv', 'before_edit': ' Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future intelligent NLP.', 'after_edit': ' Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future  NLP.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'coherence']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous re-search , which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the expected attachment score, entropy, and generalized expectation criteria. As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence. In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reducesthe runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are upto 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively.', 'domain': 'arxiv', 'before_edit': ' We motivate the development of our framework with several cautionary tales of previous re-search , which has developed numerous less-than-optimal algorithms for computing expectations and their gradients.', 'after_edit': ' We motivate the development of our framework with several cautionary tales of previous research , which has developed numerous less-than-optimal algorithms for computing expectations and their gradients.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous re-search , which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the expected attachment score, entropy, and generalized expectation criteria. As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence. In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reducesthe runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are upto 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively.', 'domain': 'arxiv', 'before_edit': ' In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reducesthe runtime complexity by a factor (or two) of the sentence length.', 'after_edit': ' In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous re-search , which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the expected attachment score, entropy, and generalized expectation criteria. As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence. In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reducesthe runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are upto 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively.', 'domain': 'arxiv', 'before_edit': ' We find our algorithms are upto 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively.', 'after_edit': ' We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': 'We give a general framework for inference in spanning tree models.', 'after_edit': 'We propose a general framework for inference in spanning tree models.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models.', 'after_edit': 'We give a general framework for computing expectations in edge-factored, non-projective spanning-tree models.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': ' These algorithms are easy to implement, given the prevalence of automatic differentiation software.', 'after_edit': ' ', 'label': 'coherence', 'raw_intents': ['clarity', 'coherence', 'coherence']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': ' We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients.', 'after_edit': ' We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients.', 'label': 'style', 'raw_intents': ['style', 'style', 'coherence']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': ' We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients.', 'after_edit': ' We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': ' We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria .', 'after_edit': ' We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, entropy, and generalized expectation criteria .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': ' We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria .', 'after_edit': ' We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, and the generalized expectation criterion .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': ' As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence .', 'after_edit': ' As a bonus, we give algorithms for quantities that are missing in the literature, including the gradient of entropy, the KL divergence, and the gradient of the KL divergence .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': ' In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length.', 'after_edit': ' In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor  of the sentence length.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': ' We validate the implementation of our framework through runtime experiments.', 'after_edit': ' We validate  our framework through runtime experiments.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2008.12988', 'context': 'We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement, given the prevalence of automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous less-than-optimal algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, entropy, and generalized expectation criteria . As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor (or two) of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'domain': 'arxiv', 'before_edit': ' We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 12 and 26 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'after_edit': ' We validate the implementation of our framework through rigorous proofs of correctness and efficiency .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2008.12988', 'context': 'We propose a general framework for computing expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms.  We motivate the development of our framework with several emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, and the generalized expectation criterion . As a bonus, we give algorithms for quantities that are missing in the literature, including the gradient of entropy, the KL divergence, and the gradient of the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate  our framework through rigorous proofs of correctness and efficiency .', 'domain': 'arxiv', 'before_edit': 'We propose a general framework for computing expectations in edge-factored, non-projective spanning-tree models.', 'after_edit': 'We give a general framework for computing expectations in edge-factored, non-projective spanning-tree models.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '2008.12988', 'context': 'We propose a general framework for computing expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms.  We motivate the development of our framework with several emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, and the generalized expectation criterion . As a bonus, we give algorithms for quantities that are missing in the literature, including the gradient of entropy, the KL divergence, and the gradient of the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate  our framework through rigorous proofs of correctness and efficiency .', 'domain': 'arxiv', 'before_edit': 'We propose a general framework for computing expectations in edge-factored, non-projective spanning-tree models.', 'after_edit': 'We propose a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'clarity', 'meaning-changed']}
{'doc_id': '2008.12988', 'context': 'We propose a general framework for computing expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms.  We motivate the development of our framework with several emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, and the generalized expectation criterion . As a bonus, we give algorithms for quantities that are missing in the literature, including the gradient of entropy, the KL divergence, and the gradient of the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate  our framework through rigorous proofs of correctness and efficiency .', 'domain': 'arxiv', 'before_edit': '  We motivate the development of our framework with several emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients.', 'after_edit': ' These algorithms are easy to implement with or without automatic differentiation software. We motivate the development of our framework with several emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2008.12988', 'context': 'We propose a general framework for computing expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms.  We motivate the development of our framework with several emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, and the generalized expectation criterion . As a bonus, we give algorithms for quantities that are missing in the literature, including the gradient of entropy, the KL divergence, and the gradient of the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate  our framework through rigorous proofs of correctness and efficiency .', 'domain': 'arxiv', 'before_edit': ' We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, and the generalized expectation criterion .', 'after_edit': ' We demonstrate how our framework efficiently computes several quantities with known algorithms, including the  expected attachment score, and the generalized expectation criterion .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'coherence']}
{'doc_id': '2008.12988', 'context': 'We propose a general framework for computing expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms.  We motivate the development of our framework with several emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, and the generalized expectation criterion . As a bonus, we give algorithms for quantities that are missing in the literature, including the gradient of entropy, the KL divergence, and the gradient of the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate  our framework through rigorous proofs of correctness and efficiency .', 'domain': 'arxiv', 'before_edit': ' We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, and the generalized expectation criterion .', 'after_edit': ' We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, entropy, and generalized expectation criteria .', 'label': 'clarity', 'raw_intents': ['clarity', 'coherence', 'clarity']}
{'doc_id': '2008.12988', 'context': 'We propose a general framework for computing expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms.  We motivate the development of our framework with several emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, and the generalized expectation criterion . As a bonus, we give algorithms for quantities that are missing in the literature, including the gradient of entropy, the KL divergence, and the gradient of the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate  our framework through rigorous proofs of correctness and efficiency .', 'domain': 'arxiv', 'before_edit': ' As a bonus, we give algorithms for quantities that are missing in the literature, including the gradient of entropy, the KL divergence, and the gradient of the KL divergence .', 'after_edit': ' As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2008.12988', 'context': 'We propose a general framework for computing expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms.  We motivate the development of our framework with several emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, and the generalized expectation criterion . As a bonus, we give algorithms for quantities that are missing in the literature, including the gradient of entropy, the KL divergence, and the gradient of the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate  our framework through rigorous proofs of correctness and efficiency .', 'domain': 'arxiv', 'before_edit': ' We validate  our framework through rigorous proofs of correctness and efficiency .', 'after_edit': ' We validate the implementation of our framework through rigorous proofs of correctness and efficiency .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'coherence']}
{'doc_id': '2008.12988', 'context': 'We propose a general framework for computing expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms.  We motivate the development of our framework with several emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the Shannon entropy, the expected attachment score, and the generalized expectation criterion . As a bonus, we give algorithms for quantities that are missing in the literature, including the gradient of entropy, the KL divergence, and the gradient of the KL divergence . In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate  our framework through rigorous proofs of correctness and efficiency .', 'domain': 'arxiv', 'before_edit': ' We validate  our framework through rigorous proofs of correctness and efficiency .', 'after_edit': ' We validate  our framework through runtime experiments. We find our algorithms are up to 15 and 9 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2009.05169', 'context': "We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process, thus focusing on task-specific parts of the input. A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust differentiable top-k operator. For example, our experiments on a challenging summarization task of long documents show that our method is much faster and up to 16 times more memory efficient while significantly outperforming both dense and state-of-the-art sparse transformer models. The method can be effortlessly applied to many models used in NLP and CV, simultaneously with other improvements since representation pooling addresses a different aspect of the attention's complexity problem .", 'domain': 'arxiv', 'before_edit': ' A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust differentiable top-k operator.', 'after_edit': ' A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust trainable top-k operator.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2009.05169', 'context': "We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process, thus focusing on task-specific parts of the input. A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust differentiable top-k operator. For example, our experiments on a challenging summarization task of long documents show that our method is much faster and up to 16 times more memory efficient while significantly outperforming both dense and state-of-the-art sparse transformer models. The method can be effortlessly applied to many models used in NLP and CV, simultaneously with other improvements since representation pooling addresses a different aspect of the attention's complexity problem .", 'domain': 'arxiv', 'before_edit': ' For example, our experiments on a challenging summarization task of long documents show that our method is much faster and up to 16 times more memory efficient while significantly outperforming both dense and state-of-the-art sparse transformer models.', 'after_edit': ' For example, our experiments on a challenging summarization task of long documents show that our method is over 3 times faster and up to 16 times more memory efficient while significantly outperforming both dense and state-of-the-art sparse transformer models.', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2009.05169', 'context': "We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process, thus focusing on task-specific parts of the input. A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust differentiable top-k operator. For example, our experiments on a challenging summarization task of long documents show that our method is much faster and up to 16 times more memory efficient while significantly outperforming both dense and state-of-the-art sparse transformer models. The method can be effortlessly applied to many models used in NLP and CV, simultaneously with other improvements since representation pooling addresses a different aspect of the attention's complexity problem .", 'domain': 'arxiv', 'before_edit': " The method can be effortlessly applied to many models used in NLP and CV, simultaneously with other improvements since representation pooling addresses a different aspect of the attention's complexity problem .", 'after_edit': ' The method can be effortlessly applied to many models used in NLP and CV, simultaneously with other improvements  .', 'label': 'clarity', 'raw_intents': ['coherence', 'clarity', 'clarity']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .", 'domain': 'arxiv', 'before_edit': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage.", 'after_edit': "Inspired by Google's Neural Machine Translation (NMT)  that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage.", 'label': 'style', 'raw_intents': ['style', 'style', 'style']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .", 'domain': 'arxiv', 'before_edit': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage.", 'after_edit': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an  uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .", 'domain': 'arxiv', 'before_edit': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage.", 'after_edit': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source contents with the optimal coverage.", 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .", 'domain': 'arxiv', 'before_edit': ' Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources.', 'after_edit': ' Unlike NMT, NAS is not based on one-to-one transformation. Instead, its attention distribution for the input should be irregular and depend on the content layout of the source documents. To address this matter, we construct an attention-prediction model is constructed to learn the dependency between attention weights and sources.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .", 'domain': 'arxiv', 'before_edit': ' Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources.', 'after_edit': ' Therefore, an attention-prediction model  to learn the dependency between attention weights and sources.', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .", 'domain': 'arxiv', 'before_edit': ' Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed.', 'after_edit': ' Therefore, an attention-prediction model is constructed to learn the dependency between the optimal attention distribution and the source. By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .", 'domain': 'arxiv', 'before_edit': ' Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed.', 'after_edit': ' Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the vanilla beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed.', 'label': 'style', 'raw_intents': ['style', 'style', 'style']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .", 'domain': 'arxiv', 'before_edit': ' Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed.', 'after_edit': ' Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware mechanism , significant improvements on the quality of summaries could be further observed.', 'label': 'style', 'raw_intents': ['style', 'style', 'style']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .", 'domain': 'arxiv', 'before_edit': ' Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed.', 'after_edit': ' Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be  observed.', 'label': 'style', 'raw_intents': ['style', 'style', 'style']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) \\mbox{%DIFAUXCMD Wu2016Google that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\mbox{%DIFAUXCMD liu2018generating By refining the regular beam search with the attention-aware inference , significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .", 'domain': 'arxiv', 'before_edit': ' Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture .', 'after_edit': " Last but not the least, the attention-aware inference has strong universality that can be easily adopted to different hierarchical summarization models to promote the models' performance .", 'label': 'style', 'raw_intents': ['style', 'style', 'style']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) that models the one-to-one alignment in translation tasks with an uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source contents with the optimal coverage. Unlike NMT, NAS is not based on one-to-one transformation. Instead, its attention distribution for the input should be irregular and depend on the content layout of the source documents. To address this matter, we construct an attention-prediction model to learn the dependency between the optimal attention distribution and the source . By refining the vanilla beam searchwith the attention-aware mechanism, significant improvements on the quality of summaries could be observed. Last but not the least, the attention-aware inference has strong universality that can be easily adopted to different hierarchical summarization models to promote the models' performance .", 'domain': 'arxiv', 'before_edit': "Inspired by Google's Neural Machine Translation (NMT) that models the one-to-one alignment in translation tasks with an uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source contents with the optimal coverage.", 'after_edit': 'This paper proposes a novel inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source contents with the optimal coverage.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) that models the one-to-one alignment in translation tasks with an uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source contents with the optimal coverage. Unlike NMT, NAS is not based on one-to-one transformation. Instead, its attention distribution for the input should be irregular and depend on the content layout of the source documents. To address this matter, we construct an attention-prediction model to learn the dependency between the optimal attention distribution and the source . By refining the vanilla beam searchwith the attention-aware mechanism, significant improvements on the quality of summaries could be observed. Last but not the least, the attention-aware inference has strong universality that can be easily adopted to different hierarchical summarization models to promote the models' performance .", 'domain': 'arxiv', 'before_edit': "Inspired by Google's Neural Machine Translation (NMT) that models the one-to-one alignment in translation tasks with an uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source contents with the optimal coverage. Unlike NMT, NAS is not based on one-to-one transformation. Instead, its attention distribution for the input should be irregular and depend on the content layout of the source documents. To address this matter, we construct an attention-prediction model to learn the dependency between the optimal attention distribution and the source . By refining the vanilla beam searchwith the attention-aware mechanism, significant improvements on the quality of summaries could be observed.", 'after_edit': "Inspired by Google's Neural Machine Translation (NMT) that models the one-to-one alignment in translation tasks with an uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for seq-to-seq models. It corrects beam search step-by-step via the optimal attention distribution to make the generated text attend to source tokens in a controlled way. Experiments show the proposed attention-aware mechanism, significant improvements on the quality of summaries could be observed.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2009.06891', 'context': "Inspired by Google's Neural Machine Translation (NMT) that models the one-to-one alignment in translation tasks with an uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source contents with the optimal coverage. Unlike NMT, NAS is not based on one-to-one transformation. Instead, its attention distribution for the input should be irregular and depend on the content layout of the source documents. To address this matter, we construct an attention-prediction model to learn the dependency between the optimal attention distribution and the source . By refining the vanilla beam searchwith the attention-aware mechanism, significant improvements on the quality of summaries could be observed. Last but not the least, the attention-aware inference has strong universality that can be easily adopted to different hierarchical summarization models to promote the models' performance .", 'domain': 'arxiv', 'before_edit': " By refining the vanilla beam searchwith the attention-aware mechanism, significant improvements on the quality of summaries could be observed. Last but not the least, the attention-aware inference has strong universality that can be easily adopted to different hierarchical summarization models to promote the models' performance .", 'after_edit': ' By refining the vanilla beam searchwith the attention-aware inference produces summaries rather differently from the beam search, and achieves promising improvements of higher scores and greater conciseness. The algorithm is also proven robust as it remains to outperform beam search significantly even with corrupted attention distributions .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2009.06891', 'context': 'This paper proposes a novel inference algorithm for seq-to-seq models. It corrects beam search step-by-step via the optimal attention distribution to make the generated text attend to source tokens in a controlled way. Experiments show the proposed attention-aware inferenceproduces summaries rather differently from the beam search , and achieves promising improvements of higher scores and greater conciseness. The algorithm is also proven robust as it remains to outperform beam search significantly even with corrupted attention distributions  .', 'domain': 'arxiv', 'before_edit': 'This paper proposes a novel inference algorithm for seq-to-seq models. It corrects beam search step-by-step via the optimal attention distribution to make the generated text attend to source tokens in a controlled way.', 'after_edit': 'This study develops a calibrated beam-based algorithm with global awareness for neural abstractive summarization, aiming to improve the local optimality problem of the original beam search in a rigorous way. Specifically, a novel global protocol is proposed based on the attention distribution to make the generated text attend to source tokens in a controlled way.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2009.06891', 'context': 'This paper proposes a novel inference algorithm for seq-to-seq models. It corrects beam search step-by-step via the optimal attention distribution to make the generated text attend to source tokens in a controlled way. Experiments show the proposed attention-aware inferenceproduces summaries rather differently from the beam search , and achieves promising improvements of higher scores and greater conciseness. The algorithm is also proven robust as it remains to outperform beam search significantly even with corrupted attention distributions  .', 'domain': 'arxiv', 'before_edit': ' It corrects beam search step-by-step via the optimal attention distribution to make the generated text attend to source tokens in a controlled way. Experiments show the proposed attention-aware inferenceproduces summaries rather differently from the beam search , and achieves promising improvements of higher scores and greater conciseness.', 'after_edit': ' It corrects beam search step-by-step via the optimal attention distribution to stipulate how a global optimal hypothesis should attend to the source. A global scoring function is then developed to regulate beam search to generate summaries in a more near-global optimal fashion. This novel design enjoys a distinctive property, i.e. the global attention distribution could be predicted before inference, enabling stepwise improvements on the beam search , and achieves promising improvements of higher scores and greater conciseness.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2009.06891', 'context': 'This paper proposes a novel inference algorithm for seq-to-seq models. It corrects beam search step-by-step via the optimal attention distribution to make the generated text attend to source tokens in a controlled way. Experiments show the proposed attention-aware inferenceproduces summaries rather differently from the beam search , and achieves promising improvements of higher scores and greater conciseness. The algorithm is also proven robust as it remains to outperform beam search significantly even with corrupted attention distributions  .', 'domain': 'arxiv', 'before_edit': ' Experiments show the proposed attention-aware inferenceproduces summaries rather differently from the beam search , and achieves promising improvements of higher scores and greater conciseness.', 'after_edit': ' Experiments show the proposed attention-aware inferenceproduces summaries rather differently from the beam search through the global scoring function. Extensive experiments on 9 datasets show that the global-aware inference significantly improves state-of-the-art summarization models even using empirical hyper-parameters.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2009.06891', 'context': 'This paper proposes a novel inference algorithm for seq-to-seq models. It corrects beam search step-by-step via the optimal attention distribution to make the generated text attend to source tokens in a controlled way. Experiments show the proposed attention-aware inferenceproduces summaries rather differently from the beam search , and achieves promising improvements of higher scores and greater conciseness. The algorithm is also proven robust as it remains to outperform beam search significantly even with corrupted attention distributions  .', 'domain': 'arxiv', 'before_edit': ' The algorithm is also proven robust as it remains to outperform beam search significantly even with corrupted attention distributions  .', 'after_edit': ' The algorithm is also proven robust as it remains to generate meaningful texts with corrupted attention distributions  .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2009.06891', 'context': 'This paper proposes a novel inference algorithm for seq-to-seq models. It corrects beam search step-by-step via the optimal attention distribution to make the generated text attend to source tokens in a controlled way. Experiments show the proposed attention-aware inferenceproduces summaries rather differently from the beam search , and achieves promising improvements of higher scores and greater conciseness. The algorithm is also proven robust as it remains to outperform beam search significantly even with corrupted attention distributions  .', 'domain': 'arxiv', 'before_edit': ' The algorithm is also proven robust as it remains to outperform beam search significantly even with corrupted attention distributions  .', 'after_edit': ' The algorithm is also proven robust as it remains to outperform beam search significantly even with corrupted attention distributions . The codes and a comprehensive set of examples are available .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  .', 'after_edit': 'We propose Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  .', 'label': 'clarity', 'raw_intents': ['clarity', 'coherence', 'clarity']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  .', 'after_edit': ' In this paper, we present Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query with relevant contexts through text generation  .', 'label': 'clarity', 'raw_intents': ['clarity', 'meaning-changed', 'clarity']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  .', 'after_edit': ' In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query  through text generation  .', 'label': 'coherence', 'raw_intents': ['clarity', 'coherence', 'coherence']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  .', 'after_edit': ' In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation of heuristically discovered relevant contexts without external resources as supervision .', 'label': 'clarity', 'raw_intents': ['clarity', 'meaning-changed', 'clarity']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'after_edit': ' We demonstrate  that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'clarity']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'after_edit': ' We demonstrate on open-domain question answering that the generated contexts substantially enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'label': 'clarity', 'raw_intents': ['clarity', 'style', 'clarity']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'after_edit': ' We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and  GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'clarity']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'after_edit': ' We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than  state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'clarity']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'after_edit': ' We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense retrieval methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'meaning-changed']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense .', 'after_edit': ' We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR  .', 'label': 'others', 'raw_intents': ['others', 'others', 'others']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy.', 'after_edit': ' We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'after_edit': ' GAR achieves state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'label': 'coherence', 'raw_intents': ['clarity', 'coherence', 'coherence']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'after_edit': ' Furthermore, GAR achieves the state-of-the-art performance on  Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'coherence']}
{'doc_id': '2009.08553', 'context': 'Conventional sparse retrieval methods such as TF-IDF and BM25 are simple and efficient, but solely rely on lexical overlap without semantic matching. Recent dense retrieval methods learn latent representations to tackle the lexical mismatch problem, while being more computationally expensive and insufficient for exact matching as they embed the text sequence into a single vector with limited capacity. In this paper, we present Generation-Augmented Retrieval (GAR) , a query expansion method that augments a query with relevant contexts through text generation  . We demonstrate on open-domain question answering that the generated contexts significantly enrich the semantics of the queries and thus GAR with sparse representations (BM25) achieves comparable or better performance than the state-of-the-art dense  methods such as DPR \\mbox{%DIFAUXCMD karpukhin2020dense . We show that generating various contexts of a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'domain': None, 'before_edit': ' Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive setting when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'after_edit': ' Furthermore, GAR achieves the state-of-the-art performance on the Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'meaning-changed']}
{'doc_id': '2011.10896', 'context': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources . The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'domain': 'arxiv', 'before_edit': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources .', 'after_edit': 'This paper presents HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources .', 'label': 'clarity', 'raw_intents': ['coherence', 'clarity', 'clarity']}
{'doc_id': '2011.10896', 'context': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources . The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'domain': 'arxiv', 'before_edit': ' In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources .', 'after_edit': ' In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles . HALO implements a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'meaning-changed']}
{'doc_id': '2011.10896', 'context': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources . The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'domain': 'arxiv', 'before_edit': ' In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources .', 'after_edit': ' In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the performance-portable execution of a hardware-agnostic application host codes across heterogeneous accelerator resources .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2011.10896', 'context': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources . The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'domain': 'arxiv', 'before_edit': ' In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources .', 'after_edit': ' In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic host application across heterogeneous accelerators .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2011.10896', 'context': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources . The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'domain': 'arxiv', 'before_edit': ' The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs,', 'after_edit': ' The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620  CPUs,', 'label': 'clarity', 'raw_intents': ['clarity', 'style', 'clarity']}
{'doc_id': '2011.10896', 'context': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources . The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'domain': 'arxiv', 'before_edit': ' Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'after_edit': ' Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for host programs to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2011.10896', 'context': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources . The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'domain': 'arxiv', 'before_edit': ' Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'after_edit': ' Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently top performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '2011.10896', 'context': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources . The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'domain': 'arxiv', 'before_edit': ' Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'after_edit': ' Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score  , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '2011.10896', 'context': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources . The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'domain': 'arxiv', 'before_edit': ' Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'after_edit': ' Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is up to five orders of magnitude higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2011.10896', 'context': 'Hardware-agnostic programming with high performance portability will be the bedrock for realizing the ubiquitous adoption of emerging accelerator technologies in future heterogeneous high-performance computing (HPC) systems, which is the key to achieving the next level of HPC performance on an expanding accelerator landscape. In this paper , we present HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles and a novel compute-centric message passing interface (C^2MPI) specification for enabling the portable and performance-optimized execution of hardware-agnostic application host codes across heterogeneous accelerator resources . The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 v4 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'domain': 'arxiv', 'before_edit': ' Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution that suffers from an unstably low performance portability score. of the documentation of their work .', 'after_edit': ' Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for the host program to run across all the computing devices with a consistently maximum performance portability score of 1.0 , which is 2x-861,883x higher than the OpenCL-based solution  .', 'label': 'clarity', 'raw_intents': ['coherence', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations.', 'after_edit': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into.', 'after_edit': ' To guide efforts at debiasing these systems , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into.', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into.', 'after_edit': ' To address and combat this , the NLP community relies on a variety of metrics that quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into.', 'label': 'fluency', 'raw_intents': ['clarity', 'fluency', 'fluency']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into.', 'after_edit': ' To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in models . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into.', 'after_edit': ' To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into.', 'after_edit': ' To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings are plugged into.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages.', 'after_edit': ' To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages.', 'label': 'clarity', 'raw_intents': ['clarity', 'style', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages.', 'after_edit': ' We measure both intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages.', 'after_edit': ' We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions . Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages.', 'label': 'coherence', 'raw_intents': ['coherence', 'style', 'coherence']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data.', 'after_edit': ' We urge researchers working on debiasing to focus on extrinsic measures of bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data.', 'after_edit': ' We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and to make using these measures more feasible via creation of additional challenge sets and annotated test data.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data.', 'after_edit': ' We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of new challenge sets and annotated test data.', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'after_edit': ' To aid this effort, we release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'label': 'clarity', 'raw_intents': ['style', 'clarity', 'clarity']}
{'doc_id': '2012.15859', 'context': 'Natural Language Processing (NLP) systems learn harmful societal biases that cause them to widely proliferate inequality as they are deployed in more and more situations. To address and combat this , the NLP community relies on a variety of metrics to identify and quantify bias in black-box modelsand to guide efforts at debiasing . Some of these metrics are intrinsic, and are measured in word embedding spaces, and some are extrinsic, which measure the bias present downstream in the tasks that the word embeddings are plugged into. This research examines whether easy-to-measure intrinsic metrics correlate well to real world extrinsic metrics . We measure both intrinsic and extrinsic bias across hundreds of trained models covering different tasks and experimental conditions and find that there is no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We advise that efforts to debias embedding spaces be always also paired with measurement of downstream model bias, and suggest that that community increase effort into making downstream measurement more feasible via creation of additional challenge sets and annotated test data. We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'domain': 'arxiv', 'before_edit': ' We additionally release code, a new intrinsic metric, and an annotated test set for gender bias for hatespeech .', 'after_edit': ' We additionally release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'fluency']}
{'doc_id': '2103.06874', 'context': "Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences--without explicit tokenization or vocabulary--and a pre-training strategy with soft inductive biases in place of hard token boundaries. To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes con-text . CANINE outperforms a comparable mBERT model by >= 1 F1 on TyDi QA, a challenging multilingual benchmark, despite having 28\\% fewer model parameters.", 'domain': None, 'before_edit': ' In this paper, we present CANINE, a neural encoder that operates directly on character sequences--without explicit tokenization or vocabulary--and a pre-training strategy with soft inductive biases in place of hard token boundaries.', 'after_edit': ' In this paper, we present CANINE, a neural encoder that operates directly on character sequences, without explicit tokenization or vocabulary--and a pre-training strategy with soft inductive biases in place of hard token boundaries.', 'label': 'coherence', 'raw_intents': ['coherence', 'clarity', 'coherence']}
{'doc_id': '2103.06874', 'context': "Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences--without explicit tokenization or vocabulary--and a pre-training strategy with soft inductive biases in place of hard token boundaries. To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes con-text . CANINE outperforms a comparable mBERT model by >= 1 F1 on TyDi QA, a challenging multilingual benchmark, despite having 28\\% fewer model parameters.", 'domain': None, 'before_edit': ' In this paper, we present CANINE, a neural encoder that operates directly on character sequences--without explicit tokenization or vocabulary--and a pre-training strategy with soft inductive biases in place of hard token boundaries.', 'after_edit': ' In this paper, we present CANINE, a neural encoder that operates directly on character sequences--without explicit tokenization or vocabulary, and a pre-training strategy with soft inductive biases in place of hard token boundaries.', 'label': 'coherence', 'raw_intents': ['coherence', 'fluency', 'coherence']}
{'doc_id': '2103.06874', 'context': "Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences--without explicit tokenization or vocabulary--and a pre-training strategy with soft inductive biases in place of hard token boundaries. To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes con-text . CANINE outperforms a comparable mBERT model by >= 1 F1 on TyDi QA, a challenging multilingual benchmark, despite having 28\\% fewer model parameters.", 'domain': None, 'before_edit': ' To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes con-text .', 'after_edit': ' To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context .', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2103.14972', 'context': 'This paper describes a corpus annotation process to support the identification of hate speech and offensive language in social media.  The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive comments ), the level of the offense (highly offensive, moderately offensive and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement  .', 'domain': 'arxiv', 'before_edit': '  The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers:', 'after_edit': ' In addition, we provide the first robust corpus this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers:', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2103.14972', 'context': 'This paper describes a corpus annotation process to support the identification of hate speech and offensive language in social media.  The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive comments ), the level of the offense (highly offensive, moderately offensive and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement  .', 'domain': 'arxiv', 'before_edit': ' a binary classification (offensive versus non-offensive comments ), the level of the offense (highly offensive, moderately offensive and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism and fat phobia).', 'after_edit': ' a binary classification (offensive versus non-offensive language ), the level of the offense (highly offensive, moderately offensive and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism and fat phobia).', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2103.14972', 'context': 'This paper describes a corpus annotation process to support the identification of hate speech and offensive language in social media.  The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive comments ), the level of the offense (highly offensive, moderately offensive and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement  .', 'domain': 'arxiv', 'before_edit': ' a binary classification (offensive versus non-offensive comments ), the level of the offense (highly offensive, moderately offensive and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism and fat phobia).', 'after_edit': ' a binary classification (offensive versus non-offensive comments ), the level of  offense (highly offensive, moderately offensive and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism and fat phobia).', 'label': 'fluency', 'raw_intents': ['fluency', 'clarity', 'fluency']}
{'doc_id': '2103.14972', 'context': 'This paper describes a corpus annotation process to support the identification of hate speech and offensive language in social media.  The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive comments ), the level of the offense (highly offensive, moderately offensive and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement  .', 'domain': 'arxiv', 'before_edit': ' Each comment was annotated by three different annotators, which achieved high inter-annotator agreement  .', 'after_edit': ' Each comment was annotated by three different annotators, which achieved high inter-annotator agreement . The proposed annotation process is also language and domain independent .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2103.14972', 'context': 'This paper describes a corpus annotation process to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The proposed annotation process is also language and domain independent  .', 'domain': 'arxiv', 'before_edit': ' The proposed annotation process is also language and domain independent  .', 'after_edit': ' The proposed annotation approach is also language and domain independent  .', 'label': 'clarity', 'raw_intents': ['clarity', 'style', 'clarity']}
{'doc_id': '2103.14972', 'context': 'This paper describes a corpus annotation process to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The proposed annotation process is also language and domain independent  .', 'domain': 'arxiv', 'before_edit': ' The proposed annotation process is also language and domain independent  .', 'after_edit': ' The proposed annotation process is also language and domain independent , nevertheless, it was currently applied for Brazilian Portuguese .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media.', 'after_edit': 'The understanding of an offense is subjective and people may have different opinions about the offensiveness of a comment. Also, offenses and hate speech may occur through sarcasm, which hides the real intention of the comment and makes the decision of the annotators more confusing. Therefore, provide a well-structured annotation process is crucial to a better understanding of hate speech and offensive language phenomena, as well as supply better performance for machine learning classifiers. In this paper, we describe a corpus annotation process  to support the identification of hate speech and offensive language in social media.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media.', 'after_edit': 'This paperdescribes a corpus annotation process , which was guided by a linguist, and a hate speech skilled to support the identification of hate speech and offensive language in social media.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media.', 'after_edit': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language on social media.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': ' In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language.', 'after_edit': ' In addition, we provide the first robust corpus of this kind for the Brazilian Portuguese language.', 'label': 'fluency', 'raw_intents': ['fluency', 'clarity', 'fluency']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': ' The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers:', 'after_edit': ' The corpus was collected from Instagram posts of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers:', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': ' a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia).', 'after_edit': ' a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive , and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia).', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'coherence']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': ' a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia).', 'after_edit': ' a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religious intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia).', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': ' a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia).', 'after_edit': ' a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism , and fat phobia).', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': ' Each comment was annotated by three different annotators, which achieved high inter-annotator agreement.', 'after_edit': ' Each comment was annotated by three different annotators, and achieved high inter-annotator agreement.', 'label': 'coherence', 'raw_intents': ['coherence', 'fluency', 'coherence']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': ' The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'after_edit': ' The new proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': ' The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'after_edit': ' The  proposed annotation approach is also language and domain-independent (although it has been applied for Brazilian Portuguese  .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2103.14972', 'context': 'This paperdescribes a corpus annotation process  to support the identification of hate speech and offensive language in social media. In addition, we provide the first robust corpus  this kind for the Brazilian Portuguese language. The corpus was collected from Instagram pages of political personalities and manually annotated, being composed by 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive language), the level of offense (highly offensive, moderately offensive  and slightly offensive messages), and the identification regarding the target of the discriminatory content (xenophobia, racism, homophobia, sexism, religion intolerance, partyism, apology to the dictatorship, antisemitism  and fat phobia). Each comment was annotated by three different annotators, which achieved high inter-annotator agreement. The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'domain': 'arxiv', 'before_edit': ' The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese  .', 'after_edit': ' The  proposed annotation approach is also language and domain independent, nevertheless, it was currently applied for Brazilian Portuguese ) .', 'label': 'fluency', 'raw_intents': ['fluency', 'others', 'fluency']}
{'doc_id': '2104.12265', 'context': 'This paper presents a new approach for offensive language and hate speech detection on social media. Our approach incorporates an offensive lexicon composed by implicit and explicit offensive and swearing expressions annotated with binary classes: context-dependent offensive and context-independent offensive. Due to the severity of the hate speech and offensive comments in Brazil  and the lack of research in Portuguese, Brazilian Portuguese is the language used to validate our method. However, the proposal may be applied to any other language or domain. Based on the obtained results, the proposed approach showed high performance results overcoming the current baselines for European and Brazilian Portuguese.', 'domain': 'arxiv', 'before_edit': ' Our approach incorporates an offensive lexicon composed by implicit and explicit offensive and swearing expressions annotated with binary classes: context-dependent offensive and context-independent offensive.', 'after_edit': ' Our approach incorporate an offensive lexicon composed by implicit and explicit offensive and swearing expressions annotated with binary classes: context-dependent offensive and context-independent offensive.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2104.12265', 'context': 'This paper presents a new approach for offensive language and hate speech detection on social media. Our approach incorporates an offensive lexicon composed by implicit and explicit offensive and swearing expressions annotated with binary classes: context-dependent offensive and context-independent offensive. Due to the severity of the hate speech and offensive comments in Brazil  and the lack of research in Portuguese, Brazilian Portuguese is the language used to validate our method. However, the proposal may be applied to any other language or domain. Based on the obtained results, the proposed approach showed high performance results overcoming the current baselines for European and Brazilian Portuguese.', 'domain': 'arxiv', 'before_edit': ' Our approach incorporates an offensive lexicon composed by implicit and explicit offensive and swearing expressions annotated with binary classes: context-dependent offensive and context-independent offensive.', 'after_edit': ' Our approach incorporates an offensive lexicon composed by implicit and explicit offensive and swearing expressions annotated with binary classes: context-dependent  and context-independent offensive.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2104.12265', 'context': 'This paper presents a new approach for offensive language and hate speech detection on social media. Our approach incorporates an offensive lexicon composed by implicit and explicit offensive and swearing expressions annotated with binary classes: context-dependent offensive and context-independent offensive. Due to the severity of the hate speech and offensive comments in Brazil  and the lack of research in Portuguese, Brazilian Portuguese is the language used to validate our method. However, the proposal may be applied to any other language or domain. Based on the obtained results, the proposed approach showed high performance results overcoming the current baselines for European and Brazilian Portuguese.', 'domain': 'arxiv', 'before_edit': ' Due to the severity of the hate speech and offensive comments in Brazil  and the lack of research in Portuguese, Brazilian Portuguese is the language used to validate our method.', 'after_edit': ' Due to the severity of the hate speech and offensive comments in Brazil , and the lack of research in Portuguese, Brazilian Portuguese is the language used to validate our method.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '100912', 'context': 'Gordon Brown , who made a speech to the UK Labour Party todayThe UK Prime Minister, Gordon Brown, has made a speech to UK Labour Party members as part of his party\'s spring conference. In the speech he made several commitments, which he said he would "work tirelessly" to achieve. On the subjects of employment and welfare, Mr. Brown said that his party "will offer new entitlements to the carers that serve this country [the UK] so well. He continued by saying that since Labour came to power in 1997, three million new jobs have been created. He also said that Labour will attempt to get 100,00 people back to work. When discussing crime in the speech, Mr. Brown claimed that his party has "already cut crime by 32\\%." He also made promises of community policing across the UK.', 'domain': 'news', 'before_edit': "Gordon Brown , who made a speech to the UK Labour Party todayThe UK Prime Minister, Gordon Brown, has made a speech to UK Labour Party members as part of his party's spring conference.", 'after_edit': "Gordon Brown in 2004.United Kingdom Prime Minister, Gordon Brown, has made a speech to UK Labour Party members as part of his party's spring conference.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'meaning-changed']}
{'doc_id': '100912', 'context': 'Gordon Brown , who made a speech to the UK Labour Party todayThe UK Prime Minister, Gordon Brown, has made a speech to UK Labour Party members as part of his party\'s spring conference. In the speech he made several commitments, which he said he would "work tirelessly" to achieve. On the subjects of employment and welfare, Mr. Brown said that his party "will offer new entitlements to the carers that serve this country [the UK] so well. He continued by saying that since Labour came to power in 1997, three million new jobs have been created. He also said that Labour will attempt to get 100,00 people back to work. When discussing crime in the speech, Mr. Brown claimed that his party has "already cut crime by 32\\%." He also made promises of community policing across the UK.', 'domain': 'news', 'before_edit': "Gordon Brown , who made a speech to the UK Labour Party todayThe UK Prime Minister, Gordon Brown, has made a speech to UK Labour Party members as part of his party's spring conference.", 'after_edit': "Gordon Brown , who made a speech to the UK Labour Party todayThe UK Prime Minister, Gordon Brown, has made a speech to UK Labour Party members as part of the party's spring conference.", 'label': 'clarity', 'raw_intents': ['clarity', 'style', 'fluency']}
{'doc_id': '100912', 'context': 'Gordon Brown , who made a speech to the UK Labour Party todayThe UK Prime Minister, Gordon Brown, has made a speech to UK Labour Party members as part of his party\'s spring conference. In the speech he made several commitments, which he said he would "work tirelessly" to achieve. On the subjects of employment and welfare, Mr. Brown said that his party "will offer new entitlements to the carers that serve this country [the UK] so well. He continued by saying that since Labour came to power in 1997, three million new jobs have been created. He also said that Labour will attempt to get 100,00 people back to work. When discussing crime in the speech, Mr. Brown claimed that his party has "already cut crime by 32\\%." He also made promises of community policing across the UK.', 'domain': 'news', 'before_edit': ' On the subjects of employment and welfare, Mr. Brown said that his party "will offer new entitlements to the carers that serve this country [the UK] so well.', 'after_edit': ' On the subjects of employment and welfare, Mr. Brown said that his party "will offer new entitlements to the careers that serve this country [the UK] so well.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '100912', 'context': 'Gordon Brown , who made a speech to the UK Labour Party todayThe UK Prime Minister, Gordon Brown, has made a speech to UK Labour Party members as part of his party\'s spring conference. In the speech he made several commitments, which he said he would "work tirelessly" to achieve. On the subjects of employment and welfare, Mr. Brown said that his party "will offer new entitlements to the carers that serve this country [the UK] so well. He continued by saying that since Labour came to power in 1997, three million new jobs have been created. He also said that Labour will attempt to get 100,00 people back to work. When discussing crime in the speech, Mr. Brown claimed that his party has "already cut crime by 32\\%." He also made promises of community policing across the UK.', 'domain': 'news', 'before_edit': ' When discussing crime in the speech, Mr. Brown claimed that his party has "already cut crime by 32\\%."', 'after_edit': ' While discussing crime in the speech, Mr. Brown claimed that his party has "already cut crime by 32\\%."', 'label': 'fluency', 'raw_intents': ['fluency', 'coherence', 'fluency']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, famed as the largest show of students\' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry. Besides for the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers  Competition" and "2008 YODEX Interior Design Competition" will also be announced in this Saturday (May 17).', 'domain': 'news', 'before_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show of students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry.", 'after_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry.", 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, famed as the largest show of students\' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry. Besides for the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers  Competition" and "2008 YODEX Interior Design Competition" will also be announced in this Saturday (May 17).', 'domain': 'news', 'before_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show of students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry.", 'after_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show of students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry.", 'label': 'clarity', 'raw_intents': ['clarity', 'style', 'clarity']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, famed as the largest show of students\' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry. Besides for the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers  Competition" and "2008 YODEX Interior Design Competition" will also be announced in this Saturday (May 17).', 'domain': 'news', 'before_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show of students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry.", 'after_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show of students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical  units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry.", 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, famed as the largest show of students\' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry. Besides for the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers  Competition" and "2008 YODEX Interior Design Competition" will also be announced in this Saturday (May 17).', 'domain': 'news', 'before_edit': ' Besides for the early-announced "Wow!', 'after_edit': ' Besides of the early-announced "Wow!', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, famed as the largest show of students\' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry. Besides for the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers  Competition" and "2008 YODEX Interior Design Competition" will also be announced in this Saturday (May 17).', 'domain': 'news', 'before_edit': ' Taiwan Design Award", winners from "2008 Young Designers  Competition" and "2008 YODEX Interior Design Competition" will also be announced in this Saturday (May 17).', 'after_edit': ' Taiwan Design Award", winners from "2008 Young Designers \' Competition" and "2008 YODEX Interior Design Competition" will also be announced in this Saturday (May 17).', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, famed as the largest show of students\' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center today and will close at Sunday (May 18) with participations from 87 academical and industrial units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from design industry. Besides for the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers  Competition" and "2008 YODEX Interior Design Competition" will also be announced in this Saturday (May 17).', 'domain': 'news', 'before_edit': ' Taiwan Design Award", winners from "2008 Young Designers  Competition" and "2008 YODEX Interior Design Competition" will also be announced in this Saturday (May 17).', 'after_edit': ' Taiwan Design Award", winners from "2008 Young Designers  Competition" and "2008 YODEX Interior Design Competition" will also be announced  this Saturday (May 17).', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '106096', 'context': "noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'domain': 'news', 'before_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'after_edit': " 2008 The 27th Young Designers' Exhibition, opened on May 15 at the Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '106096', 'context': "noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'domain': 'news', 'before_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'after_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center  and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'label': 'clarity', 'raw_intents': ['clarity', 'style', 'clarity']}
{'doc_id': '106096', 'context': "noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'domain': 'news', 'before_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'after_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday May 18. It features participation by 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '106096', 'context': "noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'domain': 'news', 'before_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'after_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academic groups in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '106096', 'context': "noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'domain': 'news', 'before_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'after_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '106096', 'context': "noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'domain': 'news', 'before_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase varied achievements from designindustry .", 'after_edit': " 2008 The 27th Young Designers' Exhibition, famed as the largest show from students' creations, recognized by International Council of Societies of Industrial Design (ICSID) since last year, started at Taipei World Trade Center yesterday and will close at Sunday (May 18) with participations from 87 academical units in Taiwan and 20 units from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations .", 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': " 2008 The 27th Young Designers' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18.", 'after_edit': " 2008 The 27th Young Designers' Exhibition, opened on May 15 at the Taipei World Trade Center and closes Sunday May 18.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': ' Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries.', 'after_edit': ' Besides the several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries.', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': ' Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries.', 'after_edit': ' Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will  showcase different solutions for  design, creative, and cultural industries.', 'label': 'clarity', 'raw_intents': ['clarity', 'coherence', 'clarity']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': ' Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries.', 'after_edit': ' Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for the design, creative, and cultural industries.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': ' In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition.', 'after_edit': ' The show\'s organizer , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition.', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': ' In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition.', 'after_edit': ' In addition , Taiwan Design Center,  also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'coherence']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': ' In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition.', 'after_edit': ' In addition , Taiwan Design Center, the show organizer, also designed several on-site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition.', 'label': 'clarity', 'raw_intents': ['fluency', 'clarity', 'clarity']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': ' In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition.', 'after_edit': ' In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Career Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': ' In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition.', 'after_edit': ' In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with the on-line exhibition.', 'label': 'clarity', 'raw_intents': ['clarity', 'fluency', 'clarity']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': ' Besides of the early-announced "Wow!', 'after_edit': ' Besides of the previously announced "Wow!', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '106096', 'context': 'noicon|Launch of the Opening Ceremony. 2008 The 27th Young Designers\' Exhibition, opened on May 15 at the Taipei World Trade Center and will close at Sunday May 18. It features participation by 87 academic groups in Taiwan and 20 groups from United States, United Kingdom, Italy, Netherlands, New Zealand, and Australia to showcase various achievements in industrial design. It is recognized by the International Council of Societies of Industrial Design (ICSID) as the largest show of student creations. Not only several design competitions, sponsors like International Forum Design (iF), EPSON, MUJI (in Japanese: 無印良品, Mujirushi Ryōhin), Tsann Kuen Trans-nation Group will also showcase different solutions for  design, creative, and cultural industries. In addition , Taiwan Design Center, the show organizer, also designed several site events like "On-line Graduate Season Show", " Carrer Match-up", "Creative and Cultural Showcase and Performance", "Seminars of YODEX 2008" to link the actual exhibition with  on-line exhibition. Besides of the early-announced "Wow! Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'domain': 'news', 'before_edit': ' Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" will also be announced this Saturday(May 17).', 'after_edit': ' Taiwan Design Award", winners from "2008 Young Designers\' Competition" and "2008 YODEX Interior Design Competition" were announced on Saturday, May 17.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'meaning-changed']}
{'doc_id': '15187', 'context': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26. Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies. Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month. The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too . The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'domain': 'news', 'before_edit': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26.', 'after_edit': 'One hundred students from Serbia who decide to pursue graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '15187', 'context': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26. Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies. Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month. The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too . The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'domain': 'news', 'before_edit': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26.', 'after_edit': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for Young Talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '15187', 'context': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26. Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies. Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month. The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too . The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'domain': 'news', 'before_edit': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26.', 'after_edit': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who wish to apply must have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '15187', 'context': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26. Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies. Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month. The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too . The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'domain': 'news', 'before_edit': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26.', 'after_edit': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least an 8.5 GPA on 5 to 10 scale, and have to be younger then 26.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '15187', 'context': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26. Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies. Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month. The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too . The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'domain': 'news', 'before_edit': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26.', 'after_edit': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and must be younger then 26.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '15187', 'context': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26. Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies. Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month. The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too . The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'domain': 'news', 'before_edit': ' Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies.', 'after_edit': ' Additionally, students must commit to work in Serbia for at least 5 years after finishing their graduate studies.', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '15187', 'context': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26. Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies. Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month. The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too . The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'domain': 'news', 'before_edit': ' Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month.', 'after_edit': " Scholarship will be awarded based on the student's GPA and the prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '15187', 'context': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26. Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies. Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month. The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too . The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'domain': 'news', 'before_edit': ' Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month.', 'after_edit': ' Winners will be decided based on their GPA and prestige of the university to which they apply . Also, one thousand university seniors will be awarded 250 a month.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '15187', 'context': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26. Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies. Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month. The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too . The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'domain': 'news', 'before_edit': ' The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too .', 'after_edit': ' The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account as well .', 'label': 'style', 'raw_intents': ['style', 'style', 'style']}
{'doc_id': '15187', 'context': 'One hundred students from Serbia who decide to start graduate studies abroad will get 15,000 a year scholarships from the Fund for young talents . Students who can apply need to have at least  8.5 GPA on 5 to 10 scale, and have to be younger then 26. Another condition is that they have to work in Serbia for at least 5 years after finishing their graduate studies. Winners will be decided based on their GPA and prestige of the university they apply to . Also, one thousand university seniors will be awarded 250 a month. The secretary of the Serbian Student Union, Marko Milovanovi, is satisfied with this action, but also notes that it would be better if economic needs of students were taken into account too . The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'domain': 'news', 'before_edit': ' The Fund for young talents will cover about 2.5\\% of students in Serbia.', 'after_edit': ' The Fund for Young Talents will provide funding to about 2.5\\% of students in Serbia.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '1928792', 'context': 'Flag of ISIL. A video was released yesterday purporting to show the execution of 21 by supporters of (ISIL)  . The video shows the prisoners being beheaded in a location apparently near in . The captives, all wearing orange in the video, were picked up in , a coastal town in Libya, during December and January. The video indicates that the Christians were targeted by ISIL because of their religion. The Coptic Orthodox Church stated they were "confident" justice would be carried out . \'s President stated: "Egypt and the whole world are in a fierce battle with extremist groups carrying extremist ideology and sharing the same goals".', 'domain': 'news', 'before_edit': ' A video was released yesterday purporting to show the execution of 21 by supporters of (ISIL)  .', 'after_edit': ' A video  purporting to show the execution of 21 by supporters of (ISIL)  .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '1928792', 'context': 'Flag of ISIL. A video was released yesterday purporting to show the execution of 21 by supporters of (ISIL)  . The video shows the prisoners being beheaded in a location apparently near in . The captives, all wearing orange in the video, were picked up in , a coastal town in Libya, during December and January. The video indicates that the Christians were targeted by ISIL because of their religion. The Coptic Orthodox Church stated they were "confident" justice would be carried out . \'s President stated: "Egypt and the whole world are in a fierce battle with extremist groups carrying extremist ideology and sharing the same goals".', 'domain': 'news', 'before_edit': ' A video was released yesterday purporting to show the execution of 21 by supporters of (ISIL)  .', 'after_edit': ' A video was released yesterday purporting to show the execution of 21 by supporters of (ISIL) has been released yesterday .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '1928792', 'context': 'Flag of ISIL. A video was released yesterday purporting to show the execution of 21 by supporters of (ISIL)  . The video shows the prisoners being beheaded in a location apparently near in . The captives, all wearing orange in the video, were picked up in , a coastal town in Libya, during December and January. The video indicates that the Christians were targeted by ISIL because of their religion. The Coptic Orthodox Church stated they were "confident" justice would be carried out . \'s President stated: "Egypt and the whole world are in a fierce battle with extremist groups carrying extremist ideology and sharing the same goals".', 'domain': 'news', 'before_edit': ' The video shows the prisoners being beheaded in a location apparently near in .', 'after_edit': ' The video shows them being beheaded in a location apparently near in .', 'label': 'style', 'raw_intents': ['style', 'clarity', 'style']}
{'doc_id': '1928792', 'context': 'Flag of ISIL. A video was released yesterday purporting to show the execution of 21 by supporters of (ISIL)  . The video shows the prisoners being beheaded in a location apparently near in . The captives, all wearing orange in the video, were picked up in , a coastal town in Libya, during December and January. The video indicates that the Christians were targeted by ISIL because of their religion. The Coptic Orthodox Church stated they were "confident" justice would be carried out . \'s President stated: "Egypt and the whole world are in a fierce battle with extremist groups carrying extremist ideology and sharing the same goals".', 'domain': 'news', 'before_edit': ' The captives, all wearing orange in the video, were picked up in , a coastal town in Libya, during December and January.', 'after_edit': ' The captives, all shown being executed in orange in the video, were picked up in , a coastal town in Libya, during December and January.', 'label': 'style', 'raw_intents': ['style', 'style', 'style']}
{'doc_id': '1928792', 'context': 'Flag of ISIL. A video was released yesterday purporting to show the execution of 21 by supporters of (ISIL)  . The video shows the prisoners being beheaded in a location apparently near in . The captives, all wearing orange in the video, were picked up in , a coastal town in Libya, during December and January. The video indicates that the Christians were targeted by ISIL because of their religion. The Coptic Orthodox Church stated they were "confident" justice would be carried out . \'s President stated: "Egypt and the whole world are in a fierce battle with extremist groups carrying extremist ideology and sharing the same goals".', 'domain': 'news', 'before_edit': ' The video indicates that the Christians were targeted by ISIL because of their religion.', 'after_edit': ' The video asserts the Christians were targeted by ISIL because of their religion.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '1928792', 'context': 'Flag of ISIL. A video was released yesterday purporting to show the execution of 21 by supporters of (ISIL)  . The video shows the prisoners being beheaded in a location apparently near in . The captives, all wearing orange in the video, were picked up in , a coastal town in Libya, during December and January. The video indicates that the Christians were targeted by ISIL because of their religion. The Coptic Orthodox Church stated they were "confident" justice would be carried out . \'s President stated: "Egypt and the whole world are in a fierce battle with extremist groups carrying extremist ideology and sharing the same goals".', 'domain': 'news', 'before_edit': ' The Coptic Orthodox Church stated they were "confident" justice would be carried out .', 'after_edit': ' The Coptic Orthodox Church stated they were "confident" justice would be done on those who executed their followers .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'meaning-changed']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery.', 'after_edit': 'The for , according reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery.', 'after_edit': 'The for by reports disputed by the Russian Consul ; shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': ' As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred.', 'after_edit': '  lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred.', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': ' As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred.', 'after_edit': ' As reported, lawyer Marcos Cesar Feres Braga , working at the , was driving with his family through the suburb of near the main complex when the incident occurred.', 'label': 'others', 'raw_intents': ['others', 'others', 'others']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': ' According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood.', 'after_edit': ' According to reports from the Rio newspaper , a 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood.', 'label': 'others', 'raw_intents': ['others', 'others', 'others']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': ' According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood.', 'after_edit': ' According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic , due to the procession in a nearby neighbourhood.', 'label': 'others', 'raw_intents': ['others', 'others', 'others']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': " Braga, trained and considered an expert according to reports in , took control of the attacker's firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene.", 'after_edit': " Braga, trained  in , took control of the attacker's firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': " Braga, trained and considered an expert according to reports in , took control of the attacker's firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene.", 'after_edit': " Braga, trained and considered an expert according to reports in , took control of the attacker's firearm and proceeded to shoot and kill the alleged attacker, while the second suspect fled the scene.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': ' "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots.', 'after_edit': ' "The vice consul got into a physical struggle with the assailant , and during the fight the aggressor\'s weapon fired shots.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': " However, Russia's Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident.", 'after_edit': "  Russia's Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident.", 'label': 'coherence', 'raw_intents': ['clarity', 'coherence', 'coherence']}
{'doc_id': '2795820', 'context': 'The for by reports disputed by the Russian Consul  shot and killed an attacker on Thursday, after two individuals attempted to commit a robbery. As reported, lawyer Marcos Cesar Feres Braga so-named by the newspaper working for the , was driving with his family through the suburb of near the main complex when the incident occurred. According to reports from the Rio newspaper , the 60-year-old man, his wife, and his daughter were stopped in traffic  due to the procession in a nearby neighbourhood. Two individuals on motorbikes approached and used a gun to smash the car window. Braga, trained and considered an expert according to reports in , took control of the attacker\'s firearm and proceeded to shoot dead the alleged attacker, while the second suspect fled the scene. Rio de Janeiro\'s homicide branch has released a statement in relation to the incident. "The vice consul got into a physical struggle with the assailant  and during the fight the aggressor\'s weapon fired shots. The assailant died of his wounds on the spot." However, Russia\'s Consul General Vladimir Tokmakov has released a statement claiming no Russian diplomats or employees of the consulate general were involved in the incident. "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'domain': 'news', 'before_edit': ' "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ".', 'after_edit': ' "Information circulating in the Brazilian press about the alleged shooting [...] of a Brazilian national by a Russian diplomat during an armed robbery does not reflect reality ."', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '31572', 'context': 'José Sócrates, prime minister of Portugal, said he was "satisfied with the decision of Volkswagen to produce a new model in the factory of Palmela," and considered that the decision, "reflected the confidence [of the investors] in the portuguese economy." Volkswagen will reveal the new model to be produced next week. By 2008  the factory at Palmela will be only producing the multi-purpose vehicle Sharan and the Eos models. With the end of the production of the multi-purpose vehicle, the factory needs to garantee new product lines, since the new Eos is insufficient to maintain the current 2,790 workers.', 'domain': 'news', 'before_edit': 'José Sócrates, prime minister of Portugal, said he was "satisfied with the decision of Volkswagen to produce a new model in the factory of Palmela," and considered that the decision, "reflected the confidence [of the investors] in the portuguese economy."', 'after_edit': 'José Sócrates, prime minister of Portugal, said he was "satisfied with the decision of Volkswagen to produce a new model in the factory of Palmela," and considered that the decision, "reflected the confidence [of the investors] in the Portuguese economy."', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '31572', 'context': 'José Sócrates, prime minister of Portugal, said he was "satisfied with the decision of Volkswagen to produce a new model in the factory of Palmela," and considered that the decision, "reflected the confidence [of the investors] in the portuguese economy." Volkswagen will reveal the new model to be produced next week. By 2008  the factory at Palmela will be only producing the multi-purpose vehicle Sharan and the Eos models. With the end of the production of the multi-purpose vehicle, the factory needs to garantee new product lines, since the new Eos is insufficient to maintain the current 2,790 workers.', 'domain': 'news', 'before_edit': ' By 2008  the factory at Palmela will be only producing the multi-purpose vehicle Sharan and the Eos models.', 'after_edit': ' By 2008 , the factory at Palmela will be only producing the multi-purpose vehicle Sharan and the Eos models.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '31572', 'context': 'José Sócrates, prime minister of Portugal, said he was "satisfied with the decision of Volkswagen to produce a new model in the factory of Palmela," and considered that the decision, "reflected the confidence [of the investors] in the portuguese economy." Volkswagen will reveal the new model to be produced next week. By 2008  the factory at Palmela will be only producing the multi-purpose vehicle Sharan and the Eos models. With the end of the production of the multi-purpose vehicle, the factory needs to garantee new product lines, since the new Eos is insufficient to maintain the current 2,790 workers.', 'domain': 'news', 'before_edit': ' By 2008  the factory at Palmela will be only producing the multi-purpose vehicle Sharan and the Eos models.', 'after_edit': ' By 2008  the factory at Palmela will only be producing the multi-purpose vehicle Sharan and the Eos models.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '31572', 'context': 'José Sócrates, prime minister of Portugal, said he was "satisfied with the decision of Volkswagen to produce a new model in the factory of Palmela," and considered that the decision, "reflected the confidence [of the investors] in the portuguese economy." Volkswagen will reveal the new model to be produced next week. By 2008  the factory at Palmela will be only producing the multi-purpose vehicle Sharan and the Eos models. With the end of the production of the multi-purpose vehicle, the factory needs to garantee new product lines, since the new Eos is insufficient to maintain the current 2,790 workers.', 'domain': 'news', 'before_edit': ' With the end of the production of the multi-purpose vehicle, the factory needs to garantee new product lines, since the new Eos is insufficient to maintain the current 2,790 workers.', 'after_edit': ' With the end of the production of the multi-purpose vehicle, the factory needs to guarantee new product lines, since the new Eos is insufficient to maintain the current 2,790 workers.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '31572', 'context': 'José Sócrates, prime minister of Portugal, said he was "satisfied with the decision of Volkswagen to produce a new model in the factory of Palmela," and considered that the decision, "reflected the confidence [of the investors] in the portuguese economy." Volkswagen will reveal the new model to be produced next week. By 2008  the factory at Palmela will be only producing the multi-purpose vehicle Sharan and the Eos models. With the end of the production of the multi-purpose vehicle, the factory needs to garantee new product lines, since the new Eos is insufficient to maintain the current 2,790 workers.', 'domain': 'news', 'before_edit': ' With the end of the production of the multi-purpose vehicle, the factory needs to garantee new product lines, since the new Eos is insufficient to maintain the current 2,790 workers.', 'after_edit': ' With the end of the production of the multi-purpose vehicle, the factory needs to garantee new product lines, since the new Eos is not sufficient to maintain the current 2,790 workers.', 'label': 'style', 'raw_intents': ['style', 'style', 'style']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': 'David Parker today resigned from Cabinet;', 'after_edit': 'David Parker resigned today from Cabinet,', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited.', 'after_edit': ' a day after he resigned his position as Attorney-General and after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited.', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'fluency']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited.', 'after_edit': ' a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration  with the Companies Office on behalf of Queens Park Mews Limited.', 'label': 'fluency', 'raw_intents': ['others', 'fluency', 'fluency']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' The declaration said that ;', 'after_edit': ' The declaration said that ', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter.', 'after_edit': ' the three shareholders had unanimously agreed not to appoint an auditor for the company ; but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter.', 'after_edit': ' the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder , Russell Hyslop, had never been consulted about the matter.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' Parker was saying early this morning , that he would persist to hold his other portfolios.', 'after_edit': ' Parker was saying early this morning  that he would persist to hold his other portfolios.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' Parker was saying early this morning , that he would persist to hold his other portfolios.', 'after_edit': ' Parker was saying early this morning , that he would persist in holding his other portfolios.', 'label': 'clarity', 'raw_intents': ['clarity', 'fluency', 'clarity']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting.', 'after_edit': ' Prime Minister Helen Clark released the news shortly before 11 a.m. (NZST) as she walked into a caucus meeting.', 'label': 'fluency', 'raw_intents': ['fluency', 'meaning-changed', 'fluency']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act.', 'after_edit': ' She said, "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'after_edit': ' The maximum penalty  under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'label': 'clarity', 'raw_intents': ['coherence', 'clarity', 'clarity']}
{'doc_id': '36498', 'context': 'David Parker today resigned from Cabinet; a day after he resigned his position as Attorney-General , after publicity around his filing of an incorrect declaration , with the Companies Office on behalf of Queens Park Mews Limited. The declaration said that ; the three shareholders had unanimously agreed not to appoint an auditor for the company , but according to Investigate Magazine, another shareholder  Russell Hyslop, had never been consulted about the matter. Parker was saying early this morning , that he would persist to hold his other portfolios. Prime Minister Helen Clark released the news shortly before 11am (NZST) as she walked into a caucus meeting. She says "I have this morning accepted Mr Parker\'s resignation from all his portfolios." Knowingly authorising a false statement is an offence under the Companies Act. The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'domain': 'news', 'before_edit': ' The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or 5 years imprisonment.', 'after_edit': ' The maximum penalty punishable under section 373(4) for filing a false return is a fine of $200,000 or five years of imprisonment.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'fluency']}
{'doc_id': '47188', 'context': "According to BBC NEWS , Merseyside police have apparently been called in and are working with the Crown Prosecution Service. As yet there has been no confirmation that the blog was acting in or had intended to act in a criminal manner. An Earlier article in the Liverpool Echo, had however suggested that some employees of Liverpool City Council felt postings on the blog may be defamatory, a civil rather  criminal matter. Liverpool City Council, has so far declined to comment on the sites dissaperance , although the site had been previosuly blocked from the council's IT systems.", 'domain': 'news', 'before_edit': 'According to BBC NEWS , Merseyside police have apparently been called in and are working with the Crown Prosecution Service.', 'after_edit': 'According to BBC News , Merseyside police have apparently been called in and are working with the Crown Prosecution Service.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '47188', 'context': "According to BBC NEWS , Merseyside police have apparently been called in and are working with the Crown Prosecution Service. As yet there has been no confirmation that the blog was acting in or had intended to act in a criminal manner. An Earlier article in the Liverpool Echo, had however suggested that some employees of Liverpool City Council felt postings on the blog may be defamatory, a civil rather  criminal matter. Liverpool City Council, has so far declined to comment on the sites dissaperance , although the site had been previosuly blocked from the council's IT systems.", 'domain': 'news', 'before_edit': ' An Earlier article in the Liverpool Echo, had however suggested that some employees of Liverpool City Council felt postings on the blog may be defamatory, a civil rather  criminal matter.', 'after_edit': ' An earlier article in the Liverpool Echo, had however suggested that some employees of Liverpool City Council felt postings on the blog may be defamatory, a civil rather  criminal matter.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '47188', 'context': "According to BBC NEWS , Merseyside police have apparently been called in and are working with the Crown Prosecution Service. As yet there has been no confirmation that the blog was acting in or had intended to act in a criminal manner. An Earlier article in the Liverpool Echo, had however suggested that some employees of Liverpool City Council felt postings on the blog may be defamatory, a civil rather  criminal matter. Liverpool City Council, has so far declined to comment on the sites dissaperance , although the site had been previosuly blocked from the council's IT systems.", 'domain': 'news', 'before_edit': ' An Earlier article in the Liverpool Echo, had however suggested that some employees of Liverpool City Council felt postings on the blog may be defamatory, a civil rather  criminal matter.', 'after_edit': ' An Earlier article in the Liverpool Echo, had however suggested that some employees of Liverpool City Council felt postings on the blog may be defamatory, a civil rather than criminal matter.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '47188', 'context': "According to BBC NEWS , Merseyside police have apparently been called in and are working with the Crown Prosecution Service. As yet there has been no confirmation that the blog was acting in or had intended to act in a criminal manner. An Earlier article in the Liverpool Echo, had however suggested that some employees of Liverpool City Council felt postings on the blog may be defamatory, a civil rather  criminal matter. Liverpool City Council, has so far declined to comment on the sites dissaperance , although the site had been previosuly blocked from the council's IT systems.", 'domain': 'news', 'before_edit': " Liverpool City Council, has so far declined to comment on the sites dissaperance , although the site had been previosuly blocked from the council's IT systems.", 'after_edit': " Liverpool City Council, has so far declined to comment on the sites disappearance , although the site had been previosuly blocked from the council's IT systems.", 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '47188', 'context': "According to BBC NEWS , Merseyside police have apparently been called in and are working with the Crown Prosecution Service. As yet there has been no confirmation that the blog was acting in or had intended to act in a criminal manner. An Earlier article in the Liverpool Echo, had however suggested that some employees of Liverpool City Council felt postings on the blog may be defamatory, a civil rather  criminal matter. Liverpool City Council, has so far declined to comment on the sites dissaperance , although the site had been previosuly blocked from the council's IT systems.", 'domain': 'news', 'before_edit': " Liverpool City Council, has so far declined to comment on the sites dissaperance , although the site had been previosuly blocked from the council's IT systems.", 'after_edit': " Liverpool City Council, has so far declined to comment on the sites dissaperance , although the site had been previously blocked from the council's IT systems.", 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '49966', 'context': "Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press. The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors. Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records. It is illegal in the State of California. The aim of the HP investigation was to find out which director(s) had leaked information to journalists. VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry. The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So", 'domain': 'news', 'before_edit': 'Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press.', 'after_edit': 'Hewlett-Packard (HP) is embroiled in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '49966', 'context': "Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press. The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors. Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records. It is illegal in the State of California. The aim of the HP investigation was to find out which director(s) had leaked information to journalists. VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry. The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So", 'domain': 'news', 'before_edit': 'Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press.', 'after_edit': 'Hewlett-Packard is in a scandal over the alleged illegal investigation of its own board members over leaks from the board to the press.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '49966', 'context': "Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press. The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors. Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records. It is illegal in the State of California. The aim of the HP investigation was to find out which director(s) had leaked information to journalists. VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry. The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So", 'domain': 'news', 'before_edit': " The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors.", 'after_edit': ' The companys admits that it hired private investigators and that they used pretexting to get private phone records of its own directors.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '49966', 'context': "Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press. The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors. Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records. It is illegal in the State of California. The aim of the HP investigation was to find out which director(s) had leaked information to journalists. VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry. The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So", 'domain': 'news', 'before_edit': ' Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records.', 'after_edit': " 'Pretexting' is where someone pretends to be the customer when calling the phone company to get their phone records.", 'label': 'fluency', 'raw_intents': ['others', 'fluency', 'fluency']}
{'doc_id': '49966', 'context': "Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press. The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors. Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records. It is illegal in the State of California. The aim of the HP investigation was to find out which director(s) had leaked information to journalists. VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry. The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So", 'domain': 'news', 'before_edit': ' It is illegal in the State of California.', 'after_edit': ' It is illegal in the state of California.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '49966', 'context': "Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press. The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors. Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records. It is illegal in the State of California. The aim of the HP investigation was to find out which director(s) had leaked information to journalists. VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry. The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So", 'domain': 'news', 'before_edit': " VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry.", 'after_edit': " Venture Capitalist investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'fluency']}
{'doc_id': '49966', 'context': "Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press. The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors. Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records. It is illegal in the State of California. The aim of the HP investigation was to find out which director(s) had leaked information to journalists. VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry. The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So", 'domain': 'news', 'before_edit': " VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry.", 'after_edit': " VC investor Tom Perkins resigned from the board on May 22, 2006 over the issue and requested that HP look into the methods used in it's leak inquiry.", 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'style']}
{'doc_id': '49966', 'context': "Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press. The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors. Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records. It is illegal in the State of California. The aim of the HP investigation was to find out which director(s) had leaked information to journalists. VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry. The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So", 'domain': 'news', 'before_edit': " VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry.", 'after_edit': ' VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in its leak inquiry.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '49966', 'context': "Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press. The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors. Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records. It is illegal in the State of California. The aim of the HP investigation was to find out which director(s) had leaked information to journalists. VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry. The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So", 'domain': 'news', 'before_edit': ' The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So', 'after_edit': ' The Federal Communications Commission is looking into phone companies who may have supplied the phone records illegally under California law. So', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '49966', 'context': "Hewlett-Packard is in a scandal over the alegged illegal investigation of its own board members over leaks from the board to the press. The companys admits that it hired private investigators and that they used pretexting to get private phone records of it's own directors. Pretexting is where someone pretends to be the customer when calling the phone company to get their phone records. It is illegal in the State of California. The aim of the HP investigation was to find out which director(s) had leaked information to journalists. VC investor Tom Perkins resigned from the board on May 22nd 2006 over the issue and requested that HP look into the methods used in it's leak inquiry. The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So", 'domain': 'news', 'before_edit': ' The FCC is looking into phone companies who may have supplied the phone records illegally under California law. So', 'after_edit': ' The FCC is looking into phone companies who may have supplied the phone records illegally under California law. ', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'clarity']}
{'doc_id': '54370', 'context': ' 73-year old retired pastor, Reverend Roland Weisselberg, burned himself alive during Reformation Day services on Tuesday in Erfurt, Germany. His self-immolation was apparently to protest against the spread of Islam, which he felt the Protestant church should take more seriously. His last words were "Jesus and Oskar," which is believed to refer to Oskar Bruesewitz, a priest who burned himself alive to protest the Communist government of East Germany. He was transported to a burns unit in Halle but died en route. Reverend Weisselberg had lived under Communism in East Germany, and had been a publisher in his former vocation. Axel Noack, Bishop of Saxony, said that he was shocked at Rev. Weisselberg\'s self-immolation, stressing that Christians could not accept a "clash of cultures". He confessed that the issue of Islam had been sidelined within the Church and was only spoken about in private. There were few Muslims there with whom they could engage in dialogue, Noack claimed .', 'domain': 'news', 'before_edit': '', 'after_edit': 'A', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'others']}
{'doc_id': '54370', 'context': ' 73-year old retired pastor, Reverend Roland Weisselberg, burned himself alive during Reformation Day services on Tuesday in Erfurt, Germany. His self-immolation was apparently to protest against the spread of Islam, which he felt the Protestant church should take more seriously. His last words were "Jesus and Oskar," which is believed to refer to Oskar Bruesewitz, a priest who burned himself alive to protest the Communist government of East Germany. He was transported to a burns unit in Halle but died en route. Reverend Weisselberg had lived under Communism in East Germany, and had been a publisher in his former vocation. Axel Noack, Bishop of Saxony, said that he was shocked at Rev. Weisselberg\'s self-immolation, stressing that Christians could not accept a "clash of cultures". He confessed that the issue of Islam had been sidelined within the Church and was only spoken about in private. There were few Muslims there with whom they could engage in dialogue, Noack claimed .', 'domain': 'news', 'before_edit': ' His self-immolation was apparently to protest against the spread of Islam, which he felt the Protestant church should take more seriously.', 'after_edit': ' His self-immolation was apparently in protest against the spread of Islam, which he felt the Protestant church should take more seriously.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '54370', 'context': ' 73-year old retired pastor, Reverend Roland Weisselberg, burned himself alive during Reformation Day services on Tuesday in Erfurt, Germany. His self-immolation was apparently to protest against the spread of Islam, which he felt the Protestant church should take more seriously. His last words were "Jesus and Oskar," which is believed to refer to Oskar Bruesewitz, a priest who burned himself alive to protest the Communist government of East Germany. He was transported to a burns unit in Halle but died en route. Reverend Weisselberg had lived under Communism in East Germany, and had been a publisher in his former vocation. Axel Noack, Bishop of Saxony, said that he was shocked at Rev. Weisselberg\'s self-immolation, stressing that Christians could not accept a "clash of cultures". He confessed that the issue of Islam had been sidelined within the Church and was only spoken about in private. There were few Muslims there with whom they could engage in dialogue, Noack claimed .', 'domain': 'news', 'before_edit': ' His last words were "Jesus and Oskar," which is believed to refer to Oskar Bruesewitz, a priest who burned himself alive to protest the Communist government of East Germany.', 'after_edit': ' His last words were "Jesus and Oskar," , believed to a reference to Oskar Bruesewitz, a priest who burned himself alive to protest the Communist government of East Germany.', 'label': 'style', 'raw_intents': ['style', 'style', 'clarity']}
{'doc_id': '54370', 'context': ' 73-year old retired pastor, Reverend Roland Weisselberg, burned himself alive during Reformation Day services on Tuesday in Erfurt, Germany. His self-immolation was apparently to protest against the spread of Islam, which he felt the Protestant church should take more seriously. His last words were "Jesus and Oskar," which is believed to refer to Oskar Bruesewitz, a priest who burned himself alive to protest the Communist government of East Germany. He was transported to a burns unit in Halle but died en route. Reverend Weisselberg had lived under Communism in East Germany, and had been a publisher in his former vocation. Axel Noack, Bishop of Saxony, said that he was shocked at Rev. Weisselberg\'s self-immolation, stressing that Christians could not accept a "clash of cultures". He confessed that the issue of Islam had been sidelined within the Church and was only spoken about in private. There were few Muslims there with whom they could engage in dialogue, Noack claimed .', 'domain': 'news', 'before_edit': ' Reverend Weisselberg had lived under Communism in East Germany, and had been a publisher in his former vocation.', 'after_edit': ' Rev. Weisselberg had lived under Communism in East Germany, and had been a publisher in his former vocation.', 'label': 'clarity', 'raw_intents': ['fluency', 'clarity', 'clarity']}
{'doc_id': '54370', 'context': ' 73-year old retired pastor, Reverend Roland Weisselberg, burned himself alive during Reformation Day services on Tuesday in Erfurt, Germany. His self-immolation was apparently to protest against the spread of Islam, which he felt the Protestant church should take more seriously. His last words were "Jesus and Oskar," which is believed to refer to Oskar Bruesewitz, a priest who burned himself alive to protest the Communist government of East Germany. He was transported to a burns unit in Halle but died en route. Reverend Weisselberg had lived under Communism in East Germany, and had been a publisher in his former vocation. Axel Noack, Bishop of Saxony, said that he was shocked at Rev. Weisselberg\'s self-immolation, stressing that Christians could not accept a "clash of cultures". He confessed that the issue of Islam had been sidelined within the Church and was only spoken about in private. There were few Muslims there with whom they could engage in dialogue, Noack claimed .', 'domain': 'news', 'before_edit': ' There were few Muslims there with whom they could engage in dialogue, Noack claimed .', 'after_edit': ' There were few Muslims there with whom they could engage in dialogue, Noack said .', 'label': 'style', 'raw_intents': ['clarity', 'style', 'style']}
{'doc_id': '55265', 'context': "Nine people are confirmed dead in a fire in a leather bag facory in Kolkata's South 24 Pargana district in Topsia area. Eighteen people were seriously injured and admitted to  National Medical College and Hospital. Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 am when the victims were inside the factory and all the doors were closed. Director General of Fire Brigade Gopal Bhattacharjee said that it was an illegal factory in Kolkata located at third floor of the building. At the time of fire, all the premises were closed and workers could not come out from the factory. The cause of the fire was yet to be ascertained. Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette causing fire in the factory .", 'domain': 'news', 'before_edit': "Nine people are confirmed dead in a fire in a leather bag facory in Kolkata's South 24 Pargana district in Topsia area.", 'after_edit': "Nine people are confirmed dead in a fire in a leather bag factory in Kolkata's South 24 Pargana district in Topsia area.", 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '55265', 'context': "Nine people are confirmed dead in a fire in a leather bag facory in Kolkata's South 24 Pargana district in Topsia area. Eighteen people were seriously injured and admitted to  National Medical College and Hospital. Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 am when the victims were inside the factory and all the doors were closed. Director General of Fire Brigade Gopal Bhattacharjee said that it was an illegal factory in Kolkata located at third floor of the building. At the time of fire, all the premises were closed and workers could not come out from the factory. The cause of the fire was yet to be ascertained. Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette causing fire in the factory .", 'domain': 'news', 'before_edit': ' Eighteen people were seriously injured and admitted to  National Medical College and Hospital.', 'after_edit': ' Eighteen people were seriously injured and admitted to the National Medical College and Hospital.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '55265', 'context': "Nine people are confirmed dead in a fire in a leather bag facory in Kolkata's South 24 Pargana district in Topsia area. Eighteen people were seriously injured and admitted to  National Medical College and Hospital. Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 am when the victims were inside the factory and all the doors were closed. Director General of Fire Brigade Gopal Bhattacharjee said that it was an illegal factory in Kolkata located at third floor of the building. At the time of fire, all the premises were closed and workers could not come out from the factory. The cause of the fire was yet to be ascertained. Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette causing fire in the factory .", 'domain': 'news', 'before_edit': ' Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 am when the victims were inside the factory and all the doors were closed.', 'after_edit': ' Inspector General of Police (Law and Order) Raj Kanojia said that the fire broke out at about 3 am when the victims were inside the factory and all the doors were closed.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '55265', 'context': "Nine people are confirmed dead in a fire in a leather bag facory in Kolkata's South 24 Pargana district in Topsia area. Eighteen people were seriously injured and admitted to  National Medical College and Hospital. Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 am when the victims were inside the factory and all the doors were closed. Director General of Fire Brigade Gopal Bhattacharjee said that it was an illegal factory in Kolkata located at third floor of the building. At the time of fire, all the premises were closed and workers could not come out from the factory. The cause of the fire was yet to be ascertained. Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette causing fire in the factory .", 'domain': 'news', 'before_edit': ' Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 am when the victims were inside the factory and all the doors were closed.', 'after_edit': ' Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 a.m. local time, when the victims were inside the factory and all the doors were closed.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'meaning-changed']}
{'doc_id': '55265', 'context': "Nine people are confirmed dead in a fire in a leather bag facory in Kolkata's South 24 Pargana district in Topsia area. Eighteen people were seriously injured and admitted to  National Medical College and Hospital. Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 am when the victims were inside the factory and all the doors were closed. Director General of Fire Brigade Gopal Bhattacharjee said that it was an illegal factory in Kolkata located at third floor of the building. At the time of fire, all the premises were closed and workers could not come out from the factory. The cause of the fire was yet to be ascertained. Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette causing fire in the factory .", 'domain': 'news', 'before_edit': ' At the time of fire, all the premises were closed and workers could not come out from the factory.', 'after_edit': ' At the time of fire, all the exits were shut and workers could not come out from the factory.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '55265', 'context': "Nine people are confirmed dead in a fire in a leather bag facory in Kolkata's South 24 Pargana district in Topsia area. Eighteen people were seriously injured and admitted to  National Medical College and Hospital. Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 am when the victims were inside the factory and all the doors were closed. Director General of Fire Brigade Gopal Bhattacharjee said that it was an illegal factory in Kolkata located at third floor of the building. At the time of fire, all the premises were closed and workers could not come out from the factory. The cause of the fire was yet to be ascertained. Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette causing fire in the factory .", 'domain': 'news', 'before_edit': ' The cause of the fire was yet to be ascertained.', 'after_edit': ' The cause of the fire is yet to be ascertained.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '55265', 'context': "Nine people are confirmed dead in a fire in a leather bag facory in Kolkata's South 24 Pargana district in Topsia area. Eighteen people were seriously injured and admitted to  National Medical College and Hospital. Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 am when the victims were inside the factory and all the doors were closed. Director General of Fire Brigade Gopal Bhattacharjee said that it was an illegal factory in Kolkata located at third floor of the building. At the time of fire, all the premises were closed and workers could not come out from the factory. The cause of the fire was yet to be ascertained. Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette causing fire in the factory .", 'domain': 'news', 'before_edit': ' Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette causing fire in the factory .', 'after_edit': ' Kolkata Mayor Bikash Ranjan Bhattacharya suggested that a short circuit or an unattended cigarette causing fire in the factory .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '55265', 'context': "Nine people are confirmed dead in a fire in a leather bag facory in Kolkata's South 24 Pargana district in Topsia area. Eighteen people were seriously injured and admitted to  National Medical College and Hospital. Inspector General of Police (Law and Order) Raj Kanojia said that the fire was broken out at about 3 am when the victims were inside the factory and all the doors were closed. Director General of Fire Brigade Gopal Bhattacharjee said that it was an illegal factory in Kolkata located at third floor of the building. At the time of fire, all the premises were closed and workers could not come out from the factory. The cause of the fire was yet to be ascertained. Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette causing fire in the factory .", 'domain': 'news', 'before_edit': ' Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette causing fire in the factory .', 'after_edit': ' Kolkata Mayor Bikash Ranjan Bhattacharya predicted short circuit or an unattended cigarette may have caused the fire .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. It is also "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. A common definition lists five oceans, in descending order by area, the Pacific , Atlantic, Indian, Southern (Antarctic), and Arctic Oceans . Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth, forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.', 'domain': 'wiki', 'before_edit': ' It is also "any of the large bodies of water into which the great ocean is divided".', 'after_edit': ' Another definition is "any of the large bodies of water into which the great ocean is divided".', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. It is also "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. A common definition lists five oceans, in descending order by area, the Pacific , Atlantic, Indian, Southern (Antarctic), and Arctic Oceans . Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth, forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.', 'domain': 'wiki', 'before_edit': ' A common definition lists five oceans, in descending order by area, the Pacific , Atlantic, Indian, Southern (Antarctic), and Arctic Oceans .', 'after_edit': ' The ocean is divided into five oceans: Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic Oceans .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. It is also "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. A common definition lists five oceans, in descending order by area, the Pacific , Atlantic, Indian, Southern (Antarctic), and Arctic Oceans . Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth, forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.', 'domain': 'wiki', 'before_edit': ' A common definition lists five oceans, in descending order by area, the Pacific , Atlantic, Indian, Southern (Antarctic), and Arctic Oceans .', 'after_edit': ' A common definition lists five oceans, in descending order by area, the Pacific , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest) .', 'label': 'clarity', 'raw_intents': ['clarity', 'meaning-changed', 'clarity']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. Another definition is "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. The ocean is divided into five oceans : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest). Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.  Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions. The pelagic zone consists of the water column of the open ocean, and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem. Light only penetrates to a few hundred meters depth, so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' The ocean is divided into five oceans : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest).', 'after_edit': ' Separate names are used to identify five different areas of the ocean : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest).', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. Another definition is "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. The ocean is divided into five oceans : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest). Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.  Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions. The pelagic zone consists of the water column of the open ocean, and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem. Light only penetrates to a few hundred meters depth, so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' The ocean is divided into five oceans : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest).', 'after_edit': ' The ocean is divided into five oceans : Pacific (the largest)  Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest).', 'label': 'fluency', 'raw_intents': ['coherence', 'fluency', 'fluency']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. Another definition is "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. The ocean is divided into five oceans : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest). Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.  Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions. The pelagic zone consists of the water column of the open ocean, and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem. Light only penetrates to a few hundred meters depth, so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': " As the world's ocean is the principal component of Earth's hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.", 'after_edit': " The ocean is the principal component of Earth's hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.", 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'coherence']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. Another definition is "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. The ocean is divided into five oceans : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest). Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.  Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions. The pelagic zone consists of the water column of the open ocean, and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem. Light only penetrates to a few hundred meters depth, so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': " As the world's ocean is the principal component of Earth's hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.", 'after_edit': " As the world's ocean is the principal component of Earth's hydrosphere, and therefore integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.", 'label': 'clarity', 'raw_intents': ['clarity', 'coherence', 'clarity']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. Another definition is "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. The ocean is divided into five oceans : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest). Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.  Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions. The pelagic zone consists of the water column of the open ocean, and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem. Light only penetrates to a few hundred meters depth, so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': " As the world's ocean is the principal component of Earth's hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.", 'after_edit': " As the world's ocean is the principal component of Earth's hydrosphere, it is integral to life on Earth . The ocean is a huge heat reservoir - influences climate and weather patterns.", 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'coherence']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. Another definition is "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. The ocean is divided into five oceans : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest). Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.  Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions. The pelagic zone consists of the water column of the open ocean, and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem. Light only penetrates to a few hundred meters depth, so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': " As the world's ocean is the principal component of Earth's hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.", 'after_edit': " As the world's ocean is the principal component of Earth's hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir and influences climate and weather patterns.", 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'coherence']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. Another definition is "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. The ocean is divided into five oceans : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest). Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.  Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions. The pelagic zone consists of the water column of the open ocean, and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem. Light only penetrates to a few hundred meters depth, so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': '  Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions.', 'after_edit': ' the carbon cycle and the water cycle. Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions.', 'label': 'meaning-changed', 'raw_intents': ['coherence', 'meaning-changed', 'meaning-changed']}
{'doc_id': '18842359', 'context': 'The ocean (also the sea or the world ocean) is the body of salt water which covers approximately 71\\% of the surface of the Earth and contains 97\\% of Earth\'s water. Another definition is "any of the large bodies of water into which the great ocean is divided"."Ocean." Merriam-Webster.com Dictionary, Merriam-Webster, URL Accessed March 14, 2021. The ocean is divided into five oceans : Pacific (the largest) , Atlantic, Indian, Southern (Antarctic), and Arctic (the smallest). Seawater covers approximately of the Earth. As the world\'s ocean is the principal component of Earth\'s hydrosphere, it is integral to life on Earth , forms part of the carbon cycle and water cycle, and - as a huge heat reservoir - influences climate and weather patterns.  Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions. The pelagic zone consists of the water column of the open ocean, and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem. Light only penetrates to a few hundred meters depth, so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': '  Oceanographers divide the ocean into different vertical and horizontal zones defined by physical and biological conditions.', 'after_edit': '  Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth.', 'after_edit': ' The pelagic zone consists of the water column from surface to ocean floor throughout the open ocean , and can be divided into further regions categorized by light abundance and by depth.', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth.', 'after_edit': ' The pelagic zone consists of the water column of the open ocean . The water column is further categorized in other zones depending on light abundance and by depth.', 'label': 'coherence', 'raw_intents': ['clarity', 'coherence', 'coherence']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth.', 'after_edit': ' The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and  depth.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'clarity']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' The photic zone includes the oceans from the surface to a depth of 200m ;', 'after_edit': ' The photic zone includes the water column from the surface to a depth of 200m ;', 'label': 'clarity', 'raw_intents': ['clarity', 'style', 'clarity']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse.', 'after_edit': ' The photic zone includes the oceans from the surface to a depth of 200m , where photosynthesis can occur and is, therefore, the most biodiverse.', 'label': 'coherence', 'raw_intents': ['coherence', 'fluency', 'coherence']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' it is the region where photosynthesis can occur and is, therefore, the most biodiverse.', 'after_edit': ' it is the region where photosynthesis can occur . This makes the photic zone the most biodiverse.', 'label': 'clarity', 'raw_intents': ['clarity', 'coherence', 'clarity']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark.', 'after_edit': ' This upper sunlit zone of the ocean ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark.', 'label': 'clarity', 'raw_intents': ['clarity', 'style', 'clarity']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark.', 'after_edit': ' It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem since that is the origin of the food supply necessary to all life forms . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark.', 'label': 'meaning-changed', 'raw_intents': ['meaning-changed', 'meaning-changed', 'clarity']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark.', 'after_edit': ' It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a depth of a few hundred meters depth , so the remaining ocean below this is cold and dark.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark.', 'after_edit': ' It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters  , so the remaining ocean below this is cold and dark.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark.', 'after_edit': ' It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below  is cold and dark.', 'label': 'clarity', 'raw_intents': ['fluency', 'clarity', 'clarity']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'after_edit': ' The continental shelf where the ocean touches land is more shallow (a few hundred meters or less) and this region is most impacted by human activity.', 'label': 'clarity', 'raw_intents': ['style', 'clarity', 'clarity']}
{'doc_id': '18842359', 'context': 'Oceanographers divide the ocean into different vertical and horizontal zones based on physical and biological conditions. The pelagic zone consists of the water column of the open ocean , and can be divided into further regions categorized by light abundance and by depth. The photic zone includes the oceans from the surface to a depth of 200m ; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Photosynthesis by plants and microscopic algae (free floating phytoplankton) allows them to URLanic matter from chemical precursors including water and carbon dioxide. It is this upper sunlit ocean that creates the food supply that ultimately sustains most of the ocean ecosystem  . Light only penetrates to a  few hundred meters depth , so the remaining ocean below this is cold and dark. The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'domain': 'wiki', 'before_edit': ' The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and this region is most impacted by human activity.', 'after_edit': ' The continental shelf around the edge of the oceans is shallower (a few hundred meters or less) and experiences more impact from human activity.', 'label': 'style', 'raw_intents': ['clarity', 'style', 'style']}
{'doc_id': '43193013', 'context': 'During study of fluid mechanics, researchers attempted to give a correct vision of maritime currents. The origin of currents is owed to physical differences between various water masses, the main parameter being the difference of density that varies in function of the temperature and the concentration of salt . The study of these currents, combined with other factors such as tides (producing a change in sea level) and wind (originally swell) for understanding the marine hydrodynamics and different processes which are linked there as sediment movement and the climate balance.', 'domain': 'wiki', 'before_edit': 'During study of fluid mechanics, researchers attempted to give a correct vision of maritime currents. The origin of currents is owed to physical differences between various water masses, the main parameter being the difference of density that varies in function of the temperature and the concentration of salt .', 'after_edit': 'During study of fluid mechanics, researchers attempted to give a correct explanation of marine currents. Currents are caused by external driving forces such as wind, gravitational effects, coriolis forces and physical differences between various water masses, the main parameter being the difference of density that varies in function of the temperature and the concentration of salt .', 'label': 'clarity', 'raw_intents': ['clarity', 'others', 'clarity']}
{'doc_id': '43193013', 'context': 'During study of fluid mechanics, researchers attempted to give a correct vision of maritime currents. The origin of currents is owed to physical differences between various water masses, the main parameter being the difference of density that varies in function of the temperature and the concentration of salt . The study of these currents, combined with other factors such as tides (producing a change in sea level) and wind (originally swell) for understanding the marine hydrodynamics and different processes which are linked there as sediment movement and the climate balance.', 'domain': 'wiki', 'before_edit': ' The origin of currents is owed to physical differences between various water masses, the main parameter being the difference of density that varies in function of the temperature and the concentration of salt .', 'after_edit': ' The origin of currents is owed to physical differences between various water masses, the main parameter being the difference of density that varies in function of the temperature and salinity .', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '43193013', 'context': 'During study of fluid mechanics, researchers attempted to give a correct vision of maritime currents. The origin of currents is owed to physical differences between various water masses, the main parameter being the difference of density that varies in function of the temperature and the concentration of salt . The study of these currents, combined with other factors such as tides (producing a change in sea level) and wind (originally swell) for understanding the marine hydrodynamics and different processes which are linked there as sediment movement and the climate balance.', 'domain': 'wiki', 'before_edit': ' The study of these currents, combined with other factors such as tides (producing a change in sea level) and wind (originally swell) for understanding the marine hydrodynamics and different processes which are linked there as sediment movement and the climate balance.', 'after_edit': ' The study of  currents, combined with other factors such as tides (producing a change in sea level) and wind (originally swell) for understanding the marine hydrodynamics and different processes which are linked there as sediment movement and the climate balance.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'fluency']}
{'doc_id': '43193013', 'context': 'During study of fluid mechanics, researchers attempted to give a correct vision of maritime currents. The origin of currents is owed to physical differences between various water masses, the main parameter being the difference of density that varies in function of the temperature and the concentration of salt . The study of these currents, combined with other factors such as tides (producing a change in sea level) and wind (originally swell) for understanding the marine hydrodynamics and different processes which are linked there as sediment movement and the climate balance.', 'domain': 'wiki', 'before_edit': ' The study of these currents, combined with other factors such as tides (producing a change in sea level) and wind (originally swell) for understanding the marine hydrodynamics and different processes which are linked there as sediment movement and the climate balance.', 'after_edit': ' The study of these currents, combined with other factors such as tides and waves is relevant for understanding marine hydrodynamics and different processes which are linked there as sediment movement and the climate balance.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'style']}
{'doc_id': '43193013', 'context': 'During study of fluid mechanics, researchers attempted to give a correct vision of maritime currents. The origin of currents is owed to physical differences between various water masses, the main parameter being the difference of density that varies in function of the temperature and the concentration of salt . The study of these currents, combined with other factors such as tides (producing a change in sea level) and wind (originally swell) for understanding the marine hydrodynamics and different processes which are linked there as sediment movement and the climate balance.', 'domain': 'wiki', 'before_edit': ' The study of these currents, combined with other factors such as tides (producing a change in sea level) and wind (originally swell) for understanding the marine hydrodynamics and different processes which are linked there as sediment movement and the climate balance.', 'after_edit': ' The study of these currents, combined with other factors such as tides (producing a change in sea level) and wind (originally swell) for understanding the marine hydrodynamics and linked processes such as sediment transport and climate balance.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '4914567', 'context': 'A trainee is commonly known as an individual taking part in a trainee program or a graduate program within a company after having graduated from university or college . A trainee is an official employee of the firm that is being trained to the job they were originally hired for. Literally, a trainee is an employee in training. Trainee programs and graduate programs are arranged by private companies and public sector employers where the trainee is offered the possibility to take part 6 to 20 months training programs . During the duration of these programs, the trainee is expected to receive a salary as well as is expected to have full-time employment awaiting in the company when the program is over. Often used as an insurance measure by companies, firms typically will have a trainee period (23 months) where the person is still being evaluated after which an official decision to hire on a permanent basis is made.', 'domain': 'wiki', 'before_edit': 'A trainee is commonly known as an individual taking part in a trainee program or a graduate program within a company after having graduated from university or college .', 'after_edit': 'A trainee is commonly known as an individual taking part in a trainee program within URLanization after having graduated from university or college .', 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '4914567', 'context': 'A trainee is commonly known as an individual taking part in a trainee program or a graduate program within a company after having graduated from university or college . A trainee is an official employee of the firm that is being trained to the job they were originally hired for. Literally, a trainee is an employee in training. Trainee programs and graduate programs are arranged by private companies and public sector employers where the trainee is offered the possibility to take part 6 to 20 months training programs . During the duration of these programs, the trainee is expected to receive a salary as well as is expected to have full-time employment awaiting in the company when the program is over. Often used as an insurance measure by companies, firms typically will have a trainee period (23 months) where the person is still being evaluated after which an official decision to hire on a permanent basis is made.', 'domain': 'wiki', 'before_edit': 'A trainee is commonly known as an individual taking part in a trainee program or a graduate program within a company after having graduated from university or college .', 'after_edit': 'A trainee is commonly known as an individual taking part in a trainee program or a graduate program within a company after having graduated from higher and technical courses .', 'label': 'clarity', 'raw_intents': ['clarity', 'others', 'clarity']}
{'doc_id': '4914567', 'context': 'A trainee is commonly known as an individual taking part in a trainee program or a graduate program within a company after having graduated from university or college . A trainee is an official employee of the firm that is being trained to the job they were originally hired for. Literally, a trainee is an employee in training. Trainee programs and graduate programs are arranged by private companies and public sector employers where the trainee is offered the possibility to take part 6 to 20 months training programs . During the duration of these programs, the trainee is expected to receive a salary as well as is expected to have full-time employment awaiting in the company when the program is over. Often used as an insurance measure by companies, firms typically will have a trainee period (23 months) where the person is still being evaluated after which an official decision to hire on a permanent basis is made.', 'domain': 'wiki', 'before_edit': ' Trainee programs and graduate programs are arranged by private companies and public sector employers where the trainee is offered the possibility to take part 6 to 20 months training programs . During the duration of these programs, the trainee is expected to receive a salary as well as is expected to have full-time employment awaiting in the company when the program is over.', 'after_edit': ' Trainee programs  are arranged by private companies and public sector employers where the trainee is offered the possibility to take part 6 to 20 months training programs . During the duration of these programs, the trainee is expected to receive a salary as well as is expected to have full-time employment awaiting in the company when the program is over.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '4914567', 'context': 'A trainee is commonly known as an individual taking part in a trainee program or a graduate program within a company after having graduated from university or college . A trainee is an official employee of the firm that is being trained to the job they were originally hired for. Literally, a trainee is an employee in training. Trainee programs and graduate programs are arranged by private companies and public sector employers where the trainee is offered the possibility to take part 6 to 20 months training programs . During the duration of these programs, the trainee is expected to receive a salary as well as is expected to have full-time employment awaiting in the company when the program is over. Often used as an insurance measure by companies, firms typically will have a trainee period (23 months) where the person is still being evaluated after which an official decision to hire on a permanent basis is made.', 'domain': 'wiki', 'before_edit': ' Trainee programs and graduate programs are arranged by private companies and public sector employers where the trainee is offered the possibility to take part 6 to 20 months training programs . During the duration of these programs, the trainee is expected to receive a salary as well as is expected to have full-time employment awaiting in the company when the program is over.', 'after_edit': " Trainee programs and graduate programs are arranged by private companies and public sector employers where the trainee position has a varied duration depending on the company's program . During the duration of these programs, the trainee is expected to receive a salary as well as is expected to have full-time employment awaiting in the company when the program is over.", 'label': 'clarity', 'raw_intents': ['meaning-changed', 'clarity', 'clarity']}
{'doc_id': '4914567', 'context': 'A trainee is commonly known as an individual taking part in a trainee program or a graduate program within a company after having graduated from university or college . A trainee is an official employee of the firm that is being trained to the job they were originally hired for. Literally, a trainee is an employee in training. Trainee programs and graduate programs are arranged by private companies and public sector employers where the trainee is offered the possibility to take part 6 to 20 months training programs . During the duration of these programs, the trainee is expected to receive a salary as well as is expected to have full-time employment awaiting in the company when the program is over. Often used as an insurance measure by companies, firms typically will have a trainee period (23 months) where the person is still being evaluated after which an official decision to hire on a permanent basis is made.', 'domain': 'wiki', 'before_edit': ' Often used as an insurance measure by companies, firms typically will have a trainee period (23 months) where the person is still being evaluated after which an official decision to hire on a permanent basis is made.', 'after_edit': ' Often used as an insurance measure by companies, firms typically will have a trainee period  where the person is still being evaluated after which an official decision to hire on a permanent basis is made.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'clarity']}
{'doc_id': '51157723', 'context': 'Thenumgal Poulose Jacob is an Indian surgeon specialized in vascular surgery, and the founder Head of the Department of Vascular Surgery at Madras Medical College. Born in Aluva, in the south Indian state of Kerala  to Thenumgal Poulose and Mariam in a Malayali family, he did his under-graduate studies at UC College before graduating in medicine from Stanley Medical College, Chennai  and secured his MS degree from the same institution. He started his practice under government service at Madras Medical College where he helped establish the department of vascular surgery in 1978 and served as its founder head till his superannuation in 1993. The Government of India awarded him the fourth highest civilian honour of the Padma Shri, in 2014, for his contributions to medical science. He is married to Esther and the couple has a daughter, Siji Jacob, an academic and a son, Hasum Jacob Thenumgal, an engineer at TCS. He is doing private practice at T. P. Jacob Clinic in Royapuram, Chennai and is also a consultant vascular surgeon at MV Hospitals for Diabetes, Royapuram.', 'domain': 'wiki', 'before_edit': 'Thenumgal Poulose Jacob is an Indian surgeon specialized in vascular surgery, and the founder Head of the Department of Vascular Surgery at Madras Medical College.', 'after_edit': 'Thenumgal Poulose Jacob is an Indian surgeon specializing in vascular surgery, and the founder Head of the Department of Vascular Surgery at Madras Medical College.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'style']}
{'doc_id': '51157723', 'context': 'Thenumgal Poulose Jacob is an Indian surgeon specialized in vascular surgery, and the founder Head of the Department of Vascular Surgery at Madras Medical College. Born in Aluva, in the south Indian state of Kerala  to Thenumgal Poulose and Mariam in a Malayali family, he did his under-graduate studies at UC College before graduating in medicine from Stanley Medical College, Chennai  and secured his MS degree from the same institution. He started his practice under government service at Madras Medical College where he helped establish the department of vascular surgery in 1978 and served as its founder head till his superannuation in 1993. The Government of India awarded him the fourth highest civilian honour of the Padma Shri, in 2014, for his contributions to medical science. He is married to Esther and the couple has a daughter, Siji Jacob, an academic and a son, Hasum Jacob Thenumgal, an engineer at TCS. He is doing private practice at T. P. Jacob Clinic in Royapuram, Chennai and is also a consultant vascular surgeon at MV Hospitals for Diabetes, Royapuram.', 'domain': 'wiki', 'before_edit': ' Born in Aluva, in the south Indian state of Kerala  to Thenumgal Poulose and Mariam in a Malayali family, he did his under-graduate studies at UC College before graduating in medicine from Stanley Medical College, Chennai  and secured his MS degree from the same institution.', 'after_edit': ' Born in Aluva, in the south Indian state of Kerala , to Thenumgal Poulose and Mariam in a Malayali family, he did his under-graduate studies at UC College before graduating in medicine from Stanley Medical College, Chennai  and secured his MS degree from the same institution.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '51157723', 'context': 'Thenumgal Poulose Jacob is an Indian surgeon specialized in vascular surgery, and the founder Head of the Department of Vascular Surgery at Madras Medical College. Born in Aluva, in the south Indian state of Kerala  to Thenumgal Poulose and Mariam in a Malayali family, he did his under-graduate studies at UC College before graduating in medicine from Stanley Medical College, Chennai  and secured his MS degree from the same institution. He started his practice under government service at Madras Medical College where he helped establish the department of vascular surgery in 1978 and served as its founder head till his superannuation in 1993. The Government of India awarded him the fourth highest civilian honour of the Padma Shri, in 2014, for his contributions to medical science. He is married to Esther and the couple has a daughter, Siji Jacob, an academic and a son, Hasum Jacob Thenumgal, an engineer at TCS. He is doing private practice at T. P. Jacob Clinic in Royapuram, Chennai and is also a consultant vascular surgeon at MV Hospitals for Diabetes, Royapuram.', 'domain': 'wiki', 'before_edit': ' Born in Aluva, in the south Indian state of Kerala  to Thenumgal Poulose and Mariam in a Malayali family, he did his under-graduate studies at UC College before graduating in medicine from Stanley Medical College, Chennai  and secured his MS degree from the same institution.', 'after_edit': ' Born in Aluva, in the south Indian state of Kerala  to Thenumgal Poulose and Mariam in a Malayali family, he did his under-graduate studies at UC College before graduating in medicine from Stanley Medical College, Chennai , and secured his MS degree from the same institution.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '51157723', 'context': 'Thenumgal Poulose Jacob is an Indian surgeon specialized in vascular surgery, and the founder Head of the Department of Vascular Surgery at Madras Medical College. Born in Aluva, in the south Indian state of Kerala  to Thenumgal Poulose and Mariam in a Malayali family, he did his under-graduate studies at UC College before graduating in medicine from Stanley Medical College, Chennai  and secured his MS degree from the same institution. He started his practice under government service at Madras Medical College where he helped establish the department of vascular surgery in 1978 and served as its founder head till his superannuation in 1993. The Government of India awarded him the fourth highest civilian honour of the Padma Shri, in 2014, for his contributions to medical science. He is married to Esther and the couple has a daughter, Siji Jacob, an academic and a son, Hasum Jacob Thenumgal, an engineer at TCS. He is doing private practice at T. P. Jacob Clinic in Royapuram, Chennai and is also a consultant vascular surgeon at MV Hospitals for Diabetes, Royapuram.', 'domain': 'wiki', 'before_edit': ' He started his practice under government service at Madras Medical College where he helped establish the department of vascular surgery in 1978 and served as its founder head till his superannuation in 1993.', 'after_edit': ' He started his practice under government service at Madras Medical College where he helped establish the department of vascular surgery in 1978 and served as its founder head until his superannuation in 1993.', 'label': 'fluency', 'raw_intents': ['clarity', 'fluency', 'fluency']}
{'doc_id': '51157723', 'context': 'Thenumgal Poulose Jacob is an Indian surgeon specialized in vascular surgery, and the founder Head of the Department of Vascular Surgery at Madras Medical College. Born in Aluva, in the south Indian state of Kerala  to Thenumgal Poulose and Mariam in a Malayali family, he did his under-graduate studies at UC College before graduating in medicine from Stanley Medical College, Chennai  and secured his MS degree from the same institution. He started his practice under government service at Madras Medical College where he helped establish the department of vascular surgery in 1978 and served as its founder head till his superannuation in 1993. The Government of India awarded him the fourth highest civilian honour of the Padma Shri, in 2014, for his contributions to medical science. He is married to Esther and the couple has a daughter, Siji Jacob, an academic and a son, Hasum Jacob Thenumgal, an engineer at TCS. He is doing private practice at T. P. Jacob Clinic in Royapuram, Chennai and is also a consultant vascular surgeon at MV Hospitals for Diabetes, Royapuram.', 'domain': 'wiki', 'before_edit': ' He is doing private practice at T. P. Jacob Clinic in Royapuram, Chennai and is also a consultant vascular surgeon at MV Hospitals for Diabetes, Royapuram.', 'after_edit': ' He is in private practice at T. P. Jacob Clinic in Royapuram, Chennai and is also a consultant vascular surgeon at MV Hospitals for Diabetes, Royapuram.', 'label': 'fluency', 'raw_intents': ['others', 'fluency', 'fluency']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' is a model and endorser in Philippines ,', 'after_edit': ' is a model and endorser in Philippines .', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' He is known as one of the winners in Mister Philippines 2011,', 'after_edit': ' He is known for being one of the winners in Mister Philippines 2011,', 'label': 'clarity', 'raw_intents': ['clarity', 'fluency', 'clarity']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'after_edit': ' which is made to the audience of his appeal . Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'label': 'coherence', 'raw_intents': ['coherence', 'clarity', 'coherence']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'after_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila . Before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'label': 'fluency', 'raw_intents': ['coherence', 'fluency', 'fluency']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'after_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis worked as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'style']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'after_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model . Fhrancis and his agency signed a contract with the Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'label': 'coherence', 'raw_intents': ['meaning-changed', 'coherence', 'coherence']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'after_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists  and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'label': 'clarity', 'raw_intents': ['clarity', 'clarity', 'coherence']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'after_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management . After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'after_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held a pageant in Patravadi Theatre. Lopez is one of the finalists Top 16  in Mister International 2011.', 'label': 'fluency', 'raw_intents': ['fluency', 'others', 'clarity']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'after_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the  Top 16  in Mister International 2011.', 'label': 'coherence', 'raw_intents': ['style', 'coherence', 'coherence']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011.', 'after_edit': ' which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16 finalists in Mister International 2011.', 'label': 'clarity', 'raw_intents': ['clarity', 'others', 'coherence']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'after_edit': ' Mister Philippines 2011 & Mister International 2011 Lopez was declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'label': 'fluency', 'raw_intents': ['fluency', 'meaning-changed', 'fluency']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'after_edit': ' Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winners of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'after_edit': ' Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011. After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'fluency']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'after_edit': ' Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined to compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'coherence']}
{'doc_id': '51355210', 'context': 'Fhrancis Oliver Lopez (born February 3, 1989 in Las Pias, Philippines), or better known as Fhrancis Lopez, is a model and endorser in Philippines , He is known as one of the winners in Mister Philippines 2011, which is made to the audience of his appeal , he is look a like an actor in the Philippines, JM de URL Biography Lopez was born and raised in the southern tip of Metro Manila , before he joined the pageant of Mister Philippines, Fhrancis work as a model and he signing contracted in a Go Green Artists his agency and the Project Management , After he won and got the prize, Fhrancis held pageant of Patravadi Theatre, Lopez is one of the finalists Top 16  in Mister International 2011. Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'domain': 'wiki', 'before_edit': ' Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant which is happening in Pattaya, Thailand and Bangkok.', 'after_edit': ' Mister Philippines 2011 & Mister International 2011 Lopez  declared as one of the winner of Mister Philippines 2011, After he won the competition on December 17, 2011, Lopez joined the compete in the Mister International 2011 pageant  in Pattaya, Thailand and Bangkok.', 'label': 'coherence', 'raw_intents': ['coherence', 'coherence', 'clarity']}
{'doc_id': '65581384', 'context': "Tuanku Panglima Paderap, also called Panglima Deli, was the third ruler of the Deli Sultanate, now part of North Sumatra, Indonesia. He succeeded his father Tuanku Panglima Perunggit, who died around 1700. Paderap has four sons, namely Tuanku Jalaluddin (or Kejuruan Metar), Tuanku Panglima Pasutan (or Kejuruan Padang), Tuanku Tawar (or Kejuruan Santun), and Tuanku Umar (or Kejuruan Junjongan). A power struggle happened in the Deli after Paderap died in 1720. Jalaluddin, Paderap's first son, cannot replace him because of a physical disability. In the end, it was Pasutan who became the fourth ruler of Deli, while his younger brother Umar got a separate territory and became the first ruler of the Serdang Sultanate.", 'domain': 'wiki', 'before_edit': ' Paderap has four sons, namely Tuanku Jalaluddin (or Kejuruan Metar), Tuanku Panglima Pasutan (or Kejuruan Padang), Tuanku Tawar (or Kejuruan Santun), and Tuanku Umar (or Kejuruan Junjongan).', 'after_edit': ' Paderap had four sons, namely Tuanku Jalaluddin (or Kejuruan Metar), Tuanku Panglima Pasutan (or Kejuruan Padang), Tuanku Tawar (or Kejuruan Santun), and Tuanku Umar (or Kejuruan Junjongan).', 'label': 'fluency', 'raw_intents': ['coherence', 'fluency', 'fluency']}
{'doc_id': '65581384', 'context': "Tuanku Panglima Paderap, also called Panglima Deli, was the third ruler of the Deli Sultanate, now part of North Sumatra, Indonesia. He succeeded his father Tuanku Panglima Perunggit, who died around 1700. Paderap has four sons, namely Tuanku Jalaluddin (or Kejuruan Metar), Tuanku Panglima Pasutan (or Kejuruan Padang), Tuanku Tawar (or Kejuruan Santun), and Tuanku Umar (or Kejuruan Junjongan). A power struggle happened in the Deli after Paderap died in 1720. Jalaluddin, Paderap's first son, cannot replace him because of a physical disability. In the end, it was Pasutan who became the fourth ruler of Deli, while his younger brother Umar got a separate territory and became the first ruler of the Serdang Sultanate.", 'domain': 'wiki', 'before_edit': " Jalaluddin, Paderap's first son, cannot replace him because of a physical disability.", 'after_edit': " Jalaluddin, Paderap's first son, could not replace him because of a physical disability.", 'label': 'fluency', 'raw_intents': ['fluency', 'fluency', 'style']}
{'doc_id': '65581384', 'context': "Tuanku Panglima Paderap, also called Panglima Deli, was the third ruler of the Deli Sultanate, now part of North Sumatra, Indonesia. He succeeded his father Tuanku Panglima Perunggit, who died around 1700. Paderap has four sons, namely Tuanku Jalaluddin (or Kejuruan Metar), Tuanku Panglima Pasutan (or Kejuruan Padang), Tuanku Tawar (or Kejuruan Santun), and Tuanku Umar (or Kejuruan Junjongan). A power struggle happened in the Deli after Paderap died in 1720. Jalaluddin, Paderap's first son, cannot replace him because of a physical disability. In the end, it was Pasutan who became the fourth ruler of Deli, while his younger brother Umar got a separate territory and became the first ruler of the Serdang Sultanate.", 'domain': 'wiki', 'before_edit': ' In the end, it was Pasutan who became the fourth ruler of Deli, while his younger brother Umar got a separate territory and became the first ruler of the Serdang Sultanate.', 'after_edit': ' It was Pasutan who became the fourth ruler of Deli, while his younger brother Umar got a separate territory and became the first ruler of the Serdang Sultanate.', 'label': 'clarity', 'raw_intents': ['clarity', 'coherence', 'clarity']}
{'doc_id': '65581384', 'context': "Tuanku Panglima Paderap, also called Panglima Deli, was the third ruler of the Deli Sultanate, now part of North Sumatra, Indonesia. He succeeded his father Tuanku Panglima Perunggit, who died around 1700. Paderap has four sons, namely Tuanku Jalaluddin (or Kejuruan Metar), Tuanku Panglima Pasutan (or Kejuruan Padang), Tuanku Tawar (or Kejuruan Santun), and Tuanku Umar (or Kejuruan Junjongan). A power struggle happened in the Deli after Paderap died in 1720. Jalaluddin, Paderap's first son, cannot replace him because of a physical disability. In the end, it was Pasutan who became the fourth ruler of Deli, while his younger brother Umar got a separate territory and became the first ruler of the Serdang Sultanate.", 'domain': 'wiki', 'before_edit': ' In the end, it was Pasutan who became the fourth ruler of Deli, while his younger brother Umar got a separate territory and became the first ruler of the Serdang Sultanate.', 'after_edit': ' In the end, it was Pasutan who became the fourth ruler of Deli, while his younger brother Umar  became the first ruler of the Serdang Sultanate.', 'label': 'clarity', 'raw_intents': ['clarity', 'coherence', 'clarity']}
